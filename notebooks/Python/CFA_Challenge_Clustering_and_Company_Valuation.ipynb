{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Z-WJGUymfP"
      },
      "source": [
        "\n",
        "# Clustering & Company Valuation using Clustering\n",
        "\n",
        "---\n",
        "\n",
        "This script contains examples on how to run unsupervised learning algorithms in Python. Specifically, the scirpt contains 2 sections:\n",
        "- **Introduction to clustering**, where we simulate a dataset and run classic clustering techniques like k-means and DBScan. We also look at how to formally evaluate a clustering algorithm.\n",
        "- **Clustering for company valuation**. Here, we use clustering to execute company valuation using the multiples method. In order for you to implement the use case, you need to download two datasets titled \"financialdata_original.csv\" and \"financialdata_extra.csv\". The script will show you how to run the 6 steps of company valuation by clustering:\n",
        "  * Step 1: Data collection & importing;\n",
        "  * Step 2: Data preprocessing;\n",
        "  * Step 3: Model selection;\n",
        "  * Step 4: Clustering;\n",
        "  * Step 5: Identify closest companies; and\n",
        "  * Step 6: Valuation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zT8iFFiJFUn"
      },
      "source": [
        "# Introduction to Clustering in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWAToLGeJN0H"
      },
      "source": [
        "We start by importing the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZYK-4WiyuB2"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import MaxNLocator # PyLab is a procedural interface to the Matplotlib object-oriented plotting library.\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew6fXj0MJYvr",
        "outputId": "07566e9c-4ff5-4b64-85a2-3875fcae0c8a"
      },
      "outputs": [],
      "source": [
        "# To start, we create a dataset.\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# create blobs\n",
        "X, y = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.6, random_state=50)\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "yORM5bJXJbs1",
        "outputId": "ce149d1f-36ad-4228-f2b2-7fbcdfa78f67"
      },
      "outputs": [],
      "source": [
        "# Let's plot data\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.scatter(X[:,0], X[:,1],cmap='Accent', s=70)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMBfdYYhJevr",
        "outputId": "be4e8678-de7d-4bee-a357-d91af8150b66"
      },
      "outputs": [],
      "source": [
        "# Apply k-Means\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "# from sklearn.metrics import pairwise_distances\n",
        "\n",
        "\n",
        "# silhouette: 1=good, 0=overlap, -1=bad\n",
        "# Within Cluster Sum of Squares: lower is better\n",
        "\n",
        "def cluster_kmeans(df, nclust):\n",
        "\n",
        "    kmeans = KMeans(n_clusters=nclust, random_state=0).fit(df)\n",
        "    label = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    sil=metrics.silhouette_score(df, label, metric='euclidean', random_state=0)\n",
        "    wcss = kmeans.inertia_\n",
        "\n",
        "    return sil, wcss, label, centroids\n",
        "\n",
        "cluster_kmeans(X, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "aV92C0pLJhd5",
        "outputId": "42d24f27-4bb4-4cb8-8c22-94f52f745357"
      },
      "outputs": [],
      "source": [
        "# Let's plot the clustering\n",
        "sil, wcss, label, centroid = cluster_kmeans(X, 4)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(X[:,0], X[:,1], c=label, cmap='Accent', s=40)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nhzLTPibJl_O",
        "outputId": "af434b0b-d89b-47ed-e6fa-dd48a56d3808"
      },
      "outputs": [],
      "source": [
        "# We need to validate the number of clusters. So let's check how the WCSS and the\n",
        "# Silhouette coefficient change if we consider different number of clusters\n",
        "\n",
        "max_n_clusters = 7\n",
        "\n",
        "tab=pd.DataFrame(columns = ['Clusters', 'Silhouette(max)', 'WCSS(min)'], dtype=int).fillna('')\n",
        "tab['Silhouette(max)']=tab['Silhouette(max)'].astype(float)\n",
        "\n",
        "fig, ax = plt.subplots(math.ceil((max_n_clusters-1) / 2), 2, figsize=(20,20), constrained_layout=True)\n",
        "ax=ax.flatten()\n",
        "for i in range(max_n_clusters-1):\n",
        "\n",
        "    nclust = i + 2\n",
        "    sil, wcss, label, centroids = cluster_kmeans(X, nclust)\n",
        "    tab = pd.concat([tab, pd.DataFrame([[nclust, sil, wcss]], columns=tab.columns)], ignore_index=True)\n",
        "\n",
        "    ax[i].scatter(X[:,0], X[:,1], c=label, cmap='Accent', s=40)\n",
        "    ax[i].scatter(centroids[:,0], centroids[:,1], c=range(nclust), cmap='Accent', s=300, marker='P')\n",
        "    ax[i].set_title('Clusters: ' + str(nclust), fontsize = 30)\n",
        "    textstr = 'Sil: ' + str(round(sil, 3)) + '\\nWCSS: ' + str(int(wcss))\n",
        "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
        "    ax[i].text(0.75, 0.97, textstr, transform=ax[i].transAxes, fontsize=25,\n",
        "        verticalalignment='top', bbox=props)\n",
        "\n",
        "plt.show()\n",
        "display(tab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "vXTpImyMJr4b",
        "outputId": "d72ef676-6617-47d0-f1d6-b1a5112e5549"
      },
      "outputs": [],
      "source": [
        "# Determine optimal number of clusters with Elbow method\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10,5))\n",
        "ax1.plot(tab.Clusters, tab['Silhouette(max)'], 'bx-', color = 'blue')\n",
        "ax1.set_xlabel('Number of clusters', fontsize = 20)\n",
        "ax1.set_ylabel('Silhouette', fontsize = 20, color = 'blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue', labelsize=13)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(tab.Clusters, tab['WCSS(min)'], 'bx-', color = 'red')\n",
        "ax2.set_ylabel('WCSS', fontsize = 20, color = 'red')\n",
        "ax2.tick_params(axis='y', labelcolor='red', labelsize=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "ghDkHudRJu87",
        "outputId": "c690af2e-c5a6-4e82-b0d4-342c9524d3f1"
      },
      "outputs": [],
      "source": [
        "# K-Means is not the only clustering algo. Let's try DBSCAN - Density-based spatial clustering of applications with noise\n",
        "# Let's create a dataset with strange data shape (moons)\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X2, y2 = make_moons(200, noise=0.05, random_state=0)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.scatter(X2[:,0], X2[:,1], cmap='Accent', s=40)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "mZVcrlYQJv_Z",
        "outputId": "280ad5d1-e186-4f40-fdc0-3b20fa46e9ca"
      },
      "outputs": [],
      "source": [
        "# Before running the code: How do you think the k-means algorithm will perfrom\n",
        "# on data like this? How would it split the data?\n",
        "# Run k-means\n",
        "sil, wcss, label, centroid = cluster_kmeans(X2, 2)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.scatter(X2[:,0], X2[:,1], c=label, cmap='Accent', s=40)\n",
        "plt.show()\n",
        "print('Silhouette:', sil)\n",
        "print('WCSS:', wcss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "UYi0m76VJyl2",
        "outputId": "09994d64-4e83-4aa8-b7db-90f7baa99440"
      },
      "outputs": [],
      "source": [
        "# Try DBSCAN - Density-based spatial clustering of applications with noise\n",
        "# DBSCAN starts by identifying the neighboring observations of each observation within some radius\n",
        "# (a hyperparameter). Any data point that is within the data point of radius of another data point\n",
        "# are in the same cluster\n",
        "from sklearn.cluster import DBSCAN\n",
        "db = DBSCAN(eps=0.3).fit(X2) # epsfloat, default=0.5 --> The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
        "\n",
        "label = db.labels_\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.scatter(X2[:,0], X2[:,1], c=label, cmap='Accent', s=40)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN933CYZI7MU"
      },
      "source": [
        "# Clustering for company valuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ZU-6zyZcTD22",
        "outputId": "8897a0a3-56a4-429a-e3da-d790a94ed15d"
      },
      "outputs": [],
      "source": [
        "# Step 1: Data collection. Let's upload fundumentals data on a set of companies.\n",
        "# For this step, you will need to upload the data \"financialdata_original.csv\"\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "2nEUOSTzYYXm",
        "outputId": "73960650-14cb-4e9a-e943-2bf7cdd359a8"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "dataset = pd.read_csv(io.BytesIO(uploaded['financialdata_original.csv']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxFgcRAOdDRI"
      },
      "outputs": [],
      "source": [
        "dataset.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "-ocvXm6GYmTM",
        "outputId": "c9e3fe82-138f-492f-ec78-e154df05f82f"
      },
      "outputs": [],
      "source": [
        "# Step 2: Data preprocessing\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd9RJwJEmIFq"
      },
      "outputs": [],
      "source": [
        "dataset.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykbRAaYBYARk"
      },
      "outputs": [],
      "source": [
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKrapzFrYNMj"
      },
      "outputs": [],
      "source": [
        "print(dataset.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agun1BJWcW3h"
      },
      "outputs": [],
      "source": [
        "dataset.isna().any() # Check for NAs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U02l4I6cjbW"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.dropna() # Drop rows with NAs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8OqVA44aTRk"
      },
      "outputs": [],
      "source": [
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFZ3wwLQAHwO"
      },
      "outputs": [],
      "source": [
        "# Step 3: Model selection - Identify the optimal cluster.\n",
        "dataset_clustering = dataset.select_dtypes(exclude = \"object\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFlen70hnbpS"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "df=pd.DataFrame(preprocessing.StandardScaler().fit_transform(dataset_clustering.values), columns = dataset_clustering.columns)\n",
        "X=df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-fqn5utNzgs"
      },
      "outputs": [],
      "source": [
        "# Apply k-Means\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "# from sklearn.metrics import pairwise_distances\n",
        "\n",
        "\n",
        "# silhouette: 1=good, 0=overlap, -1=bad\n",
        "# Within Cluster Sum of Squares: lower is better\n",
        "\n",
        "def cluster_kmeans(df, nclust):\n",
        "\n",
        "    kmeans = KMeans(n_clusters=nclust, random_state=0).fit(df)\n",
        "    label = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    sil=metrics.silhouette_score(df, label, metric='euclidean', random_state=0)\n",
        "    wcss = kmeans.inertia_\n",
        "\n",
        "    return sil, wcss, label, centroids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqEmdyDSCTpE"
      },
      "outputs": [],
      "source": [
        "# Apply k-Means\n",
        "# Remember: silhouette: 1=good, 0=overlap, -1=bad\n",
        "# Within Cluster Sum of Squares: lower is better\n",
        "cluster_kmeans(X, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13plMkcul48b"
      },
      "outputs": [],
      "source": [
        "# Since, we cannot plot the data as it is multidimensiona, we use the dimensionality reduction technique - Principal Component Analysis (PCA).\n",
        "# We notice that the first 2 PC account for ~50% of the variations in the dataset.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px\n",
        "\n",
        "pca = PCA(n_components=X.shape[1], random_state=0).fit(X)\n",
        "scores = pca.transform(X)\n",
        "\n",
        "exp_var_pca = pca.explained_variance_ratio_\n",
        "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
        "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
        "plt.ylabel('Cumulative Explained Variance', size=15)\n",
        "plt.xlabel('Number of Principal Components', size=15)\n",
        "plt.legend(loc='best', fontsize=15)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeDeu3171xsF"
      },
      "outputs": [],
      "source": [
        "# Validate number of clusters - we evaluate clusters on X\n",
        "\n",
        "max_n_clusters = 21\n",
        "\n",
        "tab=pd.DataFrame(columns = ['Clusters', 'Silhouette(max)', 'WCSS(min)'], dtype=int).fillna('')\n",
        "tab['Silhouette(max)']=tab['Silhouette(max)'].astype(float)\n",
        "label_list={}\n",
        "\n",
        "fig, ax = plt.subplots(math.ceil((max_n_clusters-1) / 2), 2, figsize=(40,40), constrained_layout=True)\n",
        "ax=ax.flatten()\n",
        "for i in range(max_n_clusters-1):\n",
        "\n",
        "    nclust = i + 2\n",
        "    sil, wcss, label, _ = cluster_kmeans(X, nclust)\n",
        "    df = pd.DataFrame(data=scores,index=label)\n",
        "    centroids = df.groupby(level=0).mean().values\n",
        "    tab = pd.concat([tab, pd.DataFrame([[nclust, sil, wcss]], columns=tab.columns)], ignore_index=True)\n",
        "    label_list[str(nclust)]=label\n",
        "\n",
        "    ax[i].scatter(scores[:,0], scores[:,1], c=label, cmap='Accent', s=40)\n",
        "    ax[i].scatter(centroids[:,0], centroids[:,1], c=range(nclust), cmap='Accent', s=300, marker='P')\n",
        "    ax[i].set_title('Clusters: ' + str(nclust), fontsize = 30)\n",
        "    textstr = 'Sil: ' + str(round(sil, 3)) + '\\nWCSS: ' + str(int(wcss))\n",
        "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
        "    ax[i].text(0.75, 0.97, textstr, transform=ax[i].transAxes, fontsize=25,\n",
        "        verticalalignment='top', bbox=props)\n",
        "\n",
        "plt.show()\n",
        "display(tab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkQr9OzWCdIr"
      },
      "outputs": [],
      "source": [
        "# Determine optimal number of clusters with Elbow method\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10,5))\n",
        "ax1.plot(tab.Clusters, tab['Silhouette(max)'], 'bx-', color = 'blue')\n",
        "ax1.set_xlabel('Number of clusters', fontsize = 20)\n",
        "ax1.set_ylabel('Silhouette', fontsize = 20, color = 'blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue', labelsize=13)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(tab.Clusters, tab['WCSS(min)'], 'bx-', color = 'red')\n",
        "ax2.set_ylabel('WCSS', fontsize = 20, color = 'red')\n",
        "ax2.tick_params(axis='y', labelcolor='red', labelsize=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s_64e0_ChjX"
      },
      "outputs": [],
      "source": [
        "# Step 4: Once we have identified the optimal number of clusters, let's run the\n",
        "# clustering and assign the appropriate cluster to each company.\n",
        "kmeans = KMeans(n_clusters=8, random_state=42)\n",
        "dataset['Cluster'] = kmeans.fit_predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJfGqbq0Dtec"
      },
      "outputs": [],
      "source": [
        "# Step 5: Identify Closest Companies\n",
        "# Let's imagine that Company 11 is not public and we want to value it using the multiples method\n",
        "# Let's first find its cluster based on the balance sheet and income statements values.\n",
        "Company11_cluster = dataset[dataset['shortName'] == 'Company_11']['Cluster'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2I6ZQBJvN4t"
      },
      "outputs": [],
      "source": [
        "# Extact the data for the companies that are in the same cluster\n",
        "similar_companies = dataset[dataset['Cluster'] == Company11_cluster]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6kJn8GoFPyC"
      },
      "outputs": [],
      "source": [
        "similar_companies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5_QxlY9ayc7"
      },
      "outputs": [],
      "source": [
        "# Let's remove Company_11 for the similar companies dataset and add all the\n",
        "# information on the market performance of the other publically traded companies\n",
        "similar_companies = similar_companies[similar_companies['shortName'] != 'Company_11']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiP5w8-sbFFA"
      },
      "outputs": [],
      "source": [
        "# Let's upload the market data for the other publically traded companies and add\n",
        "# them to our similar_companies data.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsfwOhgNcABN"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "import io\n",
        "data_extra = pd.read_csv(io.BytesIO(uploaded['financialdata_extra.csv']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M5b-uB4cNwI"
      },
      "outputs": [],
      "source": [
        "# Let's merge the datasets\n",
        "merged_data = pd.merge(similar_companies, data_extra, on='shortName', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMSlTOTJc2sa"
      },
      "outputs": [],
      "source": [
        "# Checking the merged data\n",
        "merged_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N76xA4OTFa2Q"
      },
      "outputs": [],
      "source": [
        "# Step 6: Valuation\n",
        "# Assuming 'Market Cap' as the valuation metric\n",
        "avg_market_cap = merged_data['marketCap'].mean()\n",
        "avg_market_cap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAdCqxBVXD4G"
      },
      "outputs": [],
      "source": [
        "# Let's create some other multiples\n",
        "merged_data.loc[:,'EV_to_ebitda'] = merged_data['enterpriseValue'] / merged_data['ebitda']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXWOw_2beHd9"
      },
      "outputs": [],
      "source": [
        "# Obtain the average EV/ebitda multiple\n",
        "average_EV_evitda = merged_data[\"EV_to_ebitda\"].mean()\n",
        "average_EV_evitda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFej0DdQek_Q"
      },
      "outputs": [],
      "source": [
        "ebitda_value_company11 = dataset.loc[dataset['shortName'] == 'Company_11', 'ebitda'].values[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yxbGrisfH1J"
      },
      "outputs": [],
      "source": [
        "# Company_11 estimated EV based on EV/Ebitda multiple obtained by clustering\n",
        "Company11_EV = average_EV_evitda*ebitda_value_company11\n",
        "Company11_EV"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
