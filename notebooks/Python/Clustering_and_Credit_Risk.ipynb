{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHdWkX1x7Mp4"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/umatter/EDFB/blob/main/notebooks/Python/Clustering_and_Credit_Risk.ipynb)\n",
        "\n",
        "# Practical of Real Financial Data\n",
        "\n",
        "---\n",
        "This notebook includes an end-to-end unsupervised and supervised learning task on financial data. It is prepared for GitHub + Colab sharing: the first cell bootstraps exact dependencies, and the dataset is loaded from the repo (or downloaded) so it runs end-to-end without manual uploads.\n",
        "\n",
        "*   Describe the data and carry-out all necessary pre-processing\n",
        "*   Run k-means clustering using different k values\n",
        "*   Evaluate the perfromance of the different clusters and select the best value for k\n",
        "*   Train a logistic regression classifier that predicts whether the company will default on its loan using the full dataset\n",
        "*   Train a separate model for each of the identified clusters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3zhHbqnLFMS"
      },
      "outputs": [],
      "source": [
        "# @title Setup (installs exact versions; safe to rerun)\n",
        "import sys\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install numpy==1.26.4 pandas==2.2.2 scikit-learn==1.5.1 matplotlib==3.9.0 seaborn==0.13.2 statsmodels==0.14.2 scipy==1.13.1 plotly==5.23.0\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pylab import MaxNLocator # PyLab is a procedural interface to the Matplotlib object-oriented plotting library.\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "%matplotlib inline\n",
        "\n",
        "print('Python:', sys.version)\n",
        "import sklearn, matplotlib\n",
        "print('Loaded versions:')\n",
        "print('- numpy', np.__version__)\n",
        "print('- pandas', pd.__version__)\n",
        "print('- scikit-learn', sklearn.__version__)\n",
        "print('- matplotlib', matplotlib.__version__)\n",
        "print('- seaborn', sns.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thiRojMBQv6M"
      },
      "outputs": [],
      "source": [
        "# To make this notebook's output stable across runs (we make the output reproducable)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKoNrmhKLydQ"
      },
      "outputs": [],
      "source": [
        "# Define cluster_kmeans function\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "# from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# silhouette: 1=good, 0=overlap, -1=bad\n",
        "# Within Cluster Sum of Squares: lower is better\n",
        "\n",
        "def cluster_kmeans(df, nclust):\n",
        "\n",
        "    kmeans = KMeans(n_clusters=nclust, random_state=0).fit(df)\n",
        "    label = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    sil=metrics.silhouette_score(df, label, metric='euclidean', random_state=0)\n",
        "    wcss = kmeans.inertia_\n",
        "\n",
        "    return sil, wcss, label, centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "r0Lb4v-e1nTR",
        "outputId": "fe1a286a-01fa-43a3-8d85-dc43ec09b17a"
      },
      "outputs": [],
      "source": [
        "# Load borrower_companies.csv from repo (data/borrower_companies.csv) or download from GitHub\n",
        "data_path = Path('data/borrower_companies.csv')\n",
        "data_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "if data_path.exists():\n",
        "    print('Loading data from', data_path)\n",
        "    dataset = pd.read_csv(data_path)\n",
        "else:\n",
        "    url = 'https://raw.githubusercontent.com/umatter/EDFB/main/data/borrower_companies.csv'\n",
        "    try:\n",
        "        print('Attempting to download borrower_companies.csv from', url)\n",
        "        urllib.request.urlretrieve(url, data_path)\n",
        "        dataset = pd.read_csv(data_path)\n",
        "        print('Downloaded to', data_path)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f'Could not obtain borrower_companies.csv: {e}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "OtTpqJet1okc",
        "outputId": "d6baa96c-d2c6-4182-ff49-5e839eefc364"
      },
      "outputs": [],
      "source": [
        "# Data is loaded into `dataset` above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "uPeft1iq1s6O",
        "outputId": "18f28dac-6d9c-43b2-a380-bf8f86f854a1"
      },
      "outputs": [],
      "source": [
        "# In the following steps, we investigate the properties of the data.\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4vwOB8itfol",
        "outputId": "e6d5df01-f3cb-43bd-b463-f639f042dbd8"
      },
      "outputs": [],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "-qZcdquithWn",
        "outputId": "72a60d2e-000a-435a-ed67-15cf0b6c2dea"
      },
      "outputs": [],
      "source": [
        "dataset.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "WsQiV_tJvRjO",
        "outputId": "92344cfb-531b-4ac5-c7ab-77ac9b4ad8cd"
      },
      "outputs": [],
      "source": [
        "dataset.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "nf94VdgsvSoT",
        "outputId": "bd32310b-75bc-42f6-f330-3585a57a73f4"
      },
      "outputs": [],
      "source": [
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "QtYiShXwvZ-N",
        "outputId": "39dc38ae-44a5-4340-e712-3cbfc0586552"
      },
      "outputs": [],
      "source": [
        "# We check the distribution of the features included in the dataset through box plots. A box plot is a method for graphically depicting groups of numerical data through their quartiles.\n",
        "from sklearn import preprocessing\n",
        "def box_plot(df, standardize=True):\n",
        "\n",
        "    fig=plt.figure(figsize=(20,10))\n",
        "\n",
        "    if standardize==True:\n",
        "        # standardize columns for better visualization\n",
        "        df=pd.DataFrame(preprocessing.StandardScaler().fit_transform(df.values), columns = df.columns) # Standard.Scaler (x-m)/s\n",
        "    fig=sns.boxplot(x='value', y='variable', data=pd.melt(df.reset_index(), id_vars='index', value_vars=list(df.columns)),\n",
        "               orient='h')\n",
        "    fig.tick_params(labelsize=10)\n",
        "    fig.set_xlabel('')\n",
        "    fig.set_ylabel('')\n",
        "    fig.set_title('Note that variables are standardized\\nfor better visualization', fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "box_plot(dataset.drop(columns=\"status\"), standardize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRBX1jJExIY9"
      },
      "outputs": [],
      "source": [
        "# The abave graph indicated the presence of many outliers. In the following step we apply the z-score.\n",
        "# A z-score indicated the number of standard deviations above or below the mean that each value falls.\n",
        "# For example, a Z-score of 3 indicates that an observation is three standard deviations above the average\n",
        "# while a Z-score of -3 signifies it is three standard deviations below the mean. A standard cut-off value for\n",
        "# finding outliers are Z-scores of +/-3 or 4 further from zero\n",
        "from scipy import stats\n",
        "z = np.abs(stats.zscore(dataset))\n",
        "dataset_o = dataset[(z < 4).all(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekXQJ7WPxSZf",
        "outputId": "9b0571da-561e-48be-d023-075dcd6ce4d4"
      },
      "outputs": [],
      "source": [
        "# We check the shape of the new data. We have reduced the dataset significantly\n",
        "dataset_o.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "UupVRyXjxZTm",
        "outputId": "0dbb11d3-63a3-4f55-a8ac-4b273f7ad88f"
      },
      "outputs": [],
      "source": [
        "dataset_o.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "uzgnx_0IxjXI",
        "outputId": "7f7ec039-7d49-4378-e866-c7300f80f2f2"
      },
      "outputs": [],
      "source": [
        "# Simiarly, we again check the distribution of the features through box plot. Although reduced, we still have significant amount of outliers in the sample.\n",
        "box_plot(dataset_o.drop(columns=\"status\"), standardize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyfyve951t7C"
      },
      "outputs": [],
      "source": [
        "# In the next section, we proceed with running a clustering algorithm on the data so to identify groups of homogenous borrower-companies.\n",
        "X = dataset_o.drop(columns=\"status\")\n",
        "y = dataset_o.copy().status\n",
        "X = pd.DataFrame(preprocessing.StandardScaler().fit_transform(X.values), columns = X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "E1nhMmLS_vxu",
        "outputId": "c20289e2-26f6-41ef-908d-c58956f08b1a"
      },
      "outputs": [],
      "source": [
        "# Since, we cannot plot the data as it is multidimensional, we use the dimensionality reduction technique - Principal Component Analysis (PCA).\n",
        "# We notice that the first 2 PC account for ~40% of the variations in the dataset.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import plotly.express as px\n",
        "\n",
        "pca = PCA(n_components=X.shape[1], random_state=0).fit(X)\n",
        "scores = pca.transform(StandardScaler().fit_transform(X))\n",
        "\n",
        "exp_var_pca = pca.explained_variance_ratio_\n",
        "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
        "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
        "plt.ylabel('Cumulative Explained Variance', size=15)\n",
        "plt.xlabel('Number of Principal Components', size=15)\n",
        "plt.legend(loc='best', fontsize=15)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EeDeu3171xsF",
        "outputId": "f844a033-53c1-4a27-fb46-107cd63e69f2"
      },
      "outputs": [],
      "source": [
        "# In the next step, we validate the number of clusters i.e. we evaluate clusters on X\n",
        "max_n_clusters = 7\n",
        "\n",
        "tab=pd.DataFrame(columns = ['Clusters', 'Silhouette(max)', 'WCSS(min)'], dtype=int).fillna('')\n",
        "tab['Silhouette(max)']=tab['Silhouette(max)'].astype(float)\n",
        "label_list={}\n",
        "\n",
        "fig, ax = plt.subplots(math.ceil((max_n_clusters-1) / 2), 2, figsize=(20,max_n_clusters *4), constrained_layout=True)\n",
        "ax=ax.flatten()\n",
        "for i in range(max_n_clusters-1):\n",
        "\n",
        "    nclust = i + 2\n",
        "    sil, wcss, label, _ = cluster_kmeans(X, nclust)\n",
        "    df = pd.DataFrame(data=scores,index=label)\n",
        "    centroids = df.groupby(level=0).mean().values\n",
        "    tab = pd.concat([tab, pd.DataFrame([[nclust, sil, wcss]], columns=tab.columns)], ignore_index=True)\n",
        "    label_list[str(nclust)]=label\n",
        "\n",
        "    ax[i].scatter(scores[:,0], scores[:,1], c=label, cmap='Accent', s=40)\n",
        "    ax[i].scatter(centroids[:,0], centroids[:,1], c=range(nclust), cmap='Accent', s=300, marker='P')\n",
        "    ax[i].set_title('Clusters: ' + str(nclust), fontsize = 30)\n",
        "    textstr = 'Sil: ' + str(round(sil, 3)) + '\\nWCSS: ' + str(int(wcss))\n",
        "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
        "    ax[i].text(0.75, 0.97, textstr, transform=ax[i].transAxes, fontsize=25,\n",
        "        verticalalignment='top', bbox=props)\n",
        "\n",
        "plt.show()\n",
        "display(tab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "s-TvGuLQ11Cj",
        "outputId": "b5d5c334-9293-4774-9787-029b4fbc24c3"
      },
      "outputs": [],
      "source": [
        "# Next, we determine optimal number of clusters with Elbow method and the Silhouette coefficinet.\n",
        "# What would you suggest as the ideal cut-off point?\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10,5))\n",
        "ax1.plot(tab.Clusters, tab['Silhouette(max)'], 'bx-', color = 'blue')\n",
        "ax1.set_xlabel('Number of clusters', fontsize = 20)\n",
        "ax1.set_ylabel('Silhouette', fontsize = 20, color = 'blue')\n",
        "ax1.tick_params(axis=\"x\", labelsize=15)\n",
        "ax1.tick_params(axis='y', labelcolor='blue', labelsize=13)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(tab.Clusters, tab['WCSS(min)'], 'bx-', color = 'red')\n",
        "ax2.set_ylabel('WCSS', fontsize = 20, color = 'red')\n",
        "ax2.tick_params(axis='y', labelcolor='red', labelsize=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5jcp79W-131Q",
        "outputId": "cf0b7f1b-fce5-4662-e3fd-811db411a4a5"
      },
      "outputs": [],
      "source": [
        "# In the next step, we inspect the clusters' features. Specifically, we want to check whether there is a significant difference in the distribution of the features amount the clusters.\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "number_of_clusters = 3\n",
        "\n",
        "\n",
        "label = label_list[str(number_of_clusters)]\n",
        "fig, ax = plt.subplots(math.ceil(X.shape[1] / 2), 2, figsize=(20,20), constrained_layout=True)\n",
        "ax=ax.flatten()\n",
        "from sklearn import preprocessing\n",
        "X_labels=pd.DataFrame(data=X.values, index=label, columns=X.columns)\n",
        "i=0\n",
        "\n",
        "for var in X_labels.columns:\n",
        "\n",
        "    for clust in range(number_of_clusters):\n",
        "\n",
        "        df = X_labels.copy()[X_labels.index == clust]\n",
        "        sns.distplot(df[var], ax=ax[i], norm_hist=True, label='Cluster ' + str(clust+1), hist_kws=dict(alpha=0.4))\n",
        "        ax[i].set_title(var, fontsize=30)\n",
        "        ax[i].set_xlabel('')\n",
        "        ax[i].legend(loc='center left', bbox_to_anchor=(1.0, 0.5), fontsize=16)\n",
        "\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XJbOPmPyCZ_"
      },
      "outputs": [],
      "source": [
        "# Next, we run the k-means and create 3 seperate dataset for each of the clusters\n",
        "kmeans = KMeans(3, random_state=0).fit(X)\n",
        "label = kmeans.labels_\n",
        "X[\"label\"] = label\n",
        "X['status'] = dataset_o.copy().status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__J1fbx6z94h"
      },
      "outputs": [],
      "source": [
        "cluster_0 = X[X[\"label\"] == 0]\n",
        "cluster_1 = X[X[\"label\"] == 1]\n",
        "cluster_2 = X[X[\"label\"] == 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_kOPhWe1V4b"
      },
      "source": [
        "**Training a classifer on the entire dataset vs one for each of the clusters**\n",
        "In this section, we are going to demonstrate the usefulness of unsupervised learning algorithms and clustering in particular as a pre-modelling step. Specifically, we wil\n",
        "\n",
        "* Train a logistic regression classifier that predicts whether the company will default on its loan using the full dataset\n",
        "* Train a seperate model for each of the identified clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "ZlQFdS5D1z4c",
        "outputId": "0b7e3f93-92f1-40c2-8b1a-eacdbe71a731"
      },
      "outputs": [],
      "source": [
        "# We start with the outlier free datasets containing all observations\n",
        "dataset_o.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "mnXrjZfo1rF5",
        "outputId": "e6f2cfc6-0d3e-4b42-c092-83ff3e4f3e48"
      },
      "outputs": [],
      "source": [
        "# As a reminder, we check the dispersion with box plot\n",
        "box_plot(dataset_o.drop(columns=\"status\"), standardize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "iYWAdAwK2E0i",
        "outputId": "f8251d8b-6571-43c1-dc37-8ada14e4a96d"
      },
      "outputs": [],
      "source": [
        "# Check distribution for target variable\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.catplot(x='status', kind=\"count\", data=dataset_o) # categorical plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvR5c-y1ujyK",
        "outputId": "b2c17097-2e91-4d21-a4c3-707914fa0330"
      },
      "outputs": [],
      "source": [
        "# The dataset is very unbalanced so we remove some observation for y=0 to be equal to 2*size of y=1.\n",
        "# This is called \"undersampling\"\n",
        "\n",
        "# We keep all y=1\n",
        "from sklearn.model_selection import train_test_split\n",
        "data_1 = dataset_o[dataset_o['status'] == 1]\n",
        "print(data_1.shape)\n",
        "\n",
        "# We take y=0 as double the size of data_1\n",
        "# Moreover we \"stratify\" the sampling in order to take the same distribution for each variable\n",
        "# We use the train_test_split function and we keep the test only\n",
        "all_data_0 = dataset_o[dataset_o['status'] == 0]\n",
        "percentage_corresponding_to_double_size = 2*data_1.shape[0] / all_data_0.shape[0] # 2*size_1 compared to size_0\n",
        "\n",
        "X = all_data_0.drop(columns=['status'])\n",
        "y = all_data_0['status'].to_frame()\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)\n",
        "data_0_big, data_0_small = train_test_split(all_data_0, test_size=percentage_corresponding_to_double_size,\n",
        "                                                    random_state=0, shuffle=True)\n",
        "print(data_0_big.shape) # remaining from the dataset\n",
        "print(data_0_small.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_gy9Rb_nIC5",
        "outputId": "d124dc53-8169-47e8-dbd8-34872bd01930"
      },
      "outputs": [],
      "source": [
        "# We merge the two dataset\n",
        "\n",
        "dataset=pd.concat([data_1, data_0_small], axis= 0).reset_index(drop=True)  # axis = 1 by column and = 0 by row\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xntsQC2d3Vzb",
        "outputId": "0a78b217-e5cd-4684-93f4-107f4172fad2"
      },
      "outputs": [],
      "source": [
        "# We define X and y and standardise\n",
        "X = dataset.drop(columns=['status'])\n",
        "y = dataset['status'].values.reshape(-1,1)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ7VPeGOC_vV"
      },
      "outputs": [],
      "source": [
        "X = pd.DataFrame(preprocessing.StandardScaler().fit_transform(X.values), columns = X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9ED16m83sBf",
        "outputId": "c6a86638-9158-4e0c-dbbe-9d88093acb38"
      },
      "outputs": [],
      "source": [
        "# Split train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=dataset.status)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "itqO74C94Bzq",
        "outputId": "f9f976bb-6804-4eec-f66e-ac79f6403a4e"
      },
      "outputs": [],
      "source": [
        "# Fit the model on training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(solver='lbfgs', random_state=0) # solver (https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451)\n",
        "model.fit(X_train,y_train) # training the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtO7G7FOZb0n",
        "outputId": "509118e7-03cb-4298-d98f-eaf78577a5ec"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "logit_model=sm.Logit(y_train,X_train)\n",
        "result=logit_model.fit()\n",
        "print(result.summary2())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "-Fwyo9fO4EU2",
        "outputId": "8bfd6563-b836-4210-a4e6-9f35d95519d8"
      },
      "outputs": [],
      "source": [
        "# Get fitted value on testing set\n",
        "y_test_predicted = model.predict(X_test)\n",
        "\n",
        "# Compare predictions\n",
        "display(pd.DataFrame({'True': y_test.flatten(), 'Predicted': y_test_predicted.flatten()}))\n",
        "\n",
        "# Compare predicted probabilities (default threshold for converting to 0 or 1 is 0.5)\n",
        "y_test_predicted_prob = model.predict_proba(X_test)[:,1]\n",
        "display(pd.DataFrame({'True': y_test.flatten(), 'Predicted_prob': y_test_predicted_prob.flatten(), 'Predicted': y_test_predicted.flatten()}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbO569694I00",
        "outputId": "78d353c5-0fe1-4813-a610-81a9968ab2da"
      },
      "outputs": [],
      "source": [
        "# Evaluate confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_test_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "Vt6m-3Q24J1e",
        "outputId": "01b08976-f7b1-43eb-f601-0a2a2bd8397b"
      },
      "outputs": [],
      "source": [
        "# Evaluate confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = ['0', '1']\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(y_test, y_test_predicted)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpxs8Cv34RlV",
        "outputId": "06e7b9c6-2009-4455-b762-fe888eb9c92e"
      },
      "outputs": [],
      "source": [
        "# Evaluate precision, recall, F1-score on test set\n",
        "# A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally),\n",
        "# whereas a micro-average will aggregate the contributions of all classes to compute the average metric.\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_test_predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "eIXvA4Md4UlS",
        "outputId": "d8bd0df5-8064-41a5-b40f-e98cbaba9715"
      },
      "outputs": [],
      "source": [
        "# Finally, we plot the ROC curve and the corresponding area under the curve.\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "logit_roc_auc = roc_auc_score(y_test, y_test_predicted)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_predicted_prob)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHf-fZOq4Ykk"
      },
      "source": [
        "**Unsupervised learning as a pre-modelling step**\n",
        "\n",
        "In the next section, we evaluate a logitic classifer trained seperately on each cluster, starting with \"cluster_0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "DOvIY5Js4bsm",
        "outputId": "7c58ce48-6174-49d6-c44d-646aa3f07c2b"
      },
      "outputs": [],
      "source": [
        "# We strat by describing the subset.\n",
        "cluster_0.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "OZ7BbXJLuesK",
        "outputId": "1cf71626-d41f-426c-c15a-7668e95b7169"
      },
      "outputs": [],
      "source": [
        "# Check dispersion with box plot\n",
        "box_plot(cluster_0.drop(columns=\"status\"), standardize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "3HcqozOC4ia6",
        "outputId": "4ce0b4ec-5c7b-46dc-8189-53488b5e3236"
      },
      "outputs": [],
      "source": [
        "# Check distribution for target variable\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.catplot(x='status', kind=\"count\", data=cluster_0) # categorical plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZwLGuoW4uOa",
        "outputId": "049cb82c-b634-4224-a51a-0990b4a2e764"
      },
      "outputs": [],
      "source": [
        "# Similarly as before, the subsample is very unbalanced so we remove some observation for y=0 to be equal to 2*size of y=1.\n",
        "# We keep all y=1\n",
        "from sklearn.model_selection import train_test_split\n",
        "data_1 = cluster_0[cluster_0['status'] == 1]\n",
        "print(data_1.shape)\n",
        "\n",
        "# We take y=0 as double the size of data_1\n",
        "# Moreover we \"stratify\" the sampling in order to take the same distribution for each variable\n",
        "# We use the train_test_split function and we keep the test only\n",
        "all_data_0 = cluster_0[cluster_0['status'] == 0]\n",
        "percentage_corresponding_to_double_size = 2*data_1.shape[0] / all_data_0.shape[0] # 2*size_1 compared to size_0\n",
        "\n",
        "X = all_data_0.drop(columns=['status'])\n",
        "y = all_data_0['status'].to_frame()\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)\n",
        "data_0_big, data_0_small = train_test_split(all_data_0, test_size=percentage_corresponding_to_double_size,\n",
        "                                                    random_state=0, shuffle=True)\n",
        "print(data_0_big.shape) # remaining from the dataset\n",
        "print(data_0_small.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osBNAYNf5H78",
        "outputId": "ae1ac5a9-9507-4ce3-8468-a03cba1c912e"
      },
      "outputs": [],
      "source": [
        "# We merge the two dataset\n",
        "dataset=pd.concat([data_1, data_0_small], axis= 0).reset_index(drop=True)  # axis = 1 by column and = 0 by row\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "C2AA_vwu5PV1",
        "outputId": "6857441b-8493-41e0-bcd8-d48e4c989eba"
      },
      "outputs": [],
      "source": [
        "# We check distribution for target variable after downsampling\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.catplot(x='status', kind=\"count\", data=dataset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbl8hKom5cbc",
        "outputId": "bb53534b-7bdc-4964-e14a-b9cf05603574"
      },
      "outputs": [],
      "source": [
        "# We define X and y and strandardize\n",
        "X = dataset.drop(columns=['status'])\n",
        "y = dataset['status'].values.reshape(-1,1)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqfbTRa05f80"
      },
      "outputs": [],
      "source": [
        "X = pd.DataFrame(preprocessing.StandardScaler().fit_transform(X.values), columns = X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgoF025o5g2d",
        "outputId": "e1ca48d3-d8fb-49ea-ac56-f6f72f43d7f3"
      },
      "outputs": [],
      "source": [
        "# Split train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=dataset.status)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "vB7rMeQfE1q3",
        "outputId": "31151fbc-685d-457c-ccf6-0aa64f4083d8"
      },
      "outputs": [],
      "source": [
        "# Fit the model on training set\n",
        "model = LogisticRegression(solver='lbfgs', random_state=0) # solver (https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451)\n",
        "model.fit(X_train,y_train) # training the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "ypSE_NOKEu-r",
        "outputId": "443b6ab1-a94a-416d-eec5-6af2b2ea055a"
      },
      "outputs": [],
      "source": [
        "# Get fitted value on test set\n",
        "y_test_predicted = model.predict(X_test)\n",
        "\n",
        "# Compare predictions\n",
        "display(pd.DataFrame({'True': y_test.flatten(), 'Predicted': y_test_predicted.flatten()}))\n",
        "\n",
        "# Compare predicted probabilities (default threshold for converting to 0 or 1 is 0.5)\n",
        "y_test_predicted_prob = model.predict_proba(X_test)[:,1]\n",
        "display(pd.DataFrame({'True': y_test.flatten(), 'Predicted_prob': y_test_predicted_prob.flatten(), 'Predicted': y_test_predicted.flatten()}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-c9q4K4Eu-s",
        "outputId": "daaed833-99cb-4035-9f4c-0db63ec61589"
      },
      "outputs": [],
      "source": [
        "# Evaluate confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_test_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "Bu3QUUfaEu-s",
        "outputId": "6b361a5e-9900-4de0-c84f-dc610c6bbe10"
      },
      "outputs": [],
      "source": [
        "# Evaluate confusion matrix\n",
        "plot_confusion_matrix(y_test, y_test_predicted)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smy0pKY7Eu-s",
        "outputId": "0af401b4-c509-4385-8c9c-75b8b3d776d9"
      },
      "outputs": [],
      "source": [
        "# Evaluate precision, recall, F1-score on test set\n",
        "# A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally),\n",
        "# whereas a micro-average will aggregate the contributions of all classes to compute the average metric.\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_test_predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "jcQ9F0mPEu-t",
        "outputId": "794a8e71-afe0-4f3d-af66-24c1c3290650"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "logit_roc_auc = roc_auc_score(y_test, y_test_predicted)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_predicted_prob)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r269rtzZ6pvw"
      },
      "source": [
        "\n",
        "\n",
        "**Cluster_1: ML model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "0gKXzbXI6pvw",
        "outputId": "c3b742a7-f1e3-49b6-ea5a-6ea3cc2cdeab"
      },
      "outputs": [],
      "source": [
        "cluster_1.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7mvj-Qe6pvw",
        "outputId": "0495f299-8eac-4a17-c37b-bed014d04826"
      },
      "outputs": [],
      "source": [
        "cluster_1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "azuztrHf6pvw",
        "outputId": "b1cc1b60-e910-4787-ada9-899b592fc3e8"
      },
      "outputs": [],
      "source": [
        "# Check dispersion with box plot\n",
        "box_plot(cluster_1.drop(columns=\"status\"), standardize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "XJyGXGmb6pvw",
        "outputId": "ceecbecc-ee0c-446b-c825-52599792a223"
      },
      "outputs": [],
      "source": [
        "# Check distribution for target variable\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.catplot(x='status', kind=\"count\", data=cluster_1) # categorical plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TV2Oytb6pvx",
        "outputId": "204f604d-88d3-4cdf-e31d-15c0cdceffd2"
      },
      "outputs": [],
      "source": [
        "# Dataset is very unbalanced so we remove some observation for y=0 to be equal to 2*size of y=1.\n",
        "# We keep all y=1\n",
        "from sklearn.model_selection import train_test_split\n",
        "data_1 = cluster_1[cluster_1['status'] == 1]\n",
        "print(data_1.shape)\n",
        "\n",
        "# We take y=0 as double the size of data_1\n",
        "# Moreover we \"stratify\" the sampling in order to take the same distribution for each variable\n",
        "# We use the train_test_split function and we keep the test only\n",
        "all_data_0 = cluster_1[cluster_1['status'] == 0]\n",
        "percentage_corresponding_to_double_size = 2*data_1.shape[0] / all_data_0.shape[0] # 2*size_1 compared to size_0\n",
        "\n",
        "X = all_data_0.drop(columns=['status'])\n",
        "y = all_data_0['status'].to_frame()\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)\n",
        "data_0_big, data_0_small = train_test_split(all_data_0, test_size=percentage_corresponding_to_double_size,\n",
        "                                                    random_state=0, shuffle=True)\n",
        "print(data_0_big.shape) # remaining from the dataset\n",
        "print(data_0_small.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk5jfC506pvx",
        "outputId": "74d2362b-d52c-4fe8-8850-c4849b13c3bd"
      },
      "outputs": [],
      "source": [
        "# Merge two dataset\n",
        "\n",
        "dataset=pd.concat([data_1, data_0_small], axis= 0).reset_index(drop=True)  # axis = 1 by column and = 0 by row\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "umB8xWqq6pvx",
        "outputId": "8687ec3d-4509-4940-fb16-9be755a17e99"
      },
      "outputs": [],
      "source": [
        "# Check distribution for target variable after downsampling\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.catplot(x='status', kind=\"count\", data=dataset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrM9kEbU6pvx",
        "outputId": "9ac819a9-a9fc-4334-b643-3a854de7793d"
      },
      "outputs": [],
      "source": [
        "X = dataset.drop(columns=['status'])\n",
        "y = dataset['status'].values.reshape(-1,1)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDYRo0xY6pvx"
      },
      "outputs": [],
      "source": [
        "X = pd.DataFrame(preprocessing.StandardScaler().fit_transform(X.values), columns = X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgY12mcL6pvx",
        "outputId": "466a8b62-01a0-4999-d13c-01aba4ccc0bd"
      },
      "outputs": [],
      "source": [
        "# Split train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=dataset.status)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "bl9OQPFc6pvx",
        "outputId": "a3d64659-6fdb-4f87-84c4-9616de781979"
      },
      "outputs": [],
      "source": [
        "# Fit the model on training set\n",
        "model = LogisticRegression(solver='lbfgs', random_state=0) # solver (https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451)\n",
        "model.fit(X_train,y_train) # training the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vbLvjjYx6pvx",
        "outputId": "533bf3d0-d218-489b-9527-ed591cefad01"
      },
      "outputs": [],
      "source": [
        "# Get fitted value on test set\n",
        "y_test_predicted = model.predict(X_test)\n",
        "\n",
        "# Compare predictions\n",
        "display(pd.DataFrame({'True': y_test.flatten(), 'Predicted': y_test_predicted.flatten()}))\n",
        "\n",
        "# Compare predicted probabilities (default threshold for converting to 0 or 1 is 0.5)\n",
        "y_test_predicted_prob = model.predict_proba(X_test)[:,1]\n",
        "display(pd.DataFrame({'True': y_test.flatten(), 'Predicted_prob': y_test_predicted_prob.flatten(), 'Predicted': y_test_predicted.flatten()}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "kQdFhKVm6pvy",
        "outputId": "0e96dbae-f159-4ff6-a04f-7488dad28b1c"
      },
      "outputs": [],
      "source": [
        "# Evaluate confusion matrix\n",
        "plot_confusion_matrix(y_test, y_test_predicted)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "zzELvVFr6pvy",
        "outputId": "5da6d0c4-7d3d-4279-d774-e95806bd1e50"
      },
      "outputs": [],
      "source": [
        "# Plot the ROC curve and the corresponding AUC value.\n",
        "logit_roc_auc = roc_auc_score(y_test, y_test_predicted)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_predicted_prob)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1rBUAD-7QdU"
      },
      "source": [
        "**Cluster_2: ML model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "J47lmSo47QdV",
        "outputId": "d7f7e885-78e2-4fa8-a068-af668d9892c9"
      },
      "outputs": [],
      "source": [
        "cluster_2.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLhvgw0y7QdV",
        "outputId": "612abc68-ea28-47f0-ed44-99bfab07af8b"
      },
      "outputs": [],
      "source": [
        "cluster_2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "8f2cuqUK7QdV",
        "outputId": "2e11ab23-6670-449a-884f-49eafc657ecc"
      },
      "outputs": [],
      "source": [
        "# Check dispersion with box plot\n",
        "box_plot(cluster_2.drop(columns=\"status\"), standardize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "bQksqEaw7QdV",
        "outputId": "8529b06c-6c16-4869-f1b0-5474de29bba7"
      },
      "outputs": [],
      "source": [
        "# Check distribution for target variable\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.catplot(x='status', kind=\"count\", data=cluster_2) # categorical plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-1IMyjb7QdV",
        "outputId": "f738c540-47cb-41f6-d0bc-7513cc38b3ff"
      },
      "outputs": [],
      "source": [
        "# Dataset is very unbalanced so we remove some observation for y=0 to be equal to 2*size of y=1.\n",
        "# We keep all y=1\n",
        "from sklearn.model_selection import train_test_split\n",
        "data_1 = cluster_2[cluster_2['status'] == 1]\n",
        "print(data_1.shape)\n",
        "\n",
        "# We take y=0 as double the size of data_1\n",
        "# Moreover we \"stratify\" the sampling in order to take the same distribution for each variable\n",
        "# We use the train_test_split function and we keep the test only\n",
        "all_data_0 = cluster_2[cluster_2['status'] == 0]\n",
        "percentage_corresponding_to_double_size = 2*data_1.shape[0] / all_data_0.shape[0] # 2*size_1 compared to size_0\n",
        "\n",
        "X = all_data_0.drop(columns=['status'])\n",
        "y = all_data_0['status'].to_frame()\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)\n",
        "data_0_big, data_0_small = train_test_split(all_data_0, test_size=percentage_corresponding_to_double_size,\n",
        "                                                    random_state=0, shuffle=True)\n",
        "print(data_0_big.shape) # remaining from the dataset\n",
        "print(data_0_small.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opN5zhq87QdV",
        "outputId": "7bcb85ba-b12c-4b0c-8800-5d37ea864ecc"
      },
      "outputs": [],
      "source": [
        "# Merge two dataset\n",
        "\n",
        "dataset=pd.concat([data_1, data_0_small], axis= 0).reset_index(drop=True)  # axis = 1 by column and = 0 by row\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "5URMZKgU7QdW",
        "outputId": "9c14672a-1992-416e-fea7-47b13dd24579"
      },
      "outputs": [],
      "source": [
        "# Check distribution for target variable after downsampling\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.catplot(x='status', kind=\"count\", data=dataset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-Xu-iRD7QdW",
        "outputId": "d8001057-5181-4b3a-fc0d-8e71ef48cb0b"
      },
      "outputs": [],
      "source": [
        "X = dataset.drop(columns=['status'])\n",
        "y = dataset['status'].values.reshape(-1,1)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vYBU7CW7QdW"
      },
      "outputs": [],
      "source": [
        "X = pd.DataFrame(preprocessing.StandardScaler().fit_transform(X.values), columns = X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcbMXtXZ7QdW",
        "outputId": "64038398-95ea-4ae4-a64b-06615d250c88"
      },
      "outputs": [],
      "source": [
        "# Split train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=dataset.status)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "uynYh78D7QdW",
        "outputId": "5b430ac7-65bc-46ec-da20-34519e472e7b"
      },
      "outputs": [],
      "source": [
        "# Fit the model on training set\n",
        "model = LogisticRegression(solver='lbfgs', random_state=0) # solver (https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451)\n",
        "model.fit(X_train,y_train) # training the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zDZhNJla7QdW",
        "outputId": "12396129-6deb-48db-ccb8-4aefad769c55"
      },
      "outputs": [],
      "source": [
        "# Get fitted value on test set\n",
        "y_test_predicted = model.predict(X_test)\n",
        "\n",
        "# Compare predictions\n",
        "display(pd.DataFrame({'True': y_test.flatten(), 'Predicted': y_test_predicted.flatten()}))\n",
        "\n",
        "# Compare predicted probabilities (default threshold for converting to 0 or 1 is 0.5)\n",
        "y_test_predicted_prob = model.predict_proba(X_test)[:,1]\n",
        "display(pd.DataFrame({'True': y_test.flatten(), 'Predicted_prob': y_test_predicted_prob.flatten(), 'Predicted': y_test_predicted.flatten()}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "PDgLUJiR7QdW",
        "outputId": "179ed9bf-0994-4daa-a2bb-3daada8489ba"
      },
      "outputs": [],
      "source": [
        "# Evaluate confusion matrix\n",
        "plot_confusion_matrix(y_test, y_test_predicted)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "naV8WAOG7QdX",
        "outputId": "306a6112-99b4-4faa-df51-881eb4195a7a"
      },
      "outputs": [],
      "source": [
        "logit_roc_auc = roc_auc_score(y_test, y_test_predicted)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_predicted_prob)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
