{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: common Python libraries mirroring typical R data science stacks\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stats / ML (install if needed)\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    import statsmodels.formula.api as smf\n",
    "except Exception as e:\n",
    "    print(\"statsmodels not found; install with: pip install statsmodels\")\n",
    "\n",
    "try:\n",
    "    from sklearn import model_selection, metrics, preprocessing, linear_model, ensemble, tree\n",
    "except Exception as e:\n",
    "    print(\"scikit-learn not found; install with: pip install scikit-learn\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d224d",
   "metadata": {},
   "source": [
    "# Advanced Predictive Models in R: Lasso, Trees, Random Forests, and Boosting\n",
    "\n",
    "---\n",
    "This notebook demonstrates how to use more advanced machine learning models in R for predicting customer behavior in banking. We'll cover:\n",
    "\n",
    "- **Lasso regression**: A linear model that automatically selects important features\n",
    "- **Decision Trees**: Easy-to-interpret models that make decisions like a flowchart\n",
    "- **Random Forests**: Combines many decision trees for better predictions\n",
    "- **Boosting**: Builds models sequentially, learning from previous mistakes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "1. Understand when to use each type of model\n",
    "2. Know how to prepare data for machine learning\n",
    "3. Be able to train, evaluate, and compare different models\n",
    "4. Interpret model results and feature importance\n",
    "\n",
    "## Required Packages\n",
    "Please ensure you have the required packages installed:\n",
    "\n",
    "```r\n",
    "install.packages(c(\"gamlr\", \"rpart\", \"randomForest\", \"xgboost\", \"caret\", \"pROC\"))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b89750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Load libraries with error handling\n",
    "# required_packages <- c(\"gamlr\", \"rpart\", \"randomForest\", \"xgboost\", \"caret\", \"pROC\", \"data.table\", \"ggplot2\")\n",
    "# \n",
    "# for (pkg in required_packages) {\n",
    "#   if (!require(pkg, character.only = TRUE)) {\n",
    "#     cat(\"Installing package:\", pkg, \"\\n\")\n",
    "#     install.packages(pkg)\n",
    "#     library(pkg, character.only = TRUE)\n",
    "#   }\n",
    "# }\n",
    "# \n",
    "# cat(\"All packages loaded successfully!\\n\")\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Load libraries with error handling\n",
    "required_packages = list(\"gamlr\", \"rpart\", \"randomForest\", \"xgboost\", \"caret\", \"pROC\", \"data.table\", \"ggplot2\")\n",
    "\n",
    "for (pkg in required_packages) {\n",
    "  if (!require(pkg, character.only = True)) {\n",
    "    cat(\"Installing package:\", pkg, \"\\n\")\n",
    "    install.packages(pkg)\n",
    "    # R: library(pkg, character.only = True) — install/import Python equivalents as needed\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"All packages loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19901afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Load and explore the banking dataset\n",
    "# data <- fread(\"banking.csv\")\n",
    "# \n",
    "# cat(\"Dataset dimensions:\", nrow(data), \"rows and\", ncol(data), \"columns\\n\\n\")\n",
    "# \n",
    "# # Display structure\n",
    "# str(data)\n",
    "# \n",
    "# # Summary statistics\n",
    "# summary(data)\n",
    "# \n",
    "# # Check for missing values\n",
    "# cat(\"\\nMissing values per column:\\n\")\n",
    "# sapply(data, function(x) sum(is.na(x)))\n",
    "# \n",
    "# # Look at the target variable distribution\n",
    "# cat(\"\\nTarget variable (y) distribution:\\n\")\n",
    "# table(data$y)\n",
    "# prop.table(table(data$y))\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Load and explore the banking dataset\n",
    "data = fread(\"banking.csv\")\n",
    "\n",
    "cat(\"Dataset dimensions:\", len(data), \"rows and\", data.shape[1], \"columns\\n\\n\")\n",
    "\n",
    "# Display structure\n",
    "data.info()\n",
    "\n",
    "# Summary statistics\n",
    "data.describe()\n",
    "\n",
    "# Check for missing values\n",
    "cat(\"\\nMissing values per column:\\n\")\n",
    "sapply(data, function(x) sum(x.isna()))\n",
    "\n",
    "# Look at the target variable distribution\n",
    "cat(\"\\nTarget variable (y) distribution:\\n\")\n",
    "table(data$y)\n",
    "prop.table(table(data$y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6c2a4",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Before building models, we need to prepare our data properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Convert categorical variables to factors\n",
    "# categorical_vars <- c(\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\", \"y\")\n",
    "# data[, (categorical_vars) := lapply(.SD, as.factor), .SDcols = categorical_vars]\n",
    "# \n",
    "# # Create train/test split (80/20)\n",
    "# set.seed(123)  # For reproducibility\n",
    "# train_idx <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))\n",
    "# train_data <- data[train_idx, ]\n",
    "# test_data <- data[-train_idx, ]\n",
    "# \n",
    "# cat(\"Training set:\", nrow(train_data), \"observations\\n\")\n",
    "# cat(\"Test set:\", nrow(test_data), \"observations\\n\")\n",
    "# \n",
    "# # Check that target variable is balanced in both sets\n",
    "# cat(\"\\nTarget distribution in training set:\\n\")\n",
    "# prop.table(table(train_data$y))\n",
    "# cat(\"\\nTarget distribution in test set:\\n\")\n",
    "# prop.table(table(test_data$y))\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Convert categorical variables to factors\n",
    "categorical_vars = list(\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\", \"y\")\n",
    "data[, (categorical_vars) := lapply(.SD, as.factor), .SDcols = categorical_vars]\n",
    "\n",
    "# Create train/test split (80/20)\n",
    "np.random.seed(123)  # For reproducibility\n",
    "train_idx = # TODO: translate sample()\n",
    "sample(seq_len(len(data)), size = 0.8 * len(data))\n",
    "train_data = data[train_idx, ]\n",
    "test_data = data[-train_idx, ]\n",
    "\n",
    "cat(\"Training set:\", len(train_data), \"observations\\n\")\n",
    "cat(\"Test set:\", len(test_data), \"observations\\n\")\n",
    "\n",
    "# Check that target variable is balanced in both sets\n",
    "cat(\"\\nTarget distribution in training set:\\n\")\n",
    "prop.table(table(train_data$y))\n",
    "cat(\"\\nTarget distribution in test set:\\n\")\n",
    "prop.table(table(test_data$y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bea608",
   "metadata": {},
   "source": [
    "## 1. Lasso Regression\n",
    "\n",
    "**What is Lasso?** Lasso (Least Absolute Shrinkage and Selection Operator) is a linear model that automatically selects the most important features by setting less important coefficients to zero. This helps prevent overfitting and makes the model easier to interpret.\n",
    "\n",
    "**When to use Lasso:**\n",
    "- When you have many features and want automatic feature selection\n",
    "- When you need an interpretable model\n",
    "- When you suspect many features are irrelevant\n",
    "\n",
    "### Logistic Lasso: Predicting customer subscription (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0cc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Prepare data for logistic lasso (training set)\n",
    "# X_train <- model.matrix(y ~ ., train_data)[, -1]  # Remove intercept\n",
    "# y_train <- train_data$y\n",
    "# \n",
    "# # Prepare test set\n",
    "# X_test <- model.matrix(y ~ ., test_data)[, -1]\n",
    "# y_test <- test_data$y\n",
    "# \n",
    "# # Fit logistic lasso with cross-validation to find optimal lambda\n",
    "# cv_lasso <- cv.gamlr(X_train, y_train, family = \"binomial\")\n",
    "# \n",
    "# # Plot the cross-validation curve\n",
    "# plot(cv_lasso, main = \"Lasso Cross-Validation\")\n",
    "# cat(\"Optimal lambda:\", cv_lasso$lambda.min, \"\\n\")\n",
    "# \n",
    "# # Get coefficients for the optimal model\n",
    "# lasso_coef <- coef(cv_lasso, select = \"min\")\n",
    "# cat(\"\\nNumber of selected features:\", sum(lasso_coef != 0) - 1, \"\\n\")  # -1 for intercept\n",
    "# \n",
    "# # Show non-zero coefficients (selected features)\n",
    "# selected_features <- lasso_coef[lasso_coef != 0, , drop = FALSE]\n",
    "# print(selected_features)\n",
    "# \n",
    "# # Make predictions on test set\n",
    "# lasso_pred_prob <- predict(cv_lasso, X_test, type = \"response\")\n",
    "# lasso_pred_class <- ifelse(lasso_pred_prob > 0.5, \"yes\", \"no\")\n",
    "# \n",
    "# # Evaluate performance\n",
    "# lasso_accuracy <- mean(lasso_pred_class == y_test)\n",
    "# cat(\"\\nLasso Test Accuracy:\", round(lasso_accuracy, 3), \"\\n\")\n",
    "# \n",
    "# # Confusion matrix\n",
    "# lasso_cm <- table(Predicted = lasso_pred_class, Actual = y_test)\n",
    "# print(lasso_cm)\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Prepare data for logistic lasso (training set)\n",
    "X_train = model.matrix(y ~ ., train_data)[, -1]  # Remove intercept\n",
    "y_train = train_data$y\n",
    "\n",
    "# Prepare test set\n",
    "X_test = model.matrix(y ~ ., test_data)[, -1]\n",
    "y_test = test_data$y\n",
    "\n",
    "# Fit logistic lasso with cross-validation to find optimal lambda\n",
    "cv_lasso = cv.gamlr(X_train, y_train, family = \"binomial\")\n",
    "\n",
    "# Plot the cross-validation curve\n",
    "# TODO: translate base R plot -> matplotlib\n",
    "# plot(cv_lasso, main = \"Lasso Cross-Validation\")\n",
    "cat(\"Optimal lambda:\", cv_lasso$lambda.min, \"\\n\")\n",
    "\n",
    "# Get coefficients for the optimal model\n",
    "lasso_coef = coef(cv_lasso, select = \"min\")\n",
    "cat(\"\\nNumber of selected features:\", sum(lasso_coef != 0) - 1, \"\\n\")  # -1 for intercept\n",
    "\n",
    "# Show non-zero coefficients (selected features)\n",
    "selected_features = lasso_coef[lasso_coef != 0, , drop = False]\n",
    "print(selected_features)\n",
    "\n",
    "# Make predictions on test set\n",
    "lasso_pred_prob = predict(cv_lasso, X_test, type = \"response\")\n",
    "lasso_pred_class = np.where(lasso_pred_prob > 0.5, \"yes\", \"no\")\n",
    "\n",
    "# Evaluate performance\n",
    "lasso_accuracy = mean(lasso_pred_class == y_test)\n",
    "cat(\"\\nLasso Test Accuracy:\", round(lasso_accuracy, 3), \"\\n\")\n",
    "\n",
    "# Confusion matrix\n",
    "lasso_cm = table(Predicted = lasso_pred_class, Actual = y_test)\n",
    "print(lasso_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf54999",
   "metadata": {},
   "source": [
    "## 2. Decision Trees\n",
    "\n",
    "**What are Decision Trees?** Decision trees make predictions by asking a series of yes/no questions about the features. They're like a flowchart that leads to a prediction.\n",
    "\n",
    "**When to use Decision Trees:**\n",
    "- When you need a highly interpretable model\n",
    "- When relationships between features are non-linear\n",
    "- When you want to understand the decision-making process\n",
    "\n",
    "**Pros:** Easy to interpret, handles non-linear relationships\n",
    "**Cons:** Can overfit, unstable (small data changes can create very different trees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a80d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Fit a decision tree for classification\n",
    "# tree_mod <- rpart(y ~ ., data = train_data, method = \"class\", \n",
    "#                   control = rpart.control(cp = 0.01, minsplit = 20))\n",
    "# \n",
    "# # Plot the tree\n",
    "# plot(tree_mod, uniform=TRUE, margin=0.1)\n",
    "# text(tree_mod, use.n=TRUE, all=TRUE, cex=.8)\n",
    "# title(\"Decision Tree for Customer Subscription Prediction\")\n",
    "# \n",
    "# # Print complexity parameter table\n",
    "# printcp(tree_mod)\n",
    "# \n",
    "# # Make predictions\n",
    "# tree_pred_class <- predict(tree_mod, test_data, type = \"class\")\n",
    "# tree_pred_prob <- predict(tree_mod, test_data, type = \"prob\")[, \"yes\"]\n",
    "# \n",
    "# # Evaluate performance\n",
    "# tree_accuracy <- mean(tree_pred_class == test_data$y)\n",
    "# cat(\"\\nDecision Tree Test Accuracy:\", round(tree_accuracy, 3), \"\\n\")\n",
    "# \n",
    "# # Confusion matrix\n",
    "# tree_cm <- table(Predicted = tree_pred_class, Actual = test_data$y)\n",
    "# print(tree_cm)\n",
    "# \n",
    "# # Feature importance\n",
    "# cat(\"\\nFeature Importance (Decision Tree):\\n\")\n",
    "# importance_tree <- tree_mod$variable.importance\n",
    "# print(sort(importance_tree, decreasing = TRUE)[1:10])  # Top 10\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Fit a decision tree for classification\n",
    "tree_mod = rpart(y ~ ., data = train_data, method = \"class\", \n",
    "                  control = rpart.control(cp = 0.01, minsplit = 20))\n",
    "\n",
    "# Plot the tree\n",
    "# TODO: translate base R plot -> matplotlib\n",
    "# plot(tree_mod, uniform=True, margin=0.1)\n",
    "text(tree_mod, use.n=True, all=True, cex=.8)\n",
    "title(\"Decision Tree for Customer Subscription Prediction\")\n",
    "\n",
    "# Print complexity parameter table\n",
    "printcp(tree_mod)\n",
    "\n",
    "# Make predictions\n",
    "tree_pred_class = predict(tree_mod, test_data, type = \"class\")\n",
    "tree_pred_prob = predict(tree_mod, test_data, type = \"prob\")[, \"yes\"]\n",
    "\n",
    "# Evaluate performance\n",
    "tree_accuracy = mean(tree_pred_class == test_data$y)\n",
    "cat(\"\\nDecision Tree Test Accuracy:\", round(tree_accuracy, 3), \"\\n\")\n",
    "\n",
    "# Confusion matrix\n",
    "tree_cm = table(Predicted = tree_pred_class, Actual = test_data$y)\n",
    "print(tree_cm)\n",
    "\n",
    "# Feature importance\n",
    "cat(\"\\nFeature Importance (Decision Tree):\\n\")\n",
    "importance_tree = tree_mod$variable.importance\n",
    "print(sort(importance_tree, decreasing = True)[1:10])  # Top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b58f9",
   "metadata": {},
   "source": [
    "## 3. Random Forests\n",
    "\n",
    "**What are Random Forests?** Random Forests combine many decision trees, where each tree is trained on a random subset of the data and features. The final prediction is the average (or majority vote) of all trees.\n",
    "\n",
    "**When to use Random Forests:**\n",
    "- When you want better accuracy than a single decision tree\n",
    "- When you have enough data (works well with large datasets)\n",
    "- When you want feature importance rankings\n",
    "\n",
    "**Pros:** Usually more accurate than single trees, provides feature importance, handles missing values\n",
    "**Cons:** Less interpretable than single trees, can overfit with very noisy data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13309ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Fit a random forest for classification\n",
    "# set.seed(123)\n",
    "# rf_mod <- randomForest(y ~ ., data = train_data, ntree = 500, importance = TRUE, \n",
    "#                        mtry = sqrt(ncol(train_data) - 1))  # Standard mtry for classification\n",
    "# \n",
    "# print(rf_mod)\n",
    "# \n",
    "# # Plot variable importance\n",
    "# varImpPlot(rf_mod, main = \"Random Forest Feature Importance\")\n",
    "# \n",
    "# # Make predictions\n",
    "# rf_pred_class <- predict(rf_mod, test_data)\n",
    "# rf_pred_prob <- predict(rf_mod, test_data, type = \"prob\")[, \"yes\"]\n",
    "# \n",
    "# # Evaluate performance\n",
    "# rf_accuracy <- mean(rf_pred_class == test_data$y)\n",
    "# cat(\"\\nRandom Forest Test Accuracy:\", round(rf_accuracy, 3), \"\\n\")\n",
    "# \n",
    "# # Confusion matrix\n",
    "# rf_cm <- table(Predicted = rf_pred_class, Actual = test_data$y)\n",
    "# print(rf_cm)\n",
    "# \n",
    "# # Feature importance (top 10)\n",
    "# importance_rf <- importance(rf_mod)[, \"MeanDecreaseGini\"]\n",
    "# cat(\"\\nTop 10 Most Important Features (Random Forest):\\n\")\n",
    "# print(sort(importance_rf, decreasing = TRUE)[1:10])\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Fit a random forest for classification\n",
    "np.random.seed(123)\n",
    "rf_mod = randomForest(y ~ ., data = train_data, ntree = 500, importance = True, \n",
    "                       mtry = sqrt(train_data.shape[1] - 1))  # Standard mtry for classification\n",
    "\n",
    "print(rf_mod)\n",
    "\n",
    "# Plot variable importance\n",
    "varImpPlot(rf_mod, main = \"Random Forest Feature Importance\")\n",
    "\n",
    "# Make predictions\n",
    "rf_pred_class = predict(rf_mod, test_data)\n",
    "rf_pred_prob = predict(rf_mod, test_data, type = \"prob\")[, \"yes\"]\n",
    "\n",
    "# Evaluate performance\n",
    "rf_accuracy = mean(rf_pred_class == test_data$y)\n",
    "cat(\"\\nRandom Forest Test Accuracy:\", round(rf_accuracy, 3), \"\\n\")\n",
    "\n",
    "# Confusion matrix\n",
    "rf_cm = table(Predicted = rf_pred_class, Actual = test_data$y)\n",
    "print(rf_cm)\n",
    "\n",
    "# Feature importance (top 10)\n",
    "importance_rf = importance(rf_mod)[, \"MeanDecreaseGini\"]\n",
    "cat(\"\\nTop 10 Most Important Features (Random Forest):\\n\")\n",
    "print(sort(importance_rf, decreasing = True)[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9db6a",
   "metadata": {},
   "source": [
    "## 4. Boosting (using XGBoost)\n",
    "\n",
    "**What is Boosting?** Boosting builds models sequentially, where each new model tries to correct the mistakes of the previous models. XGBoost is a popular and powerful implementation of gradient boosting.\n",
    "\n",
    "**When to use Boosting:**\n",
    "- When you want state-of-the-art predictive performance\n",
    "- When you have sufficient data and computational resources\n",
    "- When accuracy is more important than interpretability\n",
    "\n",
    "**Pros:** Often achieves the best predictive performance, handles missing values, provides feature importance\n",
    "**Cons:** More complex to tune, less interpretable, can overfit easily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Prepare data for xgboost (needs numeric labels: 0/1)\n",
    "# Xmat_train <- model.matrix(y ~ ., train_data)[, -1]\n",
    "# Xmat_test <- model.matrix(y ~ ., test_data)[, -1]\n",
    "# y_train_numeric <- as.numeric(train_data$y) - 1  # Convert to 0/1\n",
    "# y_test_numeric <- as.numeric(test_data$y) - 1\n",
    "# \n",
    "# # Create DMatrix objects\n",
    "# dtrain <- xgb.DMatrix(data = Xmat_train, label = y_train_numeric)\n",
    "# dtest <- xgb.DMatrix(data = Xmat_test, label = y_test_numeric)\n",
    "# \n",
    "# # Set parameters\n",
    "# params <- list(\n",
    "#   objective = \"binary:logistic\",\n",
    "#   eval_metric = \"auc\",\n",
    "#   max_depth = 4,\n",
    "#   eta = 0.1,\n",
    "#   subsample = 0.8,\n",
    "#   colsample_bytree = 0.8\n",
    "# )\n",
    "# \n",
    "# # Train with cross-validation to find optimal number of rounds\n",
    "# cv_result <- xgb.cv(\n",
    "#   params = params,\n",
    "#   data = dtrain,\n",
    "#   nrounds = 200,\n",
    "#   nfold = 5,\n",
    "#   early_stopping_rounds = 10,\n",
    "#   verbose = 0\n",
    "# )\n",
    "# \n",
    "# best_nrounds <- cv_result$best_iteration\n",
    "# cat(\"Optimal number of rounds:\", best_nrounds, \"\\n\")\n",
    "# \n",
    "# # Fit final model\n",
    "# xgb_mod <- xgboost(\n",
    "#   params = params,\n",
    "#   data = dtrain,\n",
    "#   nrounds = best_nrounds,\n",
    "#   verbose = 0\n",
    "# )\n",
    "# \n",
    "# # Feature importance\n",
    "# importance <- xgb.importance(model = xgb_mod)\n",
    "# xgb.plot.importance(importance[1:15, ], main = \"XGBoost Feature Importance (Top 15)\")\n",
    "# \n",
    "# # Make predictions\n",
    "# xgb_pred_prob <- predict(xgb_mod, dtest)\n",
    "# xgb_pred_class <- ifelse(xgb_pred_prob > 0.5, \"yes\", \"no\")\n",
    "# \n",
    "# # Evaluate performance\n",
    "# xgb_accuracy <- mean(xgb_pred_class == test_data$y)\n",
    "# cat(\"\\nXGBoost Test Accuracy:\", round(xgb_accuracy, 3), \"\\n\")\n",
    "# \n",
    "# # Confusion matrix\n",
    "# xgb_cm <- table(Predicted = xgb_pred_class, Actual = test_data$y)\n",
    "# print(xgb_cm)\n",
    "# \n",
    "# # ROC curve and AUC\n",
    "# roc_obj <- roc(y_test_numeric, xgb_pred_prob)\n",
    "# plot(roc_obj, main = \"ROC Curve (XGBoost)\")\n",
    "# cat(\"\\nAUC:\", round(auc(roc_obj), 3), \"\\n\")\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Prepare data for xgboost (needs numeric labels: 0/1)\n",
    "Xmat_train = model.matrix(y ~ ., train_data)[, -1]\n",
    "Xmat_test = model.matrix(y ~ ., test_data)[, -1]\n",
    "y_train_numeric = as.numerilist(train_data$y) - 1  # Convert to 0/1\n",
    "y_test_numeric = as.numerilist(test_data$y) - 1\n",
    "\n",
    "# Create DMatrix objects\n",
    "dtrain = xgb.DMatrix(data = Xmat_train, label = y_train_numeric)\n",
    "dtest = xgb.DMatrix(data = Xmat_test, label = y_test_numeric)\n",
    "\n",
    "# Set parameters\n",
    "params = list(\n",
    "  objective = \"binary:logistic\",\n",
    "  eval_metric = \"auc\",\n",
    "  max_depth = 4,\n",
    "  eta = 0.1,\n",
    "  subsample = 0.8,\n",
    "  colsample_bytree = 0.8\n",
    ")\n",
    "\n",
    "# Train with cross-validation to find optimal number of rounds\n",
    "cv_result = xgb.cv(\n",
    "  params = params,\n",
    "  data = dtrain,\n",
    "  nrounds = 200,\n",
    "  nfold = 5,\n",
    "  early_stopping_rounds = 10,\n",
    "  verbose = 0\n",
    ")\n",
    "\n",
    "best_nrounds = cv_result$best_iteration\n",
    "cat(\"Optimal number of rounds:\", best_nrounds, \"\\n\")\n",
    "\n",
    "# Fit final model\n",
    "xgb_mod = xgboost(\n",
    "  params = params,\n",
    "  data = dtrain,\n",
    "  nrounds = best_nrounds,\n",
    "  verbose = 0\n",
    ")\n",
    "\n",
    "# Feature importance\n",
    "importance = xgb.importance(model = xgb_mod)\n",
    "xgb.plot.importance(importance[1:15, ], main = \"XGBoost Feature Importance (Top 15)\")\n",
    "\n",
    "# Make predictions\n",
    "xgb_pred_prob = predict(xgb_mod, dtest)\n",
    "xgb_pred_class = np.where(xgb_pred_prob > 0.5, \"yes\", \"no\")\n",
    "\n",
    "# Evaluate performance\n",
    "xgb_accuracy = mean(xgb_pred_class == test_data$y)\n",
    "cat(\"\\nXGBoost Test Accuracy:\", round(xgb_accuracy, 3), \"\\n\")\n",
    "\n",
    "# Confusion matrix\n",
    "xgb_cm = table(Predicted = xgb_pred_class, Actual = test_data$y)\n",
    "print(xgb_cm)\n",
    "\n",
    "# ROC curve and AUC\n",
    "roc_obj = rolist(y_test_numeric, xgb_pred_prob)\n",
    "# TODO: translate base R plot -> matplotlib\n",
    "# plot(roc_obj, main = \"ROC Curve (XGBoost)\")\n",
    "cat(\"\\nAUC:\", round(aulist(roc_obj), 3), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d547c26",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Summary\n",
    "\n",
    "Let's compare all our models to see which performs best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35049666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Create a comparison table\n",
    "# model_comparison <- data.frame(\n",
    "#   Model = c(\"Lasso\", \"Decision Tree\", \"Random Forest\", \"XGBoost\"),\n",
    "#   Accuracy = c(lasso_accuracy, tree_accuracy, rf_accuracy, xgb_accuracy),\n",
    "#   stringsAsFactors = FALSE\n",
    "# )\n",
    "# \n",
    "# model_comparison$Accuracy <- round(model_comparison$Accuracy, 3)\n",
    "# model_comparison <- model_comparison[order(model_comparison$Accuracy, decreasing = TRUE), ]\n",
    "# \n",
    "# cat(\"Model Performance Comparison (Test Set Accuracy):\\n\")\n",
    "# print(model_comparison)\n",
    "# \n",
    "# # Plot comparison\n",
    "# ggplot(model_comparison, aes(x = reorder(Model, Accuracy), y = Accuracy)) +\n",
    "#   geom_col(fill = \"steelblue\", alpha = 0.7) +\n",
    "#   geom_text(aes(label = Accuracy), hjust = -0.1) +\n",
    "#   coord_flip() +\n",
    "#   labs(title = \"Model Performance Comparison\", \n",
    "#        x = \"Model\", y = \"Test Accuracy\") +\n",
    "#   theme_minimal() +\n",
    "#   ylim(0, 1)\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Create a comparison table\n",
    "model_comparison = pd.DataFrame(\n",
    "  Model = list(\"Lasso\", \"Decision Tree\", \"Random Forest\", \"XGBoost\"),\n",
    "  Accuracy = list(lasso_accuracy, tree_accuracy, rf_accuracy, xgb_accuracy),\n",
    "  stringsAsFactors = False\n",
    ")\n",
    "\n",
    "model_comparison$Accuracy = round(model_comparison$Accuracy, 3)\n",
    "model_comparison = model_comparison[order(model_comparison$Accuracy, decreasing = True), ]\n",
    "\n",
    "cat(\"Model Performance Comparison (Test Set Accuracy):\\n\")\n",
    "print(model_comparison)\n",
    "\n",
    "# Plot comparison\n",
    "ggplot(model_comparison, aes(x = reorder(Model, Accuracy), y = Accuracy)) +\n",
    "  geom_col(fill = \"steelblue\", alpha = 0.7) +\n",
    "  geom_text(aes(label = Accuracy), hjust = -0.1) +\n",
    "  coord_flip() +\n",
    "  labs(title = \"Model Performance Comparison\", \n",
    "       x = \"Model\", y = \"Test Accuracy\") +\n",
    "  theme_minimal() +\n",
    "  ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4ac4d",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Model Characteristics Summary:\n",
    "\n",
    "1. **Lasso Regression**\n",
    "   - ✅ Automatic feature selection\n",
    "   - ✅ Highly interpretable\n",
    "   - ✅ Fast to train and predict\n",
    "   - ❌ Assumes linear relationships\n",
    "\n",
    "2. **Decision Trees**\n",
    "   - ✅ Most interpretable (visual flowchart)\n",
    "   - ✅ Handles non-linear relationships\n",
    "   - ✅ No assumptions about data distribution\n",
    "   - ❌ Prone to overfitting\n",
    "   - ❌ Unstable (small changes → different tree)\n",
    "\n",
    "3. **Random Forest**\n",
    "   - ✅ Usually more accurate than single trees\n",
    "   - ✅ Robust to overfitting\n",
    "   - ✅ Provides feature importance\n",
    "   - ✅ Handles missing values well\n",
    "   - ❌ Less interpretable than single trees\n",
    "\n",
    "4. **XGBoost**\n",
    "   - ✅ Often achieves best predictive performance\n",
    "   - ✅ Handles missing values\n",
    "   - ✅ Built-in regularization\n",
    "   - ❌ More complex to tune\n",
    "   - ❌ Least interpretable\n",
    "   - ❌ Can overfit if not tuned properly\n",
    "\n",
    "### Choosing the Right Model:\n",
    "\n",
    "- **Need interpretability?** → Decision Tree or Lasso\n",
    "- **Want good performance with minimal tuning?** → Random Forest\n",
    "- **Need maximum accuracy?** → XGBoost (with proper tuning)\n",
    "- **Have limited data?** → Lasso or Decision Tree\n",
    "- **Have lots of irrelevant features?** → Lasso or XGBoost\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different hyperparameters for each model\n",
    "2. Use cross-validation for more robust evaluation\n",
    "3. Consider ensemble methods (combining multiple models)\n",
    "4. Analyze feature importance to gain business insights\n",
    "5. Test models on new, unseen data before deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9474b00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Causal Machine Learning for Financial Decision Making\n",
    "\n",
    "## Why Causal Inference Matters in Finance\n",
    "\n",
    "**The Problem with Predictive Models:** Traditional ML models excel at prediction but struggle with **causation**. In finance, we often need to answer questions like:\n",
    "\n",
    "- \"What would happen if we **change** our interest rates?\"\n",
    "- \"What is the **causal effect** of a marketing campaign on customer acquisition?\"\n",
    "- \"How much **additional revenue** would we generate from a new product feature?\"\n",
    "\n",
    "**Correlation ≠ Causation:** Predictive models find patterns but can't tell us what happens when we intervene. Causal ML bridges this gap.\n",
    "\n",
    "## Key Concepts in Causal Machine Learning\n",
    "\n",
    "### 1. **Treatment Effects**\n",
    "- **Average Treatment Effect (ATE)**: Average impact of treatment across population\n",
    "- **Conditional Average Treatment Effect (CATE)**: Treatment effect for specific subgroups\n",
    "- **Individual Treatment Effect (ITE)**: Personalized treatment effects\n",
    "\n",
    "### 2. **Confounding**\n",
    "Variables that affect both treatment assignment and outcome, creating spurious correlations.\n",
    "\n",
    "### 3. **Identification Strategies**\n",
    "- **Randomized Experiments**: Gold standard but often impractical\n",
    "- **Natural Experiments**: Exploit random variation in real-world settings\n",
    "- **Instrumental Variables**: Use variables that affect treatment but not outcome directly\n",
    "- **Regression Discontinuity**: Exploit arbitrary cutoffs in treatment assignment\n",
    "\n",
    "## Modern Causal ML Methods\n",
    "\n",
    "### 1. **Double/Debiased Machine Learning (DML)**\n",
    "Combines ML flexibility with causal inference rigor by:\n",
    "- Using ML to model nuisance parameters\n",
    "- Applying cross-fitting to avoid overfitting bias\n",
    "- Providing valid confidence intervals\n",
    "\n",
    "### 2. **Causal Forests**\n",
    "Extension of Random Forests for heterogeneous treatment effects:\n",
    "- Estimates personalized treatment effects\n",
    "- Provides uncertainty quantification\n",
    "- Handles high-dimensional data\n",
    "\n",
    "### 3. **Meta-Learners**\n",
    "- **T-Learner**: Separate models for treated/control groups\n",
    "- **S-Learner**: Single model with treatment as feature\n",
    "- **X-Learner**: Combines T-learner predictions optimally\n",
    "- **R-Learner**: Uses residual-based approach\n",
    "\n",
    "## Financial Applications\n",
    "\n",
    "### 1. **Credit Risk & Lending**\n",
    "- **Question**: \"What's the causal effect of credit limit increases on default risk?\"\n",
    "- **Challenge**: Customers who get increases may be systematically different\n",
    "- **Solution**: Use causal ML to control for selection bias\n",
    "\n",
    "### 2. **Marketing & Customer Acquisition**\n",
    "- **Question**: \"Which customers benefit most from promotional offers?\"\n",
    "- **Application**: Personalized treatment effects for targeted campaigns\n",
    "- **Business Value**: Optimize marketing spend and ROI\n",
    "\n",
    "### 3. **Product Pricing**\n",
    "- **Question**: \"How do price changes affect demand across customer segments?\"\n",
    "- **Application**: Heterogeneous price elasticity estimation\n",
    "- **Business Value**: Dynamic pricing strategies\n",
    "\n",
    "### 4. **Regulatory Compliance**\n",
    "- **Question**: \"Do our algorithms create disparate impact across protected groups?\"\n",
    "- **Application**: Causal fairness analysis\n",
    "- **Business Value**: Ensure fair lending practices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d62d0b",
   "metadata": {},
   "source": [
    "## R Packages for Causal Machine Learning\n",
    "\n",
    "Let's explore the latest R packages for causal inference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8393d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Install and load causal ML packages\n",
    "# causal_packages <- c(\n",
    "#   \"grf\",           # Generalized Random Forests (Causal Forests)\n",
    "#   \"DoubleML\",      # Double/Debiased Machine Learning\n",
    "#   \"causalTree\",    # Causal Trees\n",
    "#   \"hdm\",           # High-dimensional econometrics\n",
    "#   \"ATE\",           # Average Treatment Effects\n",
    "#   \"causaldrf\",     # Causal Dose Response Functions\n",
    "#   \"BART\",          # Bayesian Additive Regression Trees\n",
    "#   \"tmle\",          # Targeted Maximum Likelihood Estimation\n",
    "#   \"SuperLearner\"   # Ensemble methods for causal inference\n",
    "# )\n",
    "# \n",
    "# for (pkg in causal_packages) {\n",
    "#   if (!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "#     cat(\"Installing package:\", pkg, \"\\n\")\n",
    "#     install.packages(pkg, quiet = TRUE)\n",
    "#     suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n",
    "#   }\n",
    "# }\n",
    "# \n",
    "# cat(\"Causal ML packages loaded successfully!\\n\")\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Install and load causal ML packages\n",
    "causal_packages = c(\n",
    "  \"grf\",           # Generalized Random Forests (Causal Forests)\n",
    "  \"DoubleML\",      # Double/Debiased Machine Learning\n",
    "  \"causalTree\",    # Causal Trees\n",
    "  \"hdm\",           # High-dimensional econometrics\n",
    "  \"ATE\",           # Average Treatment Effects\n",
    "  \"causaldrf\",     # Causal Dose Response Functions\n",
    "  \"BART\",          # Bayesian Additive Regression Trees\n",
    "  \"tmle\",          # Targeted Maximum Likelihood Estimation\n",
    "  \"SuperLearner\"   # Ensemble methods for causal inference\n",
    ")\n",
    "\n",
    "for (pkg in causal_packages) {\n",
    "  if (!require(pkg, character.only = True, quietly = True)) {\n",
    "    cat(\"Installing package:\", pkg, \"\\n\")\n",
    "    install.packages(pkg, quiet = True)\n",
    "    suppressPackageStartupMessages(# R: library(pkg, character.only = True) — install/import Python equivalents as needed)\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"Causal ML packages loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190c2ce",
   "metadata": {},
   "source": [
    "## Practical Example: Causal Analysis of Marketing Campaign\n",
    "\n",
    "Let's demonstrate causal ML using our banking dataset. We'll analyze the causal effect of previous marketing campaigns (`poutcome`) on subscription decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Prepare data for causal analysis\n",
    "# # Create a binary treatment variable: previous campaign success\n",
    "# causal_data <- data.table::copy(data)\n",
    "# causal_data[, treatment := ifelse(poutcome == \"success\", 1, 0)]\n",
    "# causal_data[, outcome := ifelse(y == \"yes\", 1, 0)]\n",
    "# \n",
    "# # Remove the original variables to avoid data leakage\n",
    "# causal_data[, c(\"poutcome\", \"y\") := NULL]\n",
    "# \n",
    "# # Convert factors to numeric for some methods\n",
    "# factor_cols <- sapply(causal_data, is.factor)\n",
    "# causal_data_numeric <- causal_data\n",
    "# causal_data_numeric[, (names(factor_cols)[factor_cols]) := lapply(.SD, as.numeric), .SDcols = names(factor_cols)[factor_cols]]\n",
    "# \n",
    "# cat(\"Treatment distribution:\\n\")\n",
    "# table(causal_data$treatment)\n",
    "# cat(\"\\nOutcome by treatment:\\n\")\n",
    "# table(causal_data$treatment, causal_data$outcome)\n",
    "# \n",
    "# # Simple difference in means (biased estimate)\n",
    "# naive_ate <- mean(causal_data[treatment == 1, outcome]) - mean(causal_data[treatment == 0, outcome])\n",
    "# cat(\"\\nNaive ATE (difference in means):\", round(naive_ate, 3), \"\\n\")\n",
    "# cat(\"This is likely biased due to confounding!\\n\")\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Prepare data for causal analysis\n",
    "# Create a binary treatment variable: previous campaign success\n",
    "causal_data = data.table::copy(data)\n",
    "causal_data[, treatment := np.where(poutcome == \"success\", 1, 0)]\n",
    "causal_data[, outcome := np.where(y == \"yes\", 1, 0)]\n",
    "\n",
    "# Remove the original variables to avoid data leakage\n",
    "causal_data[, list(\"poutcome\", \"y\") := None]\n",
    "\n",
    "# Convert factors to numeric for some methods\n",
    "factor_cols = sapply(causal_data, is.factor)\n",
    "causal_data_numeric = causal_data\n",
    "causal_data_numeric[, (names(factor_cols)[factor_cols]) := lapply(.SD, as.numeric), .SDcols = names(factor_cols)[factor_cols]]\n",
    "\n",
    "cat(\"Treatment distribution:\\n\")\n",
    "table(causal_data$treatment)\n",
    "cat(\"\\nOutcome by treatment:\\n\")\n",
    "table(causal_data$treatment, causal_data$outcome)\n",
    "\n",
    "# Simple difference in means (biased estimate)\n",
    "naive_ate = mean(causal_data[treatment == 1, outcome]) - mean(causal_data[treatment == 0, outcome])\n",
    "cat(\"\\nNaive ATE (difference in means):\", round(naive_ate, 3), \"\\n\")\n",
    "cat(\"This is likely biased due to confounding!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbeac1",
   "metadata": {},
   "source": [
    "### Method 1: Causal Forest (Generalized Random Forest)\n",
    "\n",
    "Causal Forests estimate heterogeneous treatment effects while controlling for confounding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4171702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Prepare data for causal forest\n",
    "# X <- as.matrix(causal_data_numeric[, !c(\"treatment\", \"outcome\")])\n",
    "# Y <- causal_data_numeric$outcome\n",
    "# W <- causal_data_numeric$treatment\n",
    "# \n",
    "# # Fit causal forest\n",
    "# set.seed(123)\n",
    "# cf <- causal_forest(X, Y, W, num.trees = 2000)\n",
    "# \n",
    "# # Get average treatment effect\n",
    "# ate_cf <- average_treatment_effect(cf)\n",
    "# cat(\"Causal Forest ATE:\", round(ate_cf[1], 3), \"\\n\")\n",
    "# cat(\"95% CI: [\", round(ate_cf[1] - 1.96 * ate_cf[2], 3), \", \", \n",
    "#     round(ate_cf[1] + 1.96 * ate_cf[2], 3), \"]\\n\")\n",
    "# \n",
    "# # Get individual treatment effects\n",
    "# tau_hat <- predict(cf)$predictions\n",
    "# \n",
    "# # Analyze heterogeneity\n",
    "# cat(\"\\nTreatment Effect Heterogeneity:\\n\")\n",
    "# cat(\"Mean ITE:\", round(mean(tau_hat), 3), \"\\n\")\n",
    "# cat(\"Std Dev ITE:\", round(sd(tau_hat), 3), \"\\n\")\n",
    "# cat(\"Min ITE:\", round(min(tau_hat), 3), \"\\n\")\n",
    "# cat(\"Max ITE:\", round(max(tau_hat), 3), \"\\n\")\n",
    "# \n",
    "# # Plot distribution of treatment effects\n",
    "# hist(tau_hat, breaks = 30, main = \"Distribution of Individual Treatment Effects\",\n",
    "#      xlab = \"Treatment Effect\", col = \"lightblue\", border = \"white\")\n",
    "# abline(v = mean(tau_hat), col = \"red\", lwd = 2, lty = 2)\n",
    "# legend(\"topright\", \"Mean ITE\", col = \"red\", lty = 2, lwd = 2)\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Prepare data for causal forest\n",
    "X = as.matrix(causal_data_numeric[, !list(\"treatment\", \"outcome\")])\n",
    "Y = causal_data_numeric$outcome\n",
    "W = causal_data_numeric$treatment\n",
    "\n",
    "# Fit causal forest\n",
    "np.random.seed(123)\n",
    "cf = causal_forest(X, Y, W, num.trees = 2000)\n",
    "\n",
    "# Get average treatment effect\n",
    "ate_cf = average_treatment_effect(cf)\n",
    "cat(\"Causal Forest ATE:\", round(ate_cf[1], 3), \"\\n\")\n",
    "cat(\"95% CI: [\", round(ate_cf[1] - 1.96 * ate_cf[2], 3), \", \", \n",
    "    round(ate_cf[1] + 1.96 * ate_cf[2], 3), \"]\\n\")\n",
    "\n",
    "# Get individual treatment effects\n",
    "tau_hat = predict(cf)$predictions\n",
    "\n",
    "# Analyze heterogeneity\n",
    "cat(\"\\nTreatment Effect Heterogeneity:\\n\")\n",
    "cat(\"Mean ITE:\", round(mean(tau_hat), 3), \"\\n\")\n",
    "cat(\"Std Dev ITE:\", round(sd(tau_hat), 3), \"\\n\")\n",
    "cat(\"Min ITE:\", round(min(tau_hat), 3), \"\\n\")\n",
    "cat(\"Max ITE:\", round(max(tau_hat), 3), \"\\n\")\n",
    "\n",
    "# Plot distribution of treatment effects\n",
    "hist(tau_hat, breaks = 30, main = \"Distribution of Individual Treatment Effects\",\n",
    "     xlab = \"Treatment Effect\", col = \"lightblue\", border = \"white\")\n",
    "abline(v = mean(tau_hat), col = \"red\", lwd = 2, lty = 2)\n",
    "legend(\"topright\", \"Mean ITE\", col = \"red\", lty = 2, lwd = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e910a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Variable importance for treatment effect heterogeneity\n",
    "# var_imp <- variable_importance(cf)\n",
    "# var_names <- colnames(X)\n",
    "# \n",
    "# # Create importance data frame\n",
    "# importance_df <- data.frame(\n",
    "#   Variable = var_names,\n",
    "#   Importance = var_imp,\n",
    "#   stringsAsFactors = FALSE\n",
    "# )\n",
    "# importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]\n",
    "# \n",
    "# cat(\"\\nTop 10 Variables for Treatment Effect Heterogeneity:\\n\")\n",
    "# print(head(importance_df, 10))\n",
    "# \n",
    "# # Plot variable importance\n",
    "# top_vars <- head(importance_df, 10)\n",
    "# barplot(top_vars$Importance, names.arg = top_vars$Variable, \n",
    "#         main = \"Variable Importance for Treatment Effect Heterogeneity\",\n",
    "#         las = 2, cex.names = 0.8, col = \"steelblue\")\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Variable importance for treatment effect heterogeneity\n",
    "var_imp = variable_importance(cf)\n",
    "var_names = colnames(X)\n",
    "\n",
    "# Create importance data frame\n",
    "importance_df = pd.DataFrame(\n",
    "  Variable = var_names,\n",
    "  Importance = var_imp,\n",
    "  stringsAsFactors = False\n",
    ")\n",
    "importance_df = importance_df[order(importance_df$Importance, decreasing = True), ]\n",
    "\n",
    "cat(\"\\nTop 10 Variables for Treatment Effect Heterogeneity:\\n\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Plot variable importance\n",
    "top_vars = importance_df.head(10)\n",
    "barplot(top_vars$Importance, names.arg = top_vars$Variable, \n",
    "        main = \"Variable Importance for Treatment Effect Heterogeneity\",\n",
    "        las = 2, cex.names = 0.8, col = \"steelblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d1e41",
   "metadata": {},
   "source": [
    "### Method 2: Double Machine Learning (DML)\n",
    "\n",
    "DML provides robust causal estimates by using ML to control for confounding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18077ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # Note: DoubleML package requires Python backend\n",
    "# # Here we'll demonstrate the concept using manual implementation\n",
    "# \n",
    "# # Step 1: Predict treatment (propensity score)\n",
    "# ps_model <- randomForest(as.factor(treatment) ~ ., \n",
    "#                         data = causal_data_numeric[, !\"outcome\"], \n",
    "#                         ntree = 500)\n",
    "# ps_hat <- predict(ps_model, type = \"prob\")[, \"1\"]\n",
    "# \n",
    "# # Step 2: Predict outcome\n",
    "# outcome_model <- randomForest(outcome ~ ., \n",
    "#                              data = causal_data_numeric[, !\"treatment\"], \n",
    "#                              ntree = 500)\n",
    "# y_hat <- predict(outcome_model)\n",
    "# \n",
    "# # Step 3: Compute residuals\n",
    "# y_residual <- Y - y_hat\n",
    "# w_residual <- W - ps_hat\n",
    "# \n",
    "# # Step 4: Estimate ATE using residuals\n",
    "# ate_dml <- sum(w_residual * y_residual) / sum(w_residual^2)\n",
    "# \n",
    "# cat(\"Double ML ATE estimate:\", round(ate_dml, 3), \"\\n\")\n",
    "# \n",
    "# # Compare propensity scores by treatment group\n",
    "# cat(\"\\nPropensity Score Balance Check:\\n\")\n",
    "# cat(\"Mean PS (Treated):\", round(mean(ps_hat[W == 1]), 3), \"\\n\")\n",
    "# cat(\"Mean PS (Control):\", round(mean(ps_hat[W == 0]), 3), \"\\n\")\n",
    "# \n",
    "# # Plot propensity score distribution\n",
    "# hist(ps_hat[W == 0], breaks = 20, col = rgb(1,0,0,0.5), \n",
    "#      main = \"Propensity Score Distribution\", xlab = \"Propensity Score\",\n",
    "#      xlim = c(0, 1))\n",
    "# hist(ps_hat[W == 1], breaks = 20, col = rgb(0,0,1,0.5), add = TRUE)\n",
    "# legend(\"topright\", c(\"Control\", \"Treated\"), fill = c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)))\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# Note: DoubleML package requires Python backend\n",
    "# Here we'll demonstrate the concept using manual implementation\n",
    "\n",
    "# Step 1: Predict treatment (propensity score)\n",
    "ps_model = randomForest((treatment).astype(\"category\") ~ ., \n",
    "                        data = causal_data_numeric[, !\"outcome\"], \n",
    "                        ntree = 500)\n",
    "ps_hat = predict(ps_model, type = \"prob\")[, \"1\"]\n",
    "\n",
    "# Step 2: Predict outcome\n",
    "outcome_model = randomForest(outcome ~ ., \n",
    "                             data = causal_data_numeric[, !\"treatment\"], \n",
    "                             ntree = 500)\n",
    "y_hat = predict(outcome_model)\n",
    "\n",
    "# Step 3: Compute residuals\n",
    "y_residual = Y - y_hat\n",
    "w_residual = W - ps_hat\n",
    "\n",
    "# Step 4: Estimate ATE using residuals\n",
    "ate_dml = sum(w_residual * y_residual) / sum(w_residual^2)\n",
    "\n",
    "cat(\"Double ML ATE estimate:\", round(ate_dml, 3), \"\\n\")\n",
    "\n",
    "# Compare propensity scores by treatment group\n",
    "cat(\"\\nPropensity Score Balance Check:\\n\")\n",
    "cat(\"Mean PS (Treated):\", round(mean(ps_hat[W == 1]), 3), \"\\n\")\n",
    "cat(\"Mean PS (Control):\", round(mean(ps_hat[W == 0]), 3), \"\\n\")\n",
    "\n",
    "# Plot propensity score distribution\n",
    "hist(ps_hat[W == 0], breaks = 20, col = rgb(1,0,0,0.5), \n",
    "     main = \"Propensity Score Distribution\", xlab = \"Propensity Score\",\n",
    "     xlim = list(0, 1))\n",
    "hist(ps_hat[W == 1], breaks = 20, col = rgb(0,0,1,0.5), add = True)\n",
    "legend(\"topright\", list(\"Control\", \"Treated\"), fill = c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c698aec",
   "metadata": {},
   "source": [
    "### Method 3: Meta-Learners (T-Learner and X-Learner)\n",
    "\n",
    "Meta-learners use different strategies to combine ML models for causal inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ca38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Original R code (commented) ---\n",
    "# # T-Learner: Separate models for treated and control\n",
    "# treated_data <- causal_data_numeric[treatment == 1, !c(\"treatment\")]\n",
    "# control_data <- causal_data_numeric[treatment == 0, !c(\"treatment\")]\n",
    "# \n",
    "# # Fit separate models\n",
    "# model_treated <- randomForest(outcome ~ ., data = treated_data, ntree = 500)\n",
    "# model_control <- randomForest(outcome ~ ., data = control_data, ntree = 500)\n",
    "# \n",
    "# # Predict for all observations\n",
    "# X_all <- causal_data_numeric[, !c(\"treatment\", \"outcome\")]\n",
    "# mu1_hat <- predict(model_treated, X_all)  # Potential outcome if treated\n",
    "# mu0_hat <- predict(model_control, X_all)  # Potential outcome if control\n",
    "# \n",
    "# # Individual treatment effects\n",
    "# tau_tlearner <- mu1_hat - mu0_hat\n",
    "# ate_tlearner <- mean(tau_tlearner)\n",
    "# \n",
    "# cat(\"T-Learner ATE:\", round(ate_tlearner, 3), \"\\n\")\n",
    "# cat(\"T-Learner ITE std dev:\", round(sd(tau_tlearner), 3), \"\\n\")\n",
    "# \n",
    "# # Compare methods\n",
    "# comparison_df <- data.frame(\n",
    "#   Method = c(\"Naive\", \"Causal Forest\", \"Double ML\", \"T-Learner\"),\n",
    "#   ATE = c(naive_ate, ate_cf[1], ate_dml, ate_tlearner),\n",
    "#   stringsAsFactors = FALSE\n",
    "# )\n",
    "# comparison_df$ATE <- round(comparison_df$ATE, 3)\n",
    "# \n",
    "# cat(\"\\nMethod Comparison:\\n\")\n",
    "# print(comparison_df)\n",
    "# \n",
    "# # Plot comparison\n",
    "# barplot(comparison_df$ATE, names.arg = comparison_df$Method,\n",
    "#         main = \"Average Treatment Effect Estimates\",\n",
    "#         ylab = \"ATE\", col = c(\"red\", \"green\", \"blue\", \"orange\"),\n",
    "#         las = 2)\n",
    "# abline(h = 0, lty = 2)\n",
    "# \n",
    "\n",
    "# --- Naive Python translation (manual review recommended) ---\n",
    "# T-Learner: Separate models for treated and control\n",
    "treated_data = causal_data_numeric[treatment == 1, !list(\"treatment\")]\n",
    "control_data = causal_data_numeric[treatment == 0, !list(\"treatment\")]\n",
    "\n",
    "# Fit separate models\n",
    "model_treated = randomForest(outcome ~ ., data = treated_data, ntree = 500)\n",
    "model_control = randomForest(outcome ~ ., data = control_data, ntree = 500)\n",
    "\n",
    "# Predict for all observations\n",
    "X_all = causal_data_numeric[, !list(\"treatment\", \"outcome\")]\n",
    "mu1_hat = predict(model_treated, X_all)  # Potential outcome if treated\n",
    "mu0_hat = predict(model_control, X_all)  # Potential outcome if control\n",
    "\n",
    "# Individual treatment effects\n",
    "tau_tlearner = mu1_hat - mu0_hat\n",
    "ate_tlearner = mean(tau_tlearner)\n",
    "\n",
    "cat(\"T-Learner ATE:\", round(ate_tlearner, 3), \"\\n\")\n",
    "cat(\"T-Learner ITE std dev:\", round(sd(tau_tlearner), 3), \"\\n\")\n",
    "\n",
    "# Compare methods\n",
    "comparison_df = pd.DataFrame(\n",
    "  Method = list(\"Naive\", \"Causal Forest\", \"Double ML\", \"T-Learner\"),\n",
    "  ATE = list(naive_ate, ate_cf[1], ate_dml, ate_tlearner),\n",
    "  stringsAsFactors = False\n",
    ")\n",
    "comparison_df$ATE = round(comparison_df$ATE, 3)\n",
    "\n",
    "cat(\"\\nMethod Comparison:\\n\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot comparison\n",
    "barplot(comparison_df$ATE, names.arg = comparison_df$Method,\n",
    "        main = \"Average Treatment Effect Estimates\",\n",
    "        ylab = \"ATE\", col = list(\"red\", \"green\", \"blue\", \"orange\"),\n",
    "        las = 2)\n",
    "abline(h = 0, lty = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a50c27",
   "metadata": {},
   "source": [
    "## Business Insights and Recommendations\n",
    "\n",
    "### Interpreting the Results\n",
    "\n",
    "1. **Naive vs. Causal Estimates**: The difference between naive and causal estimates reveals the extent of confounding bias\n",
    "\n",
    "2. **Treatment Effect Heterogeneity**: Individual treatment effects vary significantly, suggesting personalized strategies could be valuable\n",
    "\n",
    "3. **Key Drivers**: Variables important for treatment effect heterogeneity identify customer segments that respond differently to campaigns\n",
    "\n",
    "### Financial Decision Making Applications\n",
    "\n",
    "#### 1. **Personalized Marketing**\n",
    "```r\n",
    "# Identify high-response customers\n",
    "high_responders <- which(tau_hat > quantile(tau_hat, 0.8))\n",
    "# Target future campaigns to these customers\n",
    "```\n",
    "\n",
    "#### 2. **Resource Allocation**\n",
    "- **Budget Optimization**: Focus marketing spend on customers with highest predicted treatment effects\n",
    "- **Channel Selection**: Use causal analysis to determine most effective communication channels\n",
    "\n",
    "#### 3. **Risk Management**\n",
    "- **Adverse Selection**: Identify if certain interventions attract riskier customers\n",
    "- **Regulatory Compliance**: Ensure marketing strategies don't create unfair disparate impact\n",
    "\n",
    "#### 4. **Product Development**\n",
    "- **Feature Testing**: Use causal ML to evaluate impact of new product features\n",
    "- **Pricing Strategy**: Estimate causal price elasticity across customer segments\n",
    "\n",
    "### Implementation Checklist\n",
    "\n",
    "✅ **Data Requirements**\n",
    "- Sufficient variation in treatment assignment\n",
    "- Rich set of pre-treatment covariates\n",
    "- Clear temporal ordering (cause before effect)\n",
    "\n",
    "✅ **Model Validation**\n",
    "- Check overlap in propensity scores\n",
    "- Test for balance in covariates\n",
    "- Validate using randomized experiments when possible\n",
    "\n",
    "✅ **Business Integration**\n",
    "- Translate causal estimates into business metrics (ROI, CLV)\n",
    "- Build decision support tools for stakeholders\n",
    "- Monitor performance of causal-ML-driven decisions\n",
    "\n",
    "### Latest Developments (2024/25)\n",
    "\n",
    "1. **Causal Representation Learning**: Deep learning methods for causal inference\n",
    "2. **Federated Causal Inference**: Privacy-preserving causal analysis across institutions\n",
    "3. **Causal Fairness**: Ensuring algorithmic decisions are causally fair\n",
    "4. **Dynamic Treatment Regimes**: Optimal sequential decision making\n",
    "\n",
    "### Recommended Reading\n",
    "\n",
    "- **Books**: \"Causal Inference: The Mixtape\" by Scott Cunningham\n",
    "- **Papers**: Athey & Imbens (2019) on Machine Learning for Causal Inference\n",
    "- **R Packages**: `grf`, `DoubleML`, `causalTree` documentation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
