{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Predictive Models in R: Lasso, Trees, Random Forests, and Boosting\n",
        "\n",
        "---\n",
        "This notebook demonstrates how to use more advanced machine learning models in R, including:\n",
        "- **Lasso regression (linear and logistic, using the `gamlr` package)**\n",
        "- **Decision Trees**\n",
        "- **Random Forests**\n",
        "- **Boosting**\n",
        "\n",
        "We will use the `banking.csv` dataset as an example. Please ensure you have the required packages installed:\n",
        "\n",
        "```r\n",
        "install.packages(c(\"gamlr\", \"rpart\", \"randomForest\", \"xgboost\", \"caret\", \"pROC\"))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load libraries\n",
        "library(gamlr)\n",
        "library(rpart)\n",
        "library(randomForest)\n",
        "library(xgboost)\n",
        "library(caret)\n",
        "library(pROC)\n",
        "library(data.table)\n",
        "library(ggplot2)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load data\n",
        "data <- fread(\"banking.csv\")\n",
        "str(data)\n",
        "summary(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Lasso Regression (Linear and Logistic, using `gamlr`)\n",
        "\n",
        "### Linear Lasso: Predicting a continuous variable (e.g., age)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare data for linear lasso\n",
        "X <- model.matrix(age ~ . - y, data)[, -1]\n",
        "y_age <- data$age\n",
        "\n",
        "# Fit lasso regression\n",
        "lasso_mod <- gamlr(X, y_age)\n",
        "plot(lasso_mod)\n",
        "\n",
        "# Cross-validated lasso\n",
        "cv_lasso <- cv.gamlr(X, y_age)\n",
        "plot(cv_lasso)\n",
        "coef(cv_lasso, select = \"min\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Lasso: Predicting a binary outcome (e.g., y)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare data for logistic lasso\n",
        "X_bin <- model.matrix(y ~ . - age, data)[, -1]\n",
        "y_bin <- as.factor(data$y)\n",
        "\n",
        "# Fit logistic lasso\n",
        "lasso_logit <- gamlr(X_bin, y_bin, family = \"binomial\")\n",
        "plot(lasso_logit)\n",
        "\n",
        "# Cross-validated logistic lasso\n",
        "cv_lasso_logit <- cv.gamlr(X_bin, y_bin, family = \"binomial\")\n",
        "plot(cv_lasso_logit)\n",
        "coef(cv_lasso_logit, select = \"min\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Decision Trees\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit a decision tree for classification\n",
        "tree_mod <- rpart(y ~ . - age, data = data, method = \"class\")\n",
        "plot(tree_mod, uniform=TRUE, margin=0.1)\n",
        "text(tree_mod, use.n=TRUE, all=TRUE, cex=.8)\n",
        "printcp(tree_mod)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Random Forests\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit a random forest for classification\n",
        "set.seed(123)\n",
        "rf_mod <- randomForest(as.factor(y) ~ . - age, data = data, ntree = 200, importance = TRUE)\n",
        "print(rf_mod)\n",
        "varImpPlot(rf_mod)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Boosting (using xgboost)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare data for xgboost\n",
        "Xmat <- model.matrix(y ~ . - age, data)[, -1]\n",
        "yvec <- as.numeric(data$y)\n",
        "\n",
        "# Split into train/test\n",
        "set.seed(123)\n",
        "train_idx <- sample(seq_len(nrow(Xmat)), size = 0.8 * nrow(Xmat))\n",
        "dtrain <- xgb.DMatrix(data = Xmat[train_idx, ], label = yvec[train_idx])\n",
        "dtest <- xgb.DMatrix(data = Xmat[-train_idx, ], label = yvec[-train_idx])\n",
        "\n",
        "# Fit xgboost model\n",
        "xgb_mod <- xgboost(data = dtrain, max.depth = 4, eta = 0.1, nrounds = 100, objective = \"binary:logistic\", eval_metric = \"auc\", verbose = 0)\n",
        "\n",
        "# Feature importance\n",
        "importance <- xgb.importance(model = xgb_mod)\n",
        "xgb.plot.importance(importance)\n",
        "\n",
        "# Predict and evaluate\n",
        "preds <- predict(xgb_mod, dtest)\n",
        "roc_obj <- roc(yvec[-train_idx], preds)\n",
        "plot(roc_obj, main = \"ROC Curve (Boosting)\")\n",
        "auc(roc_obj)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Comparison and Summary\n",
        "\n",
        "You can compare models using cross-validation, ROC/AUC, or other metrics as appropriate for your problem.\n",
        "\n",
        "This notebook provides a template for using advanced models in R. For your own data, adjust the target variable and predictors as needed."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
