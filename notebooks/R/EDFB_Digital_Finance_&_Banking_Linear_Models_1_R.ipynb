{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-cell",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/umatter/EDFB/blob/main/notebooks/R/EDFB_Digital_Finance_%26_Banking_Linear_Models_1_R.ipynb)\n",
        "\n",
        "# Linear Regression: Univariate\n",
        "\n",
        "---\n",
        "This notebook demonstrates how to train and test a linear regression model in R, using tidyverse and modern R best practices. The examples and explanations are designed to mirror the Python version, so you can compare the two approaches side by side.\n",
        "\n",
        "If you are new to regression: linear regression models a numeric target `y` as a straight-line relationship with one (or more) predictor(s) `x`. In its simplest form: `y ≈ intercept + slope * x`. We fit the parameters (intercept, slope) so the line best matches the observed data.\n",
        "\n",
        "If you are new to R: we use the tidyverse pipe (`%>%`) to express a sequence of steps left-to-right (e.g., \"take this data, then filter, then plot\"), and we rely on `tibble` data frames, which print cleanly and behave like standard data frames.\n",
        "\n",
        "## What you'll learn\n",
        "\n",
        "- How to generate and visualize linear data in R\n",
        "- How to fit a linear regression model using `lm()`\n",
        "- How to make predictions and interpret model coefficients\n",
        "- How to use real-world data (e.g., `mtcars`)\n",
        "- How to split data into train/test sets and evaluate model performance\n",
        "- How to check model assumptions (linearity, homoscedasticity, normality, independence)\n",
        "- How to use time series data and create lagged features for forecasting\n",
        "\n",
        "## Libraries\n",
        "\n",
        "- **tidyverse**: For data manipulation and visualization (dplyr, ggplot2, readr, tibble, etc.)\n",
        "- **broom**: For tidying model outputs\n",
        "- **caret**: For train/test splitting and metrics\n",
        "- **lubridate**: For date handling (if needed)\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "> **Note:** This notebook is heavily commented and includes explanations for each step, just like the Python version. If you are new to R, pay attention to the code comments and markdown cells for guidance. We also explain what each function is doing and why each step is needed.\n",
        ">\n",
        "> **Tested on:** R 4.3+ (Ubuntu 22.04 Colab VM), tidyverse/broom/caret/lubridate from Posit Package Manager."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "libraries-cell",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Setup (self-bootstrapping): set a binary CRAN mirror and install/load required packages\n",
        "options(repos = c(CRAN = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\"))\n",
        "packages <- c(\"tidyverse\", \"broom\", \"caret\", \"lubridate\", \"reshape2\", \"gridExtra\")\n",
        "new_packages <- packages[!(packages %in% installed.packages()[, \"Package\"])]\n",
        "if (length(new_packages) > 0) {\n",
        "  cat(\"Installing packages:\", paste(new_packages, collapse = \", \"), \"\\n\")\n",
        "  install.packages(new_packages, dependencies = TRUE, quiet = TRUE)\n",
        "}\n",
        "\n",
        "# Load packages with error handling and print versions (preflight)\n",
        "for (pkg in packages) {\n",
        "  if (!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
        "    stop(paste(\"Failed to load package:\", pkg))\n",
        "  }\n",
        "}\n",
        "cat(\"\\nR version:\", R.version.string, \"\\n\")\n",
        "cat(\"Loaded package versions:\\n\")\n",
        "cat(sprintf(\"- %s %s\\n\", packages, as.character(packageVersion(packages))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "generate-data-intro",
      "metadata": {},
      "source": [
        "## Generate Linear Data Example\n",
        "\n",
        "Let's generate some linear-looking data, similar to the Python example. We'll use `runif` for uniform random numbers (x values) and `rnorm` for normally distributed noise added to the line (this makes the points not perfectly on the line, which is realistic). This section helps you see what a \"linear\" pattern looks like before we fit a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "generate-data",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Set a random seed for reproducibility (so results are the same each run)\n",
        "set.seed(42)\n",
        "n <- 100\n",
        "# Generate 100 random x values between 0 and 2\n",
        "X <- tibble(x = runif(n, 0, 2))\n",
        "# Generate y values with a linear relationship plus some random noise\n",
        "y <- 4 + 3 * X$x + rnorm(n)\n",
        "# Combine x and y into a single data frame\n",
        "df <- X %>% mutate(y = y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visualize-data",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Visualize the data\n",
        "ggplot(df, aes(x = x, y = y)) +\n",
        "  geom_point(color = 'blue') +\n",
        "  labs(x = 'x', y = 'y', title = 'Simulated Linear Data') +\n",
        "  theme_minimal()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fit-model-intro",
      "metadata": {},
      "source": [
        "## Fit a Linear Model\n",
        "\n",
        "We'll use `lm()` to fit a linear regression model. In R, `lm(y ~ x, data = df)` means \"model y as a function of x\". The output includes the estimated intercept and slope, as well as diagnostics such as R-squared (how much variance in y is explained by x)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fit-model",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Fit a linear regression model: y = intercept + slope * x\n",
        "model <- lm(y ~ x, data = df)\n",
        "# Show a summary of the model (coefficients, R-squared, etc.)\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-fit",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Add predictions to the data frame\n",
        "df <- df %>% mutate(y_pred = predict(model, newdata = df))\n",
        "\n",
        "# Plot data and fitted line\n",
        "ggplot(df, aes(x = x, y = y)) +\n",
        "  geom_point(color = 'blue') +\n",
        "  geom_line(aes(y = y_pred), color = 'red') +\n",
        "  labs(x = 'x', y = 'y', title = 'Linear Fit') +\n",
        "  theme_minimal()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "predict-intro",
      "metadata": {},
      "source": [
        "## Predict New Values\n",
        "\n",
        "Let's predict for new x values. In R, `predict(model, newdata = ...)` takes a data frame with the same column names used in the formula (here, `x`) and returns predicted `y` values using the fitted intercept and slope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "predict-new",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Predict y for new x values using the fitted model\n",
        "X_new <- tibble(x = c(0.5, 1.75))\n",
        "y_new_pred <- predict(model, newdata = X_new)\n",
        "# Show the predicted values\n",
        "tibble(x = X_new$x, y_pred = y_new_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "real-data-intro",
      "metadata": {},
      "source": [
        "## Real Data Example: Import and Explore\n",
        "\n",
        "Let's read a real dataset. Here we use the built-in `mtcars` dataset as a stand-in to keep this notebook runnable everywhere (the Python version loads an external CSV). The same ideas apply: look at the data structure, check for missing values, and examine summary statistics before modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-data",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Example: Use built-in mtcars dataset for demonstration\n",
        "# (In practice, you would load your own CSV file here)\n",
        "data <- as_tibble(mtcars)\n",
        "# Show the first few rows of the data\n",
        "data %>% head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "explore-intro",
      "metadata": {},
      "source": [
        "## Data Exploration\n",
        "\n",
        "Let's check the structure, missing values, and summary statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "explore-data",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Check the structure of the data (column types, etc.)\n",
        "glimpse(data)\n",
        "# Check for missing values in each column\n",
        "colSums(is.na(data))\n",
        "# Show summary statistics for each column\n",
        "summary(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "simple-regression-intro",
      "metadata": {},
      "source": [
        "## Simple Linear Regression Example\n",
        "\n",
        "Let's predict `mpg` (miles per gallon) from `hp` (horsepower) as a univariate regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-mpg-hp",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Visualize the relationship between horsepower (hp) and miles per gallon (mpg)\n",
        "ggplot(data, aes(x = hp, y = mpg)) +\n",
        "  geom_point() +\n",
        "  theme_minimal()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fit-mpg-model",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Fit a simple linear regression: mpg ~ hp\n",
        "model2 <- lm(mpg ~ hp, data = data)\n",
        "# Show model summary\n",
        "summary(model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-mpg-fit",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Add predictions to the data frame and plot the fit\n",
        "data <- data %>% mutate(mpg_pred = predict(model2, newdata = data))\n",
        "ggplot(data, aes(x = hp, y = mpg)) +\n",
        "  geom_point(color = 'blue') +\n",
        "  geom_line(aes(y = mpg_pred), color = 'red') +\n",
        "  theme_minimal()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "train-test-intro",
      "metadata": {},
      "source": [
        "## Train/Test Split and Model Evaluation\n",
        "\n",
        "We'll use `caret::createDataPartition` to split the data. The model will be trained on the training set and evaluated on the held-out test set. We report:\n",
        "- **RMSE** (Root Mean Squared Error): typical size of prediction errors (lower is better)\n",
        "- **R-squared**: proportion of variance explained (closer to 1 is better)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train-test-split",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets (80% train, 20% test)\n",
        "set.seed(123)\n",
        "train_idx <- createDataPartition(data$mpg, p = 0.8, list = FALSE)\n",
        "train <- data[train_idx, ]\n",
        "test <- data[-train_idx, ]\n",
        "\n",
        "# Fit the model on the training set\n",
        "model3 <- lm(mpg ~ hp, data = train)\n",
        "# Predict on the test set\n",
        "test <- test %>% mutate(mpg_pred = predict(model3, newdata = test))\n",
        "\n",
        "# Calculate RMSE (Root Mean Squared Error) and R-squared\n",
        "rmse <- sqrt(mean((test$mpg - test$mpg_pred)^2))\n",
        "r2 <- cor(test$mpg, test$mpg_pred)^2\n",
        "cat('Root Mean Squared Error:', round(rmse, 2), '\\n')\n",
        "cat('R-squared:', round(r2, 2), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "residual-intro",
      "metadata": {},
      "source": [
        "## Residual Analysis\n",
        "\n",
        "Residuals are the differences between actual and predicted values. If the model is appropriate, residuals should look like random noise (no strong patterns), have roughly constant spread (homoscedasticity), and be approximately normally distributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "residual-analysis",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Calculate residuals (difference between actual and predicted values)\n",
        "residuals <- test$mpg - test$mpg_pred\n",
        "residual_df <- tibble(index = 1:length(residuals), residuals = residuals)\n",
        "\n",
        "# Plot residuals to check for patterns (should look random if model is good)\n",
        "p1 <- ggplot(residual_df, aes(x = index, y = residuals)) +\n",
        "  geom_point() +\n",
        "  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') +\n",
        "  labs(title = 'Residuals', x = 'Index', y = 'Residual') +\n",
        "  theme_minimal()\n",
        "\n",
        "# Q-Q plot to check if residuals are normally distributed\n",
        "p2 <- ggplot(residual_df, aes(sample = residuals)) +\n",
        "  stat_qq() +\n",
        "  stat_qq_line() +\n",
        "  labs(title = 'Q-Q Plot', x = 'Theoretical Quantiles', y = 'Sample Quantiles') +\n",
        "  theme_minimal()\n",
        "\n",
        "# Display plots side by side\n",
        "library(gridExtra)\n",
        "grid.arrange(p1, p2, ncol = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "correlation-intro",
      "metadata": {},
      "source": [
        "## Correlation Matrix and Heatmap\n",
        "\n",
        "Let's check the correlation between numeric variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "correlation-heatmap",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix for numeric variables\n",
        "cor_mat <- cor(data %>% select(where(is.numeric)))\n",
        "# Reshape for plotting\n",
        "melted_cor <- melt(cor_mat)\n",
        "# Plot a heatmap of correlations\n",
        "ggplot(melted_cor, aes(Var1, Var2, fill = value)) +\n",
        "  geom_tile() +\n",
        "  scale_fill_gradient2(low = 'blue', high = 'red', mid = 'white', midpoint = 0) +\n",
        "  theme_minimal() +\n",
        "  labs(title = 'Correlation Heatmap')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "timeseries-intro",
      "metadata": {},
      "source": [
        "# Time Series Example: Bitcoin Price Forecasting\n",
        "\n",
        "We'll try to load the same Bitcoin CSV used in the Python notebook (`data_BTC.csv`). If it is not found, we will fall back to a simulated series so the notebook still runs end-to-end.\n",
        "\n",
        "For forecasting with linear regression, we build **lag features**: `price_lag1` is yesterday's price, `price_lag2` is two days ago, and so on. The model learns to predict today's price from those past values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "simulate-timeseries",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Load real BTC price data if available (as in the Python notebook), else simulate\n",
        "parse_date_mixed <- function(x) {\n",
        "  d <- lubridate::ymd(x, quiet = TRUE)\n",
        "  if (all(is.na(d))) d <- lubridate::dmy(x, quiet = TRUE)\n",
        "  if (all(is.na(d))) d <- lubridate::mdy(x, quiet = TRUE)\n",
        "  d\n",
        "}\n",
        "\n",
        "csv_path <- \"data_BTC.csv\"\n",
        "if (file.exists(csv_path)) {\n",
        "  message(\"Loading real BTC data from \", csv_path)\n",
        "  btc_raw <- readr::read_csv(csv_path, show_col_types = FALSE)\n",
        "  # Expect columns: Date, BTC-USD.Close\n",
        "  btc <- btc_raw %>%\n",
        "    dplyr::transmute(\n",
        "      date = parse_date_mixed(.data$Date),\n",
        "      price = .data$`BTC-USD.Close`\n",
        "    ) %>%\n",
        "    dplyr::filter(!is.na(date), !is.na(price)) %>%\n",
        "    dplyr::arrange(date)\n",
        "} else {\n",
        "  message(\"File not found: \", csv_path, \". Falling back to simulated data.\")\n",
        "  set.seed(123)\n",
        "  n <- 200\n",
        "  btc <- tibble(\n",
        "    date = seq.Date(from = as.Date('2022-01-01'), by = 'day', length.out = n),\n",
        "    price = cumsum(rnorm(n, 0.1, 2)) + 30000\n",
        "  )\n",
        "}\n",
        "\n",
        "# Plot the price series\n",
        "ggplot(btc, aes(x = date, y = price)) +\n",
        "  geom_line(color = 'blue') +\n",
        "  labs(title = 'Bitcoin Price', x = 'Date', y = 'Price (USD)') +\n",
        "  theme_minimal()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-lags",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Create lagged features for time series forecasting\n",
        "# This function adds columns for previous values (lags) of the target variable\n",
        "create_lags <- function(df, var, lags = 5) {\n",
        "  for (i in 1:lags) {\n",
        "    df[[paste0(var, '_lag', i)]] <- dplyr::lag(df[[var]], i)\n",
        "  }\n",
        "  df\n",
        "}\n",
        "# Add 5 lagged features and drop rows with NA (due to lag)\n",
        "btc_lagged <- create_lags(btc, 'price', lags = 5) %>% drop_na()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "timeseries-model",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Split the time series into train and test sets (use last 20% as test)\n",
        "n_train <- floor(0.8 * nrow(btc_lagged))\n",
        "train <- btc_lagged[1:n_train, ]\n",
        "test <- btc_lagged[(n_train + 1):nrow(btc_lagged), ]\n",
        "\n",
        "# Fit a linear model using lagged features to predict price\n",
        "model_ts <- lm(price ~ price_lag1 + price_lag2 + price_lag3 + price_lag4 + price_lag5, data = train)\n",
        "# Show model summary\n",
        "summary(model_ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "timeseries-evaluate",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Predict on the test set and evaluate performance\n",
        "test <- test %>% mutate(pred = predict(model_ts, newdata = test))\n",
        "rmse_ts <- sqrt(mean((test$price - test$pred)^2))\n",
        "r2_ts <- cor(test$price, test$pred)^2\n",
        "cat('Time Series RMSE:', round(rmse_ts, 2), '\\n')\n",
        "cat('Time Series R-squared:', round(r2_ts, 2), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "timeseries-plot",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# Plot true vs predicted prices for the test set\n",
        "ggplot(test, aes(x = price, y = pred)) +\n",
        "  geom_point(color = 'red') +\n",
        "  geom_abline(slope = 1, intercept = 0, linetype = 'dashed') +\n",
        "  labs(title = 'True vs Predicted Bitcoin Price', x = 'True', y = 'Predicted') +\n",
        "  theme_minimal()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assumptions-intro",
      "metadata": {},
      "source": [
        "# Linear Regression Assumptions\n",
        "\n",
        "1. **Linearity**: Relationship between predictors and target is linear (scatter + smooth line helps assess this).\n",
        "2. **No (or little) multicollinearity**: Predictors are not highly correlated (check correlation matrix / VIFs for multiple predictors).\n",
        "3. **Homoscedasticity**: Residuals have constant variance (residuals vs. index/fit should show roughly equal spread).\n",
        "4. **Normality of residuals**: Residuals are normally distributed (Q–Q plot points close to the diagonal).\n",
        "5. **Independence of residuals**: No autocorrelation in residuals (ACF near zero; for time series, low lag-1 correlation is important).\n",
        "\n",
        "These checks are visual heuristics here; for formal testing, consider statistical tests (e.g., Breusch–Pagan for homoscedasticity, Shapiro–Wilk for normality, Durbin–Watson for autocorrelation) as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check-linearity",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# 1. Linearity: Check if the relationship between lagged price and current price is linear\n",
        "ggplot(train, aes(x = price_lag1, y = price)) +\n",
        "  geom_point() +\n",
        "  geom_smooth(method = 'lm', se = FALSE, color = 'red') +\n",
        "  labs(title = 'Linearity Check', x = 'Lag 1 Price', y = 'Price')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check-multicollinearity",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# 2. Multicollinearity: Check if lagged features are highly correlated\n",
        "cor(train %>% select(starts_with('price_lag')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check-homoscedasticity",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# 3. Homoscedasticity: Plot residuals to check for constant variance\n",
        "resid_ts <- test$price - test$pred\n",
        "resid_ts_df <- tibble(index = 1:length(resid_ts), residuals = resid_ts)\n",
        "ggplot(resid_ts_df, aes(x = index, y = residuals)) +\n",
        "  geom_point() +\n",
        "  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') +\n",
        "  labs(title = 'Residuals (Time Series)', x = 'Index', y = 'Residual') +\n",
        "  theme_minimal()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check-normality",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# 4. Normality of residuals: Q-Q plot to check if residuals are normally distributed\n",
        "ggplot(resid_ts_df, aes(sample = residuals)) +\n",
        "  stat_qq() +\n",
        "  stat_qq_line() +\n",
        "  labs(title = 'Q-Q Plot of Residuals', x = 'Theoretical Quantiles', y = 'Sample Quantiles') +\n",
        "  theme_minimal()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check-independence",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "# 5. Independence (autocorrelation): Plot autocorrelation of residuals\n",
        "acf_result <- acf(resid_ts, plot = FALSE)\n",
        "acf_df <- tibble(\n",
        "  lag = as.numeric(acf_result$lag),\n",
        "  acf = as.numeric(acf_result$acf)\n",
        ")\n",
        "ggplot(acf_df, aes(x = lag, y = acf)) +\n",
        "  geom_hline(yintercept = 0, color = 'black') +\n",
        "  geom_segment(aes(xend = lag, yend = 0)) +\n",
        "  geom_hline(yintercept = c(-0.2, 0.2), linetype = 'dashed', color = 'blue') +\n",
        "  labs(title = 'ACF of Residuals', x = 'Lag', y = 'ACF') +\n",
        "  theme_minimal()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Key takeaways\n",
        "- You learned how to generate, visualize, and model linear data in R\n",
        "- You saw how to use train/test splits and evaluate model performance\n",
        "- You checked model assumptions visually\n",
        "- You saw how to use lagged features for time series forecasting\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "4.3.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
