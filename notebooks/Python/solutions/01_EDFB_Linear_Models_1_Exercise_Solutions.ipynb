{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression Exercise Solutions\n",
        "\n",
        "---\n",
        "\n",
        "This notebook contains detailed solutions to all exercises from the Linear Regression: Univariate notebook. Each solution includes:\n",
        "- Complete working code\n",
        "- Detailed explanations of the approach\n",
        "- Business interpretation of results\n",
        "- Key insights and takeaways\n",
        "\n",
        "**Prerequisites:** Make sure you have run the main linear regression notebook first to have the necessary data and models loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate the banking dataset and model from the main notebook\n",
        "# This ensures we have all necessary variables for the exercises\n",
        "\n",
        "banking_url = \"https://raw.githubusercontent.com/umatter/EDFB/main/data/banking.csv\"\n",
        "dataset = pd.read_csv(banking_url)\n",
        "\n",
        "# Prepare the data exactly as in the main notebook\n",
        "df_banking = dataset.copy()\n",
        "df_banking['was_previously_contacted'] = (df_banking['pdays'] != 999).astype(int)\n",
        "df_banking['pdays_clean'] = df_banking['pdays'].replace(999, np.nan)\n",
        "df_banking['pdays_clean'] = df_banking['pdays_clean'].fillna(df_banking['pdays_clean'].median())\n",
        "\n",
        "feature_cols_cat = ['marital', 'education', 'housing', 'loan', 'contact', 'poutcome']\n",
        "feature_cols_num = ['age', 'previous', 'pdays_clean', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'was_previously_contacted']\n",
        "X_df_banking = pd.get_dummies(df_banking[feature_cols_cat + feature_cols_num], drop_first=True)\n",
        "X_banking = X_df_banking.values\n",
        "y_banking = np.log1p(df_banking['duration']).values.reshape(-1,1)\n",
        "\n",
        "# Split the data\n",
        "X_train_banking, X_test_banking, y_train_banking, y_test_banking = train_test_split(\n",
        "    X_banking, y_banking, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model_banking = LinearRegression()\n",
        "model_banking.fit(X_train_banking, y_train_banking)\n",
        "y_test_predicted_banking = model_banking.predict(X_test_banking)\n",
        "\n",
        "print(\"Banking dataset prepared successfully!\")\n",
        "print(f\"Features: {X_df_banking.shape[1]}\")\n",
        "print(f\"Training samples: {X_train_banking.shape[0]}\")\n",
        "print(f\"Test samples: {X_test_banking.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Understanding Model Coefficients\n",
        "\n",
        "**Task:** Interpret the coefficients from the banking dataset model and understand their business meaning.\n",
        "\n",
        "### Solution Approach:\n",
        "1. Extract coefficients and feature names\n",
        "2. Create a readable DataFrame\n",
        "3. Identify most influential features\n",
        "4. Provide business interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1 Solution\n",
        "\n",
        "# Create DataFrame with feature names and coefficients\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': X_df_banking.columns,\n",
        "    'Coefficient': model_banking.coef_[0],\n",
        "    'Abs_Coefficient': np.abs(model_banking.coef_[0])\n",
        "})\n",
        "\n",
        "# Sort by absolute value to see most influential features\n",
        "coef_df_sorted = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"=== MODEL COEFFICIENTS ANALYSIS ===\")\n",
        "print(f\"Intercept: {model_banking.intercept_[0]:.4f}\")\n",
        "print(\"\\nAll coefficients (sorted by absolute magnitude):\")\n",
        "print(coef_df_sorted.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== TOP 5 POSITIVE COEFFICIENTS ===\")\n",
        "top_positive = coef_df[coef_df['Coefficient'] > 0].nlargest(5, 'Coefficient')\n",
        "print(top_positive[['Feature', 'Coefficient']].to_string(index=False))\n",
        "\n",
        "print(\"\\n=== TOP 5 NEGATIVE COEFFICIENTS ===\")\n",
        "top_negative = coef_df[coef_df['Coefficient'] < 0].nsmallest(5, 'Coefficient')\n",
        "print(top_negative[['Feature', 'Coefficient']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the most important coefficients\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = coef_df_sorted.head(15)  # Top 15 most influential\n",
        "\n",
        "colors = ['red' if x < 0 else 'blue' for x in top_features['Coefficient']]\n",
        "plt.barh(range(len(top_features)), top_features['Coefficient'], color=colors, alpha=0.7)\n",
        "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Top 15 Most Influential Features (by Absolute Coefficient Value)')\n",
        "plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Business Interpretation:\n",
        "\n",
        "**Key Insights from Coefficients:**\n",
        "\n",
        "1. **Positive Coefficients (increase call duration):**\n",
        "   - Features with positive coefficients tend to increase the log duration of calls\n",
        "   - These might indicate more engaged customers or complex inquiries\n",
        "\n",
        "2. **Negative Coefficients (decrease call duration):**\n",
        "   - Features with negative coefficients tend to decrease call duration\n",
        "   - These might indicate quick decisions or less complex situations\n",
        "\n",
        "3. **Magnitude Interpretation:**\n",
        "   - Since we're predicting log(duration), a coefficient of 0.1 means approximately 10% increase in duration\n",
        "   - Larger absolute coefficients have more impact on call duration\n",
        "\n",
        "**Business Applications:**\n",
        "- Use high-impact features for call center resource planning\n",
        "- Identify customer segments that require more time\n",
        "- Optimize call routing based on expected duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Residual Analysis\n",
        "\n",
        "**Task:** Perform a comprehensive residual analysis to check model assumptions.\n",
        "\n",
        "### Solution Approach:\n",
        "1. Calculate residuals for training and test sets\n",
        "2. Create diagnostic plots\n",
        "3. Test for normality\n",
        "4. Check for patterns indicating assumption violations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2 Solution\n",
        "\n",
        "# Calculate residuals\n",
        "y_train_pred = model_banking.predict(X_train_banking)\n",
        "y_test_pred = model_banking.predict(X_test_banking)\n",
        "\n",
        "residuals_train = y_train_banking.ravel() - y_train_pred.ravel()\n",
        "residuals_test = y_test_banking.ravel() - y_test_pred.ravel()\n",
        "\n",
        "print(\"=== RESIDUAL ANALYSIS ===\")\n",
        "print(f\"Training residuals - Mean: {residuals_train.mean():.6f}, Std: {residuals_train.std():.4f}\")\n",
        "print(f\"Test residuals - Mean: {residuals_test.mean():.6f}, Std: {residuals_test.std():.4f}\")\n",
        "\n",
        "# Create comprehensive diagnostic plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Residuals vs Fitted (Training)\n",
        "axes[0,0].scatter(y_train_pred, residuals_train, alpha=0.5)\n",
        "axes[0,0].axhline(y=0, color='red', linestyle='--')\n",
        "axes[0,0].set_xlabel('Fitted Values')\n",
        "axes[0,0].set_ylabel('Residuals')\n",
        "axes[0,0].set_title('Residuals vs Fitted (Training)')\n",
        "\n",
        "# 2. Residuals vs Fitted (Test)\n",
        "axes[0,1].scatter(y_test_pred, residuals_test, alpha=0.5, color='orange')\n",
        "axes[0,1].axhline(y=0, color='red', linestyle='--')\n",
        "axes[0,1].set_xlabel('Fitted Values')\n",
        "axes[0,1].set_ylabel('Residuals')\n",
        "axes[0,1].set_title('Residuals vs Fitted (Test)')\n",
        "\n",
        "# 3. Histogram of residuals (Training)\n",
        "axes[0,2].hist(residuals_train, bins=50, alpha=0.7, density=True)\n",
        "axes[0,2].set_xlabel('Residuals')\n",
        "axes[0,2].set_ylabel('Density')\n",
        "axes[0,2].set_title('Distribution of Residuals (Training)')\n",
        "\n",
        "# 4. Q-Q plot (Training)\n",
        "from scipy import stats\n",
        "stats.probplot(residuals_train, dist=\"norm\", plot=axes[1,0])\n",
        "axes[1,0].set_title('Q-Q Plot (Training)')\n",
        "\n",
        "# 5. Residuals over time/index (Test)\n",
        "axes[1,1].plot(residuals_test, alpha=0.7)\n",
        "axes[1,1].axhline(y=0, color='red', linestyle='--')\n",
        "axes[1,1].set_xlabel('Observation Index')\n",
        "axes[1,1].set_ylabel('Residuals')\n",
        "axes[1,1].set_title('Residuals Over Index (Test)')\n",
        "\n",
        "# 6. Scale-Location plot\n",
        "sqrt_abs_resid = np.sqrt(np.abs(residuals_test))\n",
        "axes[1,2].scatter(y_test_pred, sqrt_abs_resid, alpha=0.5, color='green')\n",
        "axes[1,2].set_xlabel('Fitted Values')\n",
        "axes[1,2].set_ylabel('√|Residuals|')\n",
        "axes[1,2].set_title('Scale-Location Plot (Test)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical tests for normality\n",
        "from scipy.stats import shapiro, jarque_bera, anderson\n",
        "\n",
        "print(\"=== NORMALITY TESTS ===\")\n",
        "\n",
        "# Shapiro-Wilk test (good for smaller samples)\n",
        "if len(residuals_test) <= 5000:  # Shapiro-Wilk has sample size limitations\n",
        "    shapiro_stat, shapiro_p = shapiro(residuals_test)\n",
        "    print(f\"Shapiro-Wilk Test: statistic={shapiro_stat:.4f}, p-value={shapiro_p:.6f}\")\n",
        "    print(f\"Interpretation: {'Residuals appear normal' if shapiro_p > 0.05 else 'Residuals deviate from normality'}\")\n",
        "\n",
        "# Jarque-Bera test\n",
        "jb_stat, jb_p = jarque_bera(residuals_test)\n",
        "print(f\"\\nJarque-Bera Test: statistic={jb_stat:.4f}, p-value={jb_p:.6f}\")\n",
        "print(f\"Interpretation: {'Residuals appear normal' if jb_p > 0.05 else 'Residuals deviate from normality'}\")\n",
        "\n",
        "# Anderson-Darling test\n",
        "ad_stat, ad_critical, ad_significance = anderson(residuals_test, dist='norm')\n",
        "print(f\"\\nAnderson-Darling Test: statistic={ad_stat:.4f}\")\n",
        "for i, (crit, sig) in enumerate(zip(ad_critical, ad_significance)):\n",
        "    print(f\"  At {sig}% significance: critical value = {crit:.4f}, {'REJECT normality' if ad_stat > crit else 'ACCEPT normality'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residual Analysis Interpretation:\n",
        "\n",
        "**What to Look For:**\n",
        "\n",
        "1. **Residuals vs Fitted:**\n",
        "   - Should show random scatter around zero\n",
        "   - Patterns indicate model misspecification\n",
        "   - Funnel shapes indicate heteroscedasticity\n",
        "\n",
        "2. **Normality of Residuals:**\n",
        "   - Histogram should be approximately bell-shaped\n",
        "   - Q-Q plot points should follow the diagonal line\n",
        "   - Statistical tests provide formal assessment\n",
        "\n",
        "3. **Scale-Location Plot:**\n",
        "   - Tests for homoscedasticity (constant variance)\n",
        "   - Should show random scatter, not increasing/decreasing trend\n",
        "\n",
        "**Common Issues and Solutions:**\n",
        "- **Non-normality:** Consider transformations or robust regression\n",
        "- **Heteroscedasticity:** Use weighted least squares or robust standard errors\n",
        "- **Patterns in residuals:** Add missing variables or interaction terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Feature Engineering\n",
        "\n",
        "**Task:** Create new features and see if they improve model performance.\n",
        "\n",
        "### Solution Approach:\n",
        "1. Create interaction terms between numerical features\n",
        "2. Add polynomial (squared) terms\n",
        "3. Train new model with engineered features\n",
        "4. Compare performance with original model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3 Solution\n",
        "\n",
        "print(\"=== FEATURE ENGINEERING ===\")\n",
        "\n",
        "# Start with original features\n",
        "X_engineered = X_df_banking.copy()\n",
        "print(f\"Original features: {X_engineered.shape[1]}\")\n",
        "\n",
        "# 1. Create interaction terms between key numerical features\n",
        "numerical_features = ['age', 'previous', 'pdays_clean', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed']\n",
        "available_numerical = [col for col in numerical_features if col in X_engineered.columns]\n",
        "\n",
        "print(f\"\\nCreating interactions between: {available_numerical}\")\n",
        "\n",
        "# Create some meaningful interactions (not all combinations to avoid overfitting)\n",
        "interaction_pairs = [\n",
        "    ('age', 'previous'),  # Age and previous contacts\n",
        "    ('emp_var_rate', 'cons_conf_idx'),  # Economic indicators\n",
        "    ('cons_price_idx', 'euribor3m'),  # Economic indicators\n",
        "]\n",
        "\n",
        "for feat1, feat2 in interaction_pairs:\n",
        "    if feat1 in X_engineered.columns and feat2 in X_engineered.columns:\n",
        "        interaction_name = f'{feat1}_x_{feat2}'\n",
        "        X_engineered[interaction_name] = X_engineered[feat1] * X_engineered[feat2]\n",
        "        print(f\"Created interaction: {interaction_name}\")\n",
        "\n",
        "# 2. Add polynomial (squared) terms for key numerical features\n",
        "polynomial_features = ['age', 'previous', 'emp_var_rate', 'cons_conf_idx']\n",
        "for feat in polynomial_features:\n",
        "    if feat in X_engineered.columns:\n",
        "        squared_name = f'{feat}_squared'\n",
        "        X_engineered[squared_name] = X_engineered[feat] ** 2\n",
        "        print(f\"Created polynomial term: {squared_name}\")\n",
        "\n",
        "print(f\"\\nTotal features after engineering: {X_engineered.shape[1]}\")\n",
        "print(f\"Added {X_engineered.shape[1] - X_df_banking.shape[1]} new features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the engineered dataset\n",
        "X_eng_train, X_eng_test, y_eng_train, y_eng_test = train_test_split(\n",
        "    X_engineered.values, y_banking, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "# Train model with engineered features\n",
        "model_engineered = LinearRegression()\n",
        "model_engineered.fit(X_eng_train, y_eng_train)\n",
        "\n",
        "# Make predictions\n",
        "y_eng_train_pred = model_engineered.predict(X_eng_train)\n",
        "y_eng_test_pred = model_engineered.predict(X_eng_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "print(\"=== MODEL COMPARISON ===\")\n",
        "\n",
        "# Original model performance\n",
        "r2_orig_train = r2_score(y_train_banking, y_train_pred)\n",
        "r2_orig_test = r2_score(y_test_banking, y_test_pred)\n",
        "rmse_orig_train = np.sqrt(mean_squared_error(y_train_banking, y_train_pred))\n",
        "rmse_orig_test = np.sqrt(mean_squared_error(y_test_banking, y_test_pred))\n",
        "\n",
        "# Engineered model performance\n",
        "r2_eng_train = r2_score(y_eng_train, y_eng_train_pred)\n",
        "r2_eng_test = r2_score(y_eng_test, y_eng_test_pred)\n",
        "rmse_eng_train = np.sqrt(mean_squared_error(y_eng_train, y_eng_train_pred))\n",
        "rmse_eng_test = np.sqrt(mean_squared_error(y_eng_test, y_eng_test_pred))\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['R² Train', 'R² Test', 'RMSE Train', 'RMSE Test'],\n",
        "    'Original Model': [r2_orig_train, r2_orig_test, rmse_orig_train, rmse_orig_test],\n",
        "    'Engineered Model': [r2_eng_train, r2_eng_test, rmse_eng_train, rmse_eng_test],\n",
        "    'Improvement': [\n",
        "        r2_eng_train - r2_orig_train,\n",
        "        r2_eng_test - r2_orig_test,\n",
        "        rmse_orig_train - rmse_eng_train,  # Negative means worse (higher RMSE)\n",
        "        rmse_orig_test - rmse_eng_test\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(comparison_df.to_string(index=False, float_format='%.6f'))\n",
        "\n",
        "# Check for overfitting\n",
        "print(f\"\\n=== OVERFITTING CHECK ===\")\n",
        "print(f\"Original model - Train/Test R² gap: {r2_orig_train - r2_orig_test:.6f}\")\n",
        "print(f\"Engineered model - Train/Test R² gap: {r2_eng_train - r2_eng_test:.6f}\")\n",
        "print(f\"Gap increase: {(r2_eng_train - r2_eng_test) - (r2_orig_train - r2_orig_test):.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the most important new features\n",
        "new_features = [col for col in X_engineered.columns if col not in X_df_banking.columns]\n",
        "new_feature_indices = [X_engineered.columns.get_loc(col) for col in new_features]\n",
        "new_feature_coefs = model_engineered.coef_[0][new_feature_indices]\n",
        "\n",
        "new_coef_df = pd.DataFrame({\n",
        "    'Feature': new_features,\n",
        "    'Coefficient': new_feature_coefs,\n",
        "    'Abs_Coefficient': np.abs(new_feature_coefs)\n",
        "}).sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\n=== NEW ENGINEERED FEATURES IMPORTANCE ===\")\n",
        "print(new_coef_df.to_string(index=False))\n",
        "\n",
        "# Plot new features\n",
        "if len(new_features) > 0:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = ['red' if x < 0 else 'blue' for x in new_coef_df['Coefficient']]\n",
        "    plt.barh(range(len(new_coef_df)), new_coef_df['Coefficient'], color=colors, alpha=0.7)\n",
        "    plt.yticks(range(len(new_coef_df)), new_coef_df['Feature'])\n",
        "    plt.xlabel('Coefficient Value')\n",
        "    plt.title('Coefficients of Engineered Features')\n",
        "    plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering Insights:\n",
        "\n",
        "**Key Findings:**\n",
        "\n",
        "1. **Performance Impact:**\n",
        "   - Compare R² and RMSE improvements\n",
        "   - Check if improvements are consistent between train/test\n",
        "\n",
        "2. **Overfitting Risk:**\n",
        "   - Monitor train/test performance gap\n",
        "   - Large gaps indicate overfitting\n",
        "\n",
        "3. **Feature Importance:**\n",
        "   - Interaction terms can capture non-linear relationships\n",
        "   - Polynomial terms model curvature in relationships\n",
        "\n",
        "**Business Applications:**\n",
        "- Interaction terms reveal how features work together\n",
        "- Polynomial terms capture diminishing returns or accelerating effects\n",
        "- Balance complexity with interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Cross-Validation\n",
        "\n",
        "**Task:** Use cross-validation to get a more robust estimate of model performance.\n",
        "\n",
        "### Solution Approach:\n",
        "1. Implement 5-fold cross-validation\n",
        "2. Compare with baseline model\n",
        "3. Analyze stability of performance\n",
        "4. Discuss implications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4 Solution\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "print(\"=== CROSS-VALIDATION ANALYSIS ===\")\n",
        "\n",
        "# Set up cross-validation\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 1. Cross-validate original model\n",
        "cv_scores_r2 = cross_val_score(model_banking, X_banking, y_banking.ravel(), \n",
        "                               cv=cv, scoring='r2')\n",
        "cv_scores_rmse = -cross_val_score(model_banking, X_banking, y_banking.ravel(), \n",
        "                                  cv=cv, scoring='neg_root_mean_squared_error')\n",
        "\n",
        "print(\"Original Model Cross-Validation Results:\")\n",
        "print(f\"R² scores: {cv_scores_r2}\")\n",
        "print(f\"R² mean: {cv_scores_r2.mean():.6f} ± {cv_scores_r2.std():.6f}\")\n",
        "print(f\"RMSE scores: {cv_scores_rmse}\")\n",
        "print(f\"RMSE mean: {cv_scores_rmse.mean():.6f} ± {cv_scores_rmse.std():.6f}\")\n",
        "\n",
        "# 2. Cross-validate engineered model\n",
        "cv_scores_eng_r2 = cross_val_score(model_engineered, X_engineered.values, y_banking.ravel(), \n",
        "                                   cv=cv, scoring='r2')\n",
        "cv_scores_eng_rmse = -cross_val_score(model_engineered, X_engineered.values, y_banking.ravel(), \n",
        "                                      cv=cv, scoring='neg_root_mean_squared_error')\n",
        "\n",
        "print(\"\\nEngineered Model Cross-Validation Results:\")\n",
        "print(f\"R² scores: {cv_scores_eng_r2}\")\n",
        "print(f\"R² mean: {cv_scores_eng_r2.mean():.6f} ± {cv_scores_eng_r2.std():.6f}\")\n",
        "print(f\"RMSE scores: {cv_scores_eng_rmse}\")\n",
        "print(f\"RMSE mean: {cv_scores_eng_rmse.mean():.6f} ± {cv_scores_eng_rmse.std():.6f}\")\n",
        "\n",
        "# 3. Baseline model (predict mean)\n",
        "dummy_regressor = DummyRegressor(strategy='mean')\n",
        "cv_scores_dummy_r2 = cross_val_score(dummy_regressor, X_banking, y_banking.ravel(), \n",
        "                                     cv=cv, scoring='r2')\n",
        "cv_scores_dummy_rmse = -cross_val_score(dummy_regressor, X_banking, y_banking.ravel(), \n",
        "                                        cv=cv, scoring='neg_root_mean_squared_error')\n",
        "\n",
        "print(\"\\nBaseline Model (Mean Prediction) Cross-Validation Results:\")\n",
        "print(f\"R² mean: {cv_scores_dummy_r2.mean():.6f} ± {cv_scores_dummy_r2.std():.6f}\")\n",
        "print(f\"RMSE mean: {cv_scores_dummy_rmse.mean():.6f} ± {cv_scores_dummy_rmse.std():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-validation results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# R² comparison\n",
        "cv_data_r2 = [cv_scores_dummy_r2, cv_scores_r2, cv_scores_eng_r2]\n",
        "labels = ['Baseline\\n(Mean)', 'Original\\nModel', 'Engineered\\nModel']\n",
        "\n",
        "ax1.boxplot(cv_data_r2, labels=labels)\n",
        "ax1.set_ylabel('R² Score')\n",
        "ax1.set_title('Cross-Validation R² Comparison')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# RMSE comparison\n",
        "cv_data_rmse = [cv_scores_dummy_rmse, cv_scores_rmse, cv_scores_eng_rmse]\n",
        "\n",
        "ax2.boxplot(cv_data_rmse, labels=labels)\n",
        "ax2.set_ylabel('RMSE')\n",
        "ax2.set_title('Cross-Validation RMSE Comparison')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical significance test\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "print(\"\\n=== STATISTICAL SIGNIFICANCE TESTS ===\")\n",
        "t_stat, p_value = ttest_rel(cv_scores_r2, cv_scores_dummy_r2)\n",
        "print(f\"Original vs Baseline (R²): t-statistic={t_stat:.4f}, p-value={p_value:.6f}\")\n",
        "print(f\"Significant improvement: {'Yes' if p_value < 0.05 else 'No'}\")\n",
        "\n",
        "t_stat, p_value = ttest_rel(cv_scores_eng_r2, cv_scores_r2)\n",
        "print(f\"\\nEngineered vs Original (R²): t-statistic={t_stat:.4f}, p-value={p_value:.6f}\")\n",
        "print(f\"Significant improvement: {'Yes' if p_value < 0.05 else 'No'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model stability analysis\n",
        "print(\"\\n=== MODEL STABILITY ANALYSIS ===\")\n",
        "\n",
        "def coefficient_of_variation(scores):\n",
        "    return scores.std() / scores.mean() if scores.mean() != 0 else np.inf\n",
        "\n",
        "stability_df = pd.DataFrame({\n",
        "    'Model': ['Baseline', 'Original', 'Engineered'],\n",
        "    'R² CV': [coefficient_of_variation(cv_scores_dummy_r2),\n",
        "              coefficient_of_variation(cv_scores_r2),\n",
        "              coefficient_of_variation(cv_scores_eng_r2)],\n",
        "    'RMSE CV': [coefficient_of_variation(cv_scores_dummy_rmse),\n",
        "                coefficient_of_variation(cv_scores_rmse),\n",
        "                coefficient_of_variation(cv_scores_eng_rmse)]\n",
        "})\n",
        "\n",
        "print(\"Coefficient of Variation (lower = more stable):\")\n",
        "print(stability_df.to_string(index=False, float_format='%.6f'))\n",
        "\n",
        "# Performance summary\n",
        "summary_df = pd.DataFrame({\n",
        "    'Model': ['Baseline', 'Original', 'Engineered'],\n",
        "    'Mean R²': [cv_scores_dummy_r2.mean(), cv_scores_r2.mean(), cv_scores_eng_r2.mean()],\n",
        "    'Std R²': [cv_scores_dummy_r2.std(), cv_scores_r2.std(), cv_scores_eng_r2.std()],\n",
        "    'Mean RMSE': [cv_scores_dummy_rmse.mean(), cv_scores_rmse.mean(), cv_scores_eng_rmse.mean()],\n",
        "    'Std RMSE': [cv_scores_dummy_rmse.std(), cv_scores_rmse.std(), cv_scores_eng_rmse.std()]\n",
        "})\n",
        "\n",
        "print(\"\\nPerformance Summary:\")\n",
        "print(summary_df.to_string(index=False, float_format='%.6f'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Validation Insights:\n",
        "\n",
        "**Key Benefits of Cross-Validation:**\n",
        "\n",
        "1. **Robust Performance Estimates:**\n",
        "   - Reduces dependence on specific train/test split\n",
        "   - Provides confidence intervals for performance\n",
        "\n",
        "2. **Model Stability Assessment:**\n",
        "   - Low standard deviation indicates stable performance\n",
        "   - High variation suggests overfitting or data sensitivity\n",
        "\n",
        "3. **Statistical Significance:**\n",
        "   - Paired t-tests determine if improvements are significant\n",
        "   - Helps avoid false conclusions from random variation\n",
        "\n",
        "**Business Implications:**\n",
        "- More reliable performance estimates for production deployment\n",
        "- Better understanding of model reliability\n",
        "- Informed decisions about model complexity trade-offs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Synthetic Data Generation\n",
        "\n",
        "**Task:** Create your own synthetic dataset with known relationships and test your model.\n",
        "\n",
        "### Solution Approach:\n",
        "1. Generate synthetic data with known coefficients\n",
        "2. Add realistic noise and outliers\n",
        "3. Test model's ability to recover true coefficients\n",
        "4. Experiment with different noise levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 5 Solution\n",
        "\n",
        "print(\"=== SYNTHETIC DATA GENERATION ===\")\n",
        "\n",
        "# Set parameters\n",
        "n_samples = 500\n",
        "true_coefficients = np.array([2.0, 3.0, -1.5])  # Known true coefficients\n",
        "true_intercept = 1.0\n",
        "\n",
        "def generate_synthetic_data(n_samples, true_coef, true_intercept, noise_level=1.0, outlier_fraction=0.05):\n",
        "    \"\"\"\n",
        "    Generate synthetic data with known linear relationship\n",
        "    \"\"\"\n",
        "    # Generate features from different distributions to make it realistic\n",
        "    X1 = np.random.normal(0, 1, n_samples)  # Standard normal\n",
        "    X2 = np.random.uniform(-2, 2, n_samples)  # Uniform\n",
        "    X3 = np.random.exponential(1, n_samples)  # Exponential (right-skewed)\n",
        "    \n",
        "    X = np.column_stack([X1, X2, X3])\n",
        "    \n",
        "    # Generate target with known relationship\n",
        "    y_true = true_intercept + X @ true_coef\n",
        "    \n",
        "    # Add noise\n",
        "    noise = np.random.normal(0, noise_level, n_samples)\n",
        "    y = y_true + noise\n",
        "    \n",
        "    # Add outliers\n",
        "    n_outliers = int(outlier_fraction * n_samples)\n",
        "    outlier_indices = np.random.choice(n_samples, n_outliers, replace=False)\n",
        "    y[outlier_indices] += np.random.normal(0, 5 * noise_level, n_outliers)\n",
        "    \n",
        "    return X, y, y_true\n",
        "\n",
        "# Generate synthetic data with different noise levels\n",
        "noise_levels = [0.5, 1.0, 2.0, 3.0]\n",
        "results = []\n",
        "\n",
        "for noise_level in noise_levels:\n",
        "    print(f\"\\n--- Noise Level: {noise_level} ---\")\n",
        "    \n",
        "    # Generate data\n",
        "    X_syn, y_syn, y_true_syn = generate_synthetic_data(\n",
        "        n_samples, true_coefficients, true_intercept, noise_level\n",
        "    )\n",
        "    \n",
        "    # Split data\n",
        "    X_train_syn, X_test_syn, y_train_syn, y_test_syn = train_test_split(\n",
        "        X_syn, y_syn, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Train model\n",
        "    model_syn = LinearRegression()\n",
        "    model_syn.fit(X_train_syn, y_train_syn)\n",
        "    \n",
        "    # Evaluate\n",
        "    y_pred_syn = model_syn.predict(X_test_syn)\n",
        "    r2_syn = r2_score(y_test_syn, y_pred_syn)\n",
        "    rmse_syn = np.sqrt(mean_squared_error(y_test_syn, y_pred_syn))\n",
        "    \n",
        "    # Compare estimated vs true coefficients\n",
        "    estimated_coef = model_syn.coef_\n",
        "    estimated_intercept = model_syn.intercept_\n",
        "    \n",
        "    coef_error = np.abs(estimated_coef - true_coefficients)\n",
        "    intercept_error = abs(estimated_intercept - true_intercept)\n",
        "    \n",
        "    print(f\"True coefficients: {true_coefficients}\")\n",
        "    print(f\"Estimated coefficients: {estimated_coef}\")\n",
        "    print(f\"Coefficient errors: {coef_error}\")\n",
        "    print(f\"True intercept: {true_intercept:.3f}, Estimated: {estimated_intercept:.3f}, Error: {intercept_error:.3f}\")\n",
        "    print(f\"R²: {r2_syn:.6f}, RMSE: {rmse_syn:.6f}\")\n",
        "    \n",
        "    results.append({\n",
        "        'noise_level': noise_level,\n",
        "        'r2': r2_syn,\n",
        "        'rmse': rmse_syn,\n",
        "        'coef_error_mean': coef_error.mean(),\n",
        "        'coef_error_max': coef_error.max(),\n",
        "        'intercept_error': intercept_error\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# R² vs noise level\n",
        "axes[0,0].plot(results_df['noise_level'], results_df['r2'], 'bo-')\n",
        "axes[0,0].set_xlabel('Noise Level')\n",
        "axes[0,0].set_ylabel('R²')\n",
        "axes[0,0].set_title('R² vs Noise Level')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# RMSE vs noise level\n",
        "axes[0,1].plot(results_df['noise_level'], results_df['rmse'], 'ro-')\n",
        "axes[0,1].set_xlabel('Noise Level')\n",
        "axes[0,1].set_ylabel('RMSE')\n",
        "axes[0,1].set_title('RMSE vs Noise Level')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Coefficient error vs noise level\n",
        "axes[1,0].plot(results_df['noise_level'], results_df['coef_error_mean'], 'go-', label='Mean Error')\n",
        "axes[1,0].plot(results_df['noise_level'], results_df['coef_error_max'], 'g^-', label='Max Error')\n",
        "axes[1,0].set_xlabel('Noise Level')\n",
        "axes[1,0].set_ylabel('Coefficient Error')\n",
        "axes[1,0].set_title('Coefficient Recovery Error vs Noise Level')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Intercept error vs noise level\n",
        "axes[1,1].plot(results_df['noise_level'], results_df['intercept_error'], 'mo-')\n",
        "axes[1,1].set_xlabel('Noise Level')\n",
        "axes[1,1].set_ylabel('Intercept Error')\n",
        "axes[1,1].set_title('Intercept Recovery Error vs Noise Level')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== SUMMARY TABLE ===\")\n",
        "print(results_df.to_string(index=False, float_format='%.6f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate with a specific example (medium noise)\n",
        "X_demo, y_demo, y_true_demo = generate_synthetic_data(\n",
        "    n_samples, true_coefficients, true_intercept, noise_level=1.0\n",
        ")\n",
        "\n",
        "# Visualize the data\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "feature_names = ['X1 (Normal)', 'X2 (Uniform)', 'X3 (Exponential)']\n",
        "\n",
        "for i in range(3):\n",
        "    axes[i].scatter(X_demo[:, i], y_demo, alpha=0.6, label='Observed')\n",
        "    axes[i].scatter(X_demo[:, i], y_true_demo, alpha=0.6, color='red', s=10, label='True (no noise)')\n",
        "    axes[i].set_xlabel(feature_names[i])\n",
        "    axes[i].set_ylabel('Target')\n",
        "    axes[i].set_title(f'Target vs {feature_names[i]}')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Train final model and show detailed results\n",
        "model_demo = LinearRegression()\n",
        "model_demo.fit(X_demo, y_demo)\n",
        "\n",
        "print(\"\\n=== DETAILED COEFFICIENT RECOVERY ===\")\n",
        "coef_comparison = pd.DataFrame({\n",
        "    'Feature': ['X1', 'X2', 'X3', 'Intercept'],\n",
        "    'True Value': list(true_coefficients) + [true_intercept],\n",
        "    'Estimated Value': list(model_demo.coef_) + [model_demo.intercept_],\n",
        "    'Absolute Error': list(np.abs(model_demo.coef_ - true_coefficients)) + [abs(model_demo.intercept_ - true_intercept)],\n",
        "    'Relative Error (%)': [\n",
        "        abs(est - true) / abs(true) * 100 if true != 0 else np.inf\n",
        "        for est, true in zip(list(model_demo.coef_) + [model_demo.intercept_], \n",
        "                           list(true_coefficients) + [true_intercept])\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(coef_comparison.to_string(index=False, float_format='%.6f'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Synthetic Data Insights:\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "1. **Coefficient Recovery:**\n",
        "   - Linear regression successfully recovers true coefficients\n",
        "   - Accuracy decreases with higher noise levels\n",
        "   - Outliers can significantly impact estimates\n",
        "\n",
        "2. **Noise Impact:**\n",
        "   - Higher noise reduces R² and increases RMSE\n",
        "   - Coefficient estimation becomes less accurate\n",
        "   - Relationship is predictable and quantifiable\n",
        "\n",
        "3. **Model Validation:**\n",
        "   - Synthetic data provides ground truth for testing\n",
        "   - Helps understand model limitations\n",
        "   - Useful for algorithm development and debugging\n",
        "\n",
        "**Practical Applications:**\n",
        "- Test new algorithms on known problems\n",
        "- Understand impact of data quality on model performance\n",
        "- Generate training data when real data is limited"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Time Series Forecasting Analysis\n",
        "\n",
        "**Task:** Analyze the Bitcoin forecasting model more deeply.\n",
        "\n",
        "### Solution Approach:\n",
        "1. Calculate directional accuracy\n",
        "2. Create cumulative returns comparison\n",
        "3. Test different lag lengths\n",
        "4. Discuss trading implications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 6 Solution\n",
        "# First, let's recreate the Bitcoin data and model from the main notebook\n",
        "\n",
        "print(\"=== BITCOIN TIME SERIES ANALYSIS ===\")\n",
        "\n",
        "# Load Bitcoin data\n",
        "btc_url = \"https://raw.githubusercontent.com/umatter/EDFB/main/data/data_BTC.csv\"\n",
        "\n",
        "try:\n",
        "    data_btc = pd.read_csv(btc_url)\n",
        "except Exception as e:\n",
        "    print(\"Falling back to KaggleHub dataset due to:\", repr(e))\n",
        "    try:\n",
        "        import kagglehub\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kagglehub\"])\n",
        "        import kagglehub\n",
        "    path = kagglehub.dataset_download(\"nguynchtrai/data-btc\")\n",
        "    import os, glob\n",
        "    candidates = sorted(glob.glob(os.path.join(path, \"*.csv\")))\n",
        "    if not candidates:\n",
        "        raise RuntimeError(\"No CSV files found in Kaggle dataset directory: \" + path)\n",
        "    print(\"Loaded from:\", candidates[0])\n",
        "    data_btc = pd.read_csv(candidates[0])\n",
        "\n",
        "# Standardize columns\n",
        "if 'Timestamp' in data_btc.columns and 'Close' in data_btc.columns:\n",
        "    data_btc['Date'] = pd.to_datetime(data_btc['Timestamp'], unit='ms')\n",
        "    data_btc = data_btc.sort_values('Date').reset_index(drop=True)\n",
        "    data_btc = data_btc.rename(columns={'Close': 'BTC-USD.Close'})\n",
        "elif 'Date' in data_btc.columns and 'BTC-USD.Close' in data_btc.columns:\n",
        "    try:\n",
        "        data_btc['Date'] = pd.to_datetime(data_btc['Date'])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "data_btc = data_btc.drop(columns=['Timestamp'], errors='ignore')\n",
        "data_btc = data_btc.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "print(f\"Bitcoin data loaded: {len(data_btc)} observations\")\n",
        "print(f\"Date range: {data_btc['Date'].min()} to {data_btc['Date'].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to create lagged features and train model\n",
        "def create_lagged_features_btc(data, lag):\n",
        "    df = data.copy()\n",
        "    df['ret'] = np.log(df['BTC-USD.Close']).diff()\n",
        "    for i in range(1, lag+1):\n",
        "        df[f'lag_ret_{i}'] = df['ret'].shift(i)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def train_btc_model(data, lag):\n",
        "    # Create features\n",
        "    data_lagged = create_lagged_features_btc(data, lag)\n",
        "    \n",
        "    # Prepare features and target\n",
        "    feature_cols = [f'lag_ret_{i}' for i in range(1, lag+1)]\n",
        "    X = data_lagged[feature_cols].values\n",
        "    y = data_lagged['ret'].values\n",
        "    \n",
        "    # Chronological split\n",
        "    split_idx = int(len(data_lagged) * 0.8)\n",
        "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "    \n",
        "    # Train model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    return {\n",
        "        'model': model,\n",
        "        'X_train': X_train, 'X_test': X_test,\n",
        "        'y_train': y_train, 'y_test': y_test,\n",
        "        'y_pred': y_pred,\n",
        "        'data': data_lagged,\n",
        "        'split_idx': split_idx\n",
        "    }\n",
        "\n",
        "# Test different lag lengths\n",
        "lag_lengths = [1, 3, 5, 10]\n",
        "lag_results = {}\n",
        "\n",
        "print(\"\\n=== TESTING DIFFERENT LAG LENGTHS ===\")\n",
        "\n",
        "for lag in lag_lengths:\n",
        "    print(f\"\\n--- Lag Length: {lag} ---\")\n",
        "    \n",
        "    result = train_btc_model(data_btc, lag)\n",
        "    lag_results[lag] = result\n",
        "    \n",
        "    # Calculate metrics\n",
        "    r2 = r2_score(result['y_test'], result['y_pred'])\n",
        "    rmse = np.sqrt(mean_squared_error(result['y_test'], result['y_pred']))\n",
        "    \n",
        "    # Directional accuracy\n",
        "    direction_correct = np.sum(np.sign(result['y_test']) == np.sign(result['y_pred']))\n",
        "    directional_accuracy = direction_correct / len(result['y_test'])\n",
        "    \n",
        "    # Naive baseline (lag-1 prediction)\n",
        "    naive_pred = result['X_test'][:, 0]  # First lag is lag-1\n",
        "    rmse_naive = np.sqrt(mean_squared_error(result['y_test'], naive_pred))\n",
        "    \n",
        "    print(f\"R²: {r2:.6f}\")\n",
        "    print(f\"RMSE: {rmse:.6f}\")\n",
        "    print(f\"RMSE (naive): {rmse_naive:.6f}\")\n",
        "    print(f\"RMSE ratio (model/naive): {rmse/rmse_naive:.3f}\")\n",
        "    print(f\"Directional accuracy: {directional_accuracy:.3f} ({direction_correct}/{len(result['y_test'])} correct)\")\n",
        "    \n",
        "    lag_results[lag]['metrics'] = {\n",
        "        'r2': r2, 'rmse': rmse, 'rmse_naive': rmse_naive,\n",
        "        'directional_accuracy': directional_accuracy\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare lag lengths\n",
        "comparison_data = []\n",
        "for lag, result in lag_results.items():\n",
        "    metrics = result['metrics']\n",
        "    comparison_data.append({\n",
        "        'Lag Length': lag,\n",
        "        'R²': metrics['r2'],\n",
        "        'RMSE': metrics['rmse'],\n",
        "        'RMSE Ratio': metrics['rmse'] / metrics['rmse_naive'],\n",
        "        'Directional Accuracy': metrics['directional_accuracy']\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n=== LAG LENGTH COMPARISON ===\")\n",
        "print(comparison_df.to_string(index=False, float_format='%.6f'))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# R² comparison\n",
        "axes[0,0].plot(comparison_df['Lag Length'], comparison_df['R²'], 'bo-')\n",
        "axes[0,0].set_xlabel('Lag Length')\n",
        "axes[0,0].set_ylabel('R²')\n",
        "axes[0,0].set_title('R² vs Lag Length')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# RMSE ratio comparison\n",
        "axes[0,1].plot(comparison_df['Lag Length'], comparison_df['RMSE Ratio'], 'ro-')\n",
        "axes[0,1].axhline(y=1, color='black', linestyle='--', alpha=0.5, label='Naive baseline')\n",
        "axes[0,1].set_xlabel('Lag Length')\n",
        "axes[0,1].set_ylabel('RMSE Ratio (Model/Naive)')\n",
        "axes[0,1].set_title('RMSE Ratio vs Lag Length')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Directional accuracy\n",
        "axes[1,0].plot(comparison_df['Lag Length'], comparison_df['Directional Accuracy'], 'go-')\n",
        "axes[1,0].axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='Random guess')\n",
        "axes[1,0].set_xlabel('Lag Length')\n",
        "axes[1,0].set_ylabel('Directional Accuracy')\n",
        "axes[1,0].set_title('Directional Accuracy vs Lag Length')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# RMSE absolute values\n",
        "axes[1,1].plot(comparison_df['Lag Length'], comparison_df['RMSE'], 'mo-')\n",
        "axes[1,1].set_xlabel('Lag Length')\n",
        "axes[1,1].set_ylabel('RMSE')\n",
        "axes[1,1].set_title('RMSE vs Lag Length')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cumulative returns analysis (using best performing model)\n",
        "best_lag = comparison_df.loc[comparison_df['Directional Accuracy'].idxmax(), 'Lag Length']\n",
        "print(f\"\\n=== CUMULATIVE RETURNS ANALYSIS (Lag {best_lag}) ===\")\n",
        "\n",
        "best_result = lag_results[best_lag]\n",
        "y_test = best_result['y_test']\n",
        "y_pred = best_result['y_pred']\n",
        "\n",
        "# Create trading strategies\n",
        "# Strategy 1: Perfect foresight (actual returns)\n",
        "perfect_returns = y_test\n",
        "\n",
        "# Strategy 2: Model predictions\n",
        "# Trade based on predicted direction: if predict positive return, go long\n",
        "model_returns = np.where(y_pred > 0, y_test, -y_test)  # Go long if positive prediction, short if negative\n",
        "\n",
        "# Strategy 3: Naive (always long)\n",
        "naive_returns = y_test\n",
        "\n",
        "# Strategy 4: Random (50% chance of going long)\n",
        "np.random.seed(42)\n",
        "random_signals = np.random.choice([-1, 1], size=len(y_test))\n",
        "random_returns = random_signals * y_test\n",
        "\n",
        "# Calculate cumulative returns\n",
        "cum_perfect = np.cumsum(perfect_returns)\n",
        "cum_model = np.cumsum(model_returns)\n",
        "cum_naive = np.cumsum(naive_returns)\n",
        "cum_random = np.cumsum(random_returns)\n",
        "\n",
        "# Plot cumulative returns\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.plot(cum_perfect, label='Perfect Foresight (Buy & Hold)', linewidth=2)\n",
        "plt.plot(cum_model, label=f'Model Strategy (Lag {best_lag})', linewidth=2)\n",
        "plt.plot(cum_naive, label='Naive (Always Long)', linewidth=1, alpha=0.7)\n",
        "plt.plot(cum_random, label='Random Strategy', linewidth=1, alpha=0.7)\n",
        "plt.xlabel('Time Period')\n",
        "plt.ylabel('Cumulative Log Returns')\n",
        "plt.title('Cumulative Returns Comparison: Trading Strategies')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Calculate strategy statistics\n",
        "strategies = {\n",
        "    'Perfect Foresight': perfect_returns,\n",
        "    'Model Strategy': model_returns,\n",
        "    'Naive (Buy & Hold)': naive_returns,\n",
        "    'Random': random_returns\n",
        "}\n",
        "\n",
        "strategy_stats = []\n",
        "for name, returns in strategies.items():\n",
        "    total_return = np.sum(returns)\n",
        "    volatility = np.std(returns)\n",
        "    sharpe_ratio = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0\n",
        "    max_drawdown = np.min(np.cumsum(returns) - np.maximum.accumulate(np.cumsum(returns)))\n",
        "    \n",
        "    strategy_stats.append({\n",
        "        'Strategy': name,\n",
        "        'Total Return': total_return,\n",
        "        'Volatility': volatility,\n",
        "        'Sharpe Ratio': sharpe_ratio,\n",
        "        'Max Drawdown': max_drawdown\n",
        "    })\n",
        "\n",
        "strategy_df = pd.DataFrame(strategy_stats)\n",
        "print(\"\\nStrategy Performance Comparison:\")\n",
        "print(strategy_df.to_string(index=False, float_format='%.6f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed analysis of model predictions\n",
        "print(\"\\n=== DETAILED PREDICTION ANALYSIS ===\")\n",
        "\n",
        "# Prediction accuracy by magnitude\n",
        "pred_magnitude = np.abs(y_pred)\n",
        "actual_magnitude = np.abs(y_test)\n",
        "\n",
        "# Bin predictions by confidence (magnitude)\n",
        "confidence_bins = np.percentile(pred_magnitude, [0, 25, 50, 75, 100])\n",
        "bin_labels = ['Low', 'Medium-Low', 'Medium-High', 'High']\n",
        "\n",
        "print(\"Directional Accuracy by Prediction Confidence:\")\n",
        "for i, label in enumerate(bin_labels):\n",
        "    mask = (pred_magnitude >= confidence_bins[i]) & (pred_magnitude < confidence_bins[i+1])\n",
        "    if i == len(bin_labels) - 1:  # Include the maximum value in the last bin\n",
        "        mask = pred_magnitude >= confidence_bins[i]\n",
        "    \n",
        "    if np.sum(mask) > 0:\n",
        "        accuracy = np.mean(np.sign(y_test[mask]) == np.sign(y_pred[mask]))\n",
        "        count = np.sum(mask)\n",
        "        avg_magnitude = np.mean(pred_magnitude[mask])\n",
        "        print(f\"{label} Confidence: {accuracy:.3f} ({count} predictions, avg magnitude: {avg_magnitude:.6f})\")\n",
        "\n",
        "# Market regime analysis\n",
        "print(\"\\nDirectional Accuracy by Market Regime:\")\n",
        "\n",
        "# Define regimes based on actual return magnitude\n",
        "low_vol_mask = actual_magnitude < np.percentile(actual_magnitude, 33)\n",
        "med_vol_mask = (actual_magnitude >= np.percentile(actual_magnitude, 33)) & (actual_magnitude < np.percentile(actual_magnitude, 67))\n",
        "high_vol_mask = actual_magnitude >= np.percentile(actual_magnitude, 67)\n",
        "\n",
        "regimes = {\n",
        "    'Low Volatility': low_vol_mask,\n",
        "    'Medium Volatility': med_vol_mask,\n",
        "    'High Volatility': high_vol_mask\n",
        "}\n",
        "\n",
        "for regime_name, mask in regimes.items():\n",
        "    if np.sum(mask) > 0:\n",
        "        accuracy = np.mean(np.sign(y_test[mask]) == np.sign(y_pred[mask]))\n",
        "        count = np.sum(mask)\n",
        "        avg_return = np.mean(np.abs(y_test[mask]))\n",
        "        print(f\"{regime_name}: {accuracy:.3f} ({count} periods, avg |return|: {avg_return:.6f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Time Series Forecasting Insights:\n",
        "\n",
        "**Key Findings:**\n",
        "\n",
        "1. **Lag Length Impact:**\n",
        "   - More lags don't always improve performance\n",
        "   - Risk of overfitting with too many lags\n",
        "   - Optimal lag length depends on market dynamics\n",
        "\n",
        "2. **Directional Accuracy:**\n",
        "   - Often more important than precise magnitude prediction\n",
        "   - Even modest improvements over 50% can be profitable\n",
        "   - Higher confidence predictions tend to be more accurate\n",
        "\n",
        "3. **Trading Strategy Performance:**\n",
        "   - Model-based strategies can outperform naive approaches\n",
        "   - Transaction costs and market impact not considered\n",
        "   - Risk management crucial for practical implementation\n",
        "\n",
        "**Practical Trading Implications:**\n",
        "\n",
        "1. **Position Sizing:**\n",
        "   - Scale positions based on prediction confidence\n",
        "   - Larger positions when model is more certain\n",
        "\n",
        "2. **Risk Management:**\n",
        "   - Set stop-losses and take-profits\n",
        "   - Monitor maximum drawdown\n",
        "   - Consider volatility regimes\n",
        "\n",
        "3. **Model Limitations:**\n",
        "   - Linear models may miss complex patterns\n",
        "   - Market conditions change over time\n",
        "   - Need for regular model retraining\n",
        "\n",
        "**Recommendations:**\n",
        "- Combine with other indicators and models\n",
        "- Implement proper backtesting with transaction costs\n",
        "- Consider ensemble methods for robustness\n",
        "- Regular performance monitoring and model updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 7: Model Comparison\n",
        "\n",
        "**Task:** Compare linear regression with Ridge and Lasso regression.\n",
        "\n",
        "### Solution Approach:\n",
        "1. Implement Ridge and Lasso regression\n",
        "2. Tune hyperparameters\n",
        "3. Compare performance metrics\n",
        "4. Analyze feature selection effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 7 Solution\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"=== REGULARIZED REGRESSION COMPARISON ===\")\n",
        "\n",
        "# Use the banking dataset with engineered features for this comparison\n",
        "X_comparison = X_engineered.values\n",
        "y_comparison = y_banking.ravel()\n",
        "\n",
        "# Split the data\n",
        "X_train_comp, X_test_comp, y_train_comp, y_test_comp = train_test_split(\n",
        "    X_comparison, y_comparison, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features (important for regularized regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_comp)\n",
        "X_test_scaled = scaler.transform(X_test_comp)\n",
        "\n",
        "print(f\"Dataset: {X_train_scaled.shape[0]} training, {X_test_scaled.shape[0]} test samples\")\n",
        "print(f\"Features: {X_train_scaled.shape[1]}\")\n",
        "\n",
        "# Define models and hyperparameter grids\n",
        "models = {\n",
        "    'Linear Regression': {\n",
        "        'model': LinearRegression(),\n",
        "        'params': {}\n",
        "    },\n",
        "    'Ridge Regression': {\n",
        "        'model': Ridge(),\n",
        "        'params': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}\n",
        "    },\n",
        "    'Lasso Regression': {\n",
        "        'model': Lasso(max_iter=2000),\n",
        "        'params': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "\n",
        "for name, config in models.items():\n",
        "    print(f\"\\n--- Training {name} ---\")\n",
        "    \n",
        "    if config['params']:  # If hyperparameters to tune\n",
        "        # Use GridSearchCV for hyperparameter tuning\n",
        "        grid_search = GridSearchCV(\n",
        "            config['model'], config['params'], \n",
        "            cv=5, scoring='r2', n_jobs=-1\n",
        "        )\n",
        "        grid_search.fit(X_train_scaled, y_train_comp)\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_params = grid_search.best_params_\n",
        "        print(f\"Best parameters: {best_params}\")\n",
        "        print(f\"Best CV score: {grid_search.best_score_:.6f}\")\n",
        "    else:\n",
        "        # No hyperparameters to tune\n",
        "        best_model = config['model']\n",
        "        best_model.fit(X_train_scaled, y_train_comp)\n",
        "        best_params = {}\n",
        "    \n",
        "    # Make predictions\n",
        "    y_train_pred = best_model.predict(X_train_scaled)\n",
        "    y_test_pred = best_model.predict(X_test_scaled)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_r2 = r2_score(y_train_comp, y_train_pred)\n",
        "    test_r2 = r2_score(y_test_comp, y_test_pred)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train_comp, y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test_comp, y_test_pred))\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'model': best_model,\n",
        "        'params': best_params,\n",
        "        'train_r2': train_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'train_rmse': train_rmse,\n",
        "        'test_rmse': test_rmse,\n",
        "        'overfitting': train_r2 - test_r2\n",
        "    }\n",
        "    \n",
        "    print(f\"Train R²: {train_r2:.6f}, Test R²: {test_r2:.6f}\")\n",
        "    print(f\"Train RMSE: {train_rmse:.6f}, Test RMSE: {test_rmse:.6f}\")\n",
        "    print(f\"Overfitting (R² gap): {train_r2 - test_r2:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "comparison_data = []\n",
        "for name, result in results.items():\n",
        "    comparison_data.append({\n",
        "        'Model': name,\n",
        "        'Train R²': result['train_r2'],\n",
        "        'Test R²': result['test_r2'],\n",
        "        'Train RMSE': result['train_rmse'],\n",
        "        'Test RMSE': result['test_rmse'],\n",
        "        'Overfitting': result['overfitting'],\n",
        "        'Best Params': str(result['params'])\n",
        "    })\n",
        "\n",
        "comparison_table = pd.DataFrame(comparison_data)\n",
        "print(\"\\n=== MODEL COMPARISON SUMMARY ===\")\n",
        "print(comparison_table.to_string(index=False, float_format='%.6f'))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "models_list = list(results.keys())\n",
        "train_r2_values = [results[model]['train_r2'] for model in models_list]\n",
        "test_r2_values = [results[model]['test_r2'] for model in models_list]\n",
        "train_rmse_values = [results[model]['train_rmse'] for model in models_list]\n",
        "test_rmse_values = [results[model]['test_rmse'] for model in models_list]\n",
        "\n",
        "# R² comparison\n",
        "x_pos = np.arange(len(models_list))\n",
        "width = 0.35\n",
        "\n",
        "axes[0,0].bar(x_pos - width/2, train_r2_values, width, label='Train', alpha=0.8)\n",
        "axes[0,0].bar(x_pos + width/2, test_r2_values, width, label='Test', alpha=0.8)\n",
        "axes[0,0].set_xlabel('Model')\n",
        "axes[0,0].set_ylabel('R²')\n",
        "axes[0,0].set_title('R² Comparison')\n",
        "axes[0,0].set_xticks(x_pos)\n",
        "axes[0,0].set_xticklabels(models_list, rotation=45)\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# RMSE comparison\n",
        "axes[0,1].bar(x_pos - width/2, train_rmse_values, width, label='Train', alpha=0.8)\n",
        "axes[0,1].bar(x_pos + width/2, test_rmse_values, width, label='Test', alpha=0.8)\n",
        "axes[0,1].set_xlabel('Model')\n",
        "axes[0,1].set_ylabel('RMSE')\n",
        "axes[0,1].set_title('RMSE Comparison')\n",
        "axes[0,1].set_xticks(x_pos)\n",
        "axes[0,1].set_xticklabels(models_list, rotation=45)\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Overfitting comparison\n",
        "overfitting_values = [results[model]['overfitting'] for model in models_list]\n",
        "axes[1,0].bar(models_list, overfitting_values, alpha=0.8, color='red')\n",
        "axes[1,0].set_xlabel('Model')\n",
        "axes[1,0].set_ylabel('Overfitting (Train R² - Test R²)')\n",
        "axes[1,0].set_title('Overfitting Comparison')\n",
        "axes[1,0].tick_params(axis='x', rotation=45)\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Test R² vs Overfitting scatter\n",
        "axes[1,1].scatter(test_r2_values, overfitting_values, s=100, alpha=0.7)\n",
        "for i, model in enumerate(models_list):\n",
        "    axes[1,1].annotate(model, (test_r2_values[i], overfitting_values[i]), \n",
        "                      xytext=(5, 5), textcoords='offset points')\n",
        "axes[1,1].set_xlabel('Test R²')\n",
        "axes[1,1].set_ylabel('Overfitting')\n",
        "axes[1,1].set_title('Test Performance vs Overfitting')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze feature selection (Lasso) and regularization effects\n",
        "print(\"\\n=== FEATURE SELECTION ANALYSIS ===\")\n",
        "\n",
        "# Get coefficients from each model\n",
        "feature_names = X_engineered.columns\n",
        "\n",
        "coef_comparison = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Linear': results['Linear Regression']['model'].coef_,\n",
        "    'Ridge': results['Ridge Regression']['model'].coef_,\n",
        "    'Lasso': results['Lasso Regression']['model'].coef_\n",
        "})\n",
        "\n",
        "# Count non-zero coefficients\n",
        "print(\"Non-zero coefficients:\")\n",
        "for model in ['Linear', 'Ridge', 'Lasso']:\n",
        "    non_zero = np.sum(np.abs(coef_comparison[model]) > 1e-6)\n",
        "    print(f\"{model}: {non_zero}/{len(feature_names)} ({non_zero/len(feature_names)*100:.1f}%)\")\n",
        "\n",
        "# Show features selected by Lasso\n",
        "lasso_selected = coef_comparison[np.abs(coef_comparison['Lasso']) > 1e-6]\n",
        "print(f\"\\nFeatures selected by Lasso ({len(lasso_selected)}):\"))\n",
        "lasso_selected_sorted = lasso_selected.reindex(\n",
        "    lasso_selected['Lasso'].abs().sort_values(ascending=False).index\n",
        ")\n",
        "print(lasso_selected_sorted[['Feature', 'Lasso']].to_string(index=False, float_format='%.6f'))\n",
        "\n",
        "# Visualize coefficient comparison for top features\n",
        "top_features = coef_comparison.reindex(\n",
        "    coef_comparison['Linear'].abs().sort_values(ascending=False).index\n",
        ").head(15)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "x_pos = np.arange(len(top_features))\n",
        "width = 0.25\n",
        "\n",
        "ax.bar(x_pos - width, top_features['Linear'], width, label='Linear', alpha=0.8)\n",
        "ax.bar(x_pos, top_features['Ridge'], width, label='Ridge', alpha=0.8)\n",
        "ax.bar(x_pos + width, top_features['Lasso'], width, label='Lasso', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Features')\n",
        "ax.set_ylabel('Coefficient Value')\n",
        "ax.set_title('Coefficient Comparison: Top 15 Features')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(top_features['Feature'], rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regularization path analysis\n",
        "print(\"\\n=== REGULARIZATION PATH ANALYSIS ===\")\n",
        "\n",
        "# Ridge regularization path\n",
        "alphas_ridge = np.logspace(-3, 3, 50)\n",
        "ridge_coefs = []\n",
        "ridge_scores = []\n",
        "\n",
        "for alpha in alphas_ridge:\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train_scaled, y_train_comp)\n",
        "    ridge_coefs.append(ridge.coef_)\n",
        "    ridge_scores.append(ridge.score(X_test_scaled, y_test_comp))\n",
        "\n",
        "ridge_coefs = np.array(ridge_coefs)\n",
        "\n",
        "# Lasso regularization path\n",
        "alphas_lasso = np.logspace(-3, 1, 50)\n",
        "lasso_coefs = []\n",
        "lasso_scores = []\n",
        "\n",
        "for alpha in alphas_lasso:\n",
        "    lasso = Lasso(alpha=alpha, max_iter=2000)\n",
        "    lasso.fit(X_train_scaled, y_train_comp)\n",
        "    lasso_coefs.append(lasso.coef_)\n",
        "    lasso_scores.append(lasso.score(X_test_scaled, y_test_comp))\n",
        "\n",
        "lasso_coefs = np.array(lasso_coefs)\n",
        "\n",
        "# Plot regularization paths\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Ridge path\n",
        "for i in range(min(10, ridge_coefs.shape[1])):  # Plot first 10 features\n",
        "    axes[0,0].plot(alphas_ridge, ridge_coefs[:, i], alpha=0.7)\n",
        "axes[0,0].set_xscale('log')\n",
        "axes[0,0].set_xlabel('Alpha')\n",
        "axes[0,0].set_ylabel('Coefficient Value')\n",
        "axes[0,0].set_title('Ridge Regularization Path')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Lasso path\n",
        "for i in range(min(10, lasso_coefs.shape[1])):  # Plot first 10 features\n",
        "    axes[0,1].plot(alphas_lasso, lasso_coefs[:, i], alpha=0.7)\n",
        "axes[0,1].set_xscale('log')\n",
        "axes[0,1].set_xlabel('Alpha')\n",
        "axes[0,1].set_ylabel('Coefficient Value')\n",
        "axes[0,1].set_title('Lasso Regularization Path')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Ridge scores\n",
        "axes[1,0].plot(alphas_ridge, ridge_scores, 'b-', linewidth=2)\n",
        "axes[1,0].set_xscale('log')\n",
        "axes[1,0].set_xlabel('Alpha')\n",
        "axes[1,0].set_ylabel('Test R²')\n",
        "axes[1,0].set_title('Ridge: Test Performance vs Regularization')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Lasso scores\n",
        "axes[1,1].plot(alphas_lasso, lasso_scores, 'r-', linewidth=2)\n",
        "axes[1,1].set_xscale('log')\n",
        "axes[1,1].set_xlabel('Alpha')\n",
        "axes[1,1].set_ylabel('Test R²')\n",
        "axes[1,1].set_title('Lasso: Test Performance vs Regularization')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal alphas\n",
        "best_ridge_idx = np.argmax(ridge_scores)\n",
        "best_lasso_idx = np.argmax(lasso_scores)\n",
        "\n",
        "print(f\"Optimal Ridge alpha: {alphas_ridge[best_ridge_idx]:.6f} (R² = {ridge_scores[best_ridge_idx]:.6f})\")\n",
        "print(f\"Optimal Lasso alpha: {alphas_lasso[best_lasso_idx]:.6f} (R² = {lasso_scores[best_lasso_idx]:.6f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regularized Regression Insights:\n",
        "\n",
        "**Key Differences Between Methods:**\n",
        "\n",
        "1. **Linear Regression:**\n",
        "   - No regularization, can overfit with many features\n",
        "   - All features retained with potentially large coefficients\n",
        "   - Best when you have few features relative to samples\n",
        "\n",
        "2. **Ridge Regression (L2 Regularization):**\n",
        "   - Shrinks coefficients toward zero but doesn't eliminate them\n",
        "   - Handles multicollinearity well\n",
        "   - Good when all features are somewhat relevant\n",
        "   - Reduces overfitting while maintaining interpretability\n",
        "\n",
        "3. **Lasso Regression (L1 Regularization):**\n",
        "   - Performs automatic feature selection by setting coefficients to zero\n",
        "   - Creates sparse models (fewer features)\n",
        "   - Good when only subset of features are truly important\n",
        "   - Can be unstable with highly correlated features\n",
        "\n",
        "**When to Use Each Method:**\n",
        "\n",
        "- **Linear Regression:** Small datasets, few features, interpretability crucial\n",
        "- **Ridge:** Many features, multicollinearity, want to keep all features\n",
        "- **Lasso:** Many features, want automatic feature selection, sparse solutions\n",
        "\n",
        "**Business Applications:**\n",
        "- **Ridge:** Risk modeling where all factors matter\n",
        "- **Lasso:** Marketing attribution with many channels\n",
        "- **Linear:** Simple pricing models with few key factors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 8: Business Impact Analysis\n",
        "\n",
        "**Task:** Quantify the business value of your duration prediction model.\n",
        "\n",
        "### Solution Approach:\n",
        "1. Create engagement categories based on call duration\n",
        "2. Calculate classification accuracy for each category\n",
        "3. Estimate business value of correct predictions\n",
        "4. Develop actionable insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 8 Solution\n",
        "\n",
        "print(\"=== BUSINESS IMPACT ANALYSIS ===\")\n",
        "\n",
        "# Define engagement thresholds (log duration)\n",
        "low_threshold = np.log(2 * 60)    # 2 minutes = 120 seconds\n",
        "high_threshold = np.log(5 * 60)   # 5 minutes = 300 seconds\n",
        "\n",
        "print(f\"Engagement thresholds:\")\n",
        "print(f\"Low engagement: < {low_threshold:.3f} (< 2 minutes)\")\n",
        "print(f\"Medium engagement: {low_threshold:.3f} - {high_threshold:.3f} (2-5 minutes)\")\n",
        "print(f\"High engagement: > {high_threshold:.3f} (> 5 minutes)\")\n",
        "\n",
        "def categorize_engagement(log_duration):\n",
        "    \"\"\"\n",
        "    Categorize engagement based on log duration\n",
        "    \"\"\"\n",
        "    return np.where(log_duration < low_threshold, 'Low',\n",
        "                   np.where(log_duration < high_threshold, 'Medium', 'High'))\n",
        "\n",
        "# Categorize actual and predicted durations\n",
        "y_test_categories = categorize_engagement(y_test_banking.ravel())\n",
        "y_pred_categories = categorize_engagement(y_test_predicted_banking.ravel())\n",
        "\n",
        "# Calculate distribution of engagement levels\n",
        "engagement_dist = pd.Series(y_test_categories).value_counts(normalize=True).sort_index()\n",
        "print(f\"\\nActual engagement distribution:\")\n",
        "for level, pct in engagement_dist.items():\n",
        "    count = pd.Series(y_test_categories).value_counts()[level]\n",
        "    print(f\"{level}: {pct:.3f} ({count} calls)\")\n",
        "\n",
        "# Create confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "cm = confusion_matrix(y_test_categories, y_pred_categories, \n",
        "                     labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "cm_df = pd.DataFrame(cm, \n",
        "                    index=['Actual Low', 'Actual Medium', 'Actual High'],\n",
        "                    columns=['Pred Low', 'Pred Medium', 'Pred High'])\n",
        "print(cm_df)\n",
        "\n",
        "# Calculate accuracy for each engagement level\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_categories, y_pred_categories))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', \n",
        "            cbar_kws={'label': 'Number of Calls'})\n",
        "plt.title('Engagement Level Prediction: Confusion Matrix')\n",
        "plt.ylabel('Actual Engagement')\n",
        "plt.xlabel('Predicted Engagement')\n",
        "plt.show()\n",
        "\n",
        "# Calculate precision, recall, and F1 for each class\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    y_test_categories, y_pred_categories, labels=['Low', 'Medium', 'High']\n",
        ")\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Engagement Level': ['Low', 'Medium', 'High'],\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1,\n",
        "    'Support': support\n",
        "})\n",
        "\n",
        "print(\"\\nDetailed Metrics by Engagement Level:\")\n",
        "print(metrics_df.to_string(index=False, float_format='%.3f'))\n",
        "\n",
        "# Visualize metrics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "metrics_to_plot = ['Precision', 'Recall', 'F1-Score']\n",
        "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    axes[i].bar(metrics_df['Engagement Level'], metrics_df[metric], \n",
        "               color=colors[i], alpha=0.8)\n",
        "    axes[i].set_ylabel(metric)\n",
        "    axes[i].set_title(f'{metric} by Engagement Level')\n",
        "    axes[i].set_ylim(0, 1)\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for j, v in enumerate(metrics_df[metric]):\n",
        "        axes[i].text(j, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Business value calculation\n",
        "print(\"\\n=== BUSINESS VALUE ESTIMATION ===\")\n",
        "\n",
        "# Define business values (hypothetical but realistic)\n",
        "business_values = {\n",
        "    'cost_per_call': 5.0,  # Cost to make a follow-up call\n",
        "    'revenue_low': 50.0,   # Expected revenue from low engagement customer\n",
        "    'revenue_medium': 150.0,  # Expected revenue from medium engagement customer\n",
        "    'revenue_high': 400.0,    # Expected revenue from high engagement customer\n",
        "    'conversion_rate_low': 0.05,    # 5% conversion rate for low engagement\n",
        "    'conversion_rate_medium': 0.15,  # 15% conversion rate for medium engagement\n",
        "    'conversion_rate_high': 0.35,   # 35% conversion rate for high engagement\n",
        "}\n",
        "\n",
        "print(\"Business assumptions:\")\n",
        "for key, value in business_values.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Calculate expected value for each engagement level\n",
        "expected_values = {\n",
        "    'Low': business_values['revenue_low'] * business_values['conversion_rate_low'] - business_values['cost_per_call'],\n",
        "    'Medium': business_values['revenue_medium'] * business_values['conversion_rate_medium'] - business_values['cost_per_call'],\n",
        "    'High': business_values['revenue_high'] * business_values['conversion_rate_high'] - business_values['cost_per_call']\n",
        "}\n",
        "\n",
        "print(f\"\\nExpected value per follow-up call:\")\n",
        "for level, value in expected_values.items():\n",
        "    print(f\"{level} engagement: ${value:.2f}\")\n",
        "\n",
        "# Strategy 1: Follow up with all customers (baseline)\n",
        "total_calls_baseline = len(y_test_categories)\n",
        "baseline_value = sum(expected_values[level] * np.sum(y_test_categories == level) \n",
        "                    for level in ['Low', 'Medium', 'High'])\n",
        "\n",
        "print(f\"\\nBaseline strategy (follow up with all {total_calls_baseline} customers):\")\n",
        "print(f\"Total expected value: ${baseline_value:.2f}\")\n",
        "print(f\"Average value per call: ${baseline_value/total_calls_baseline:.2f}\")\n",
        "\n",
        "# Strategy 2: Only follow up with predicted high engagement customers\n",
        "high_pred_mask = y_pred_categories == 'High'\n",
        "high_pred_count = np.sum(high_pred_mask)\n",
        "high_pred_actual = y_test_categories[high_pred_mask]\n",
        "\n",
        "strategy2_value = sum(expected_values[level] * np.sum(high_pred_actual == level) \n",
        "                     for level in ['Low', 'Medium', 'High'])\n",
        "\n",
        "print(f\"\\nStrategy 2 (follow up only with predicted high engagement):\")\n",
        "print(f\"Calls made: {high_pred_count} ({high_pred_count/total_calls_baseline*100:.1f}% of total)\")\n",
        "print(f\"Total expected value: ${strategy2_value:.2f}\")\n",
        "print(f\"Average value per call: ${strategy2_value/high_pred_count:.2f}\" if high_pred_count > 0 else \"No calls made\")\n",
        "print(f\"Value vs baseline: ${strategy2_value - baseline_value:.2f}\")\n",
        "\n",
        "# Strategy 3: Follow up with predicted medium and high engagement\n",
        "med_high_pred_mask = (y_pred_categories == 'Medium') | (y_pred_categories == 'High')\n",
        "med_high_pred_count = np.sum(med_high_pred_mask)\n",
        "med_high_pred_actual = y_test_categories[med_high_pred_mask]\n",
        "\n",
        "strategy3_value = sum(expected_values[level] * np.sum(med_high_pred_actual == level) \n",
        "                     for level in ['Low', 'Medium', 'High'])\n",
        "\n",
        "print(f\"\\nStrategy 3 (follow up with predicted medium + high engagement):\")\n",
        "print(f\"Calls made: {med_high_pred_count} ({med_high_pred_count/total_calls_baseline*100:.1f}% of total)\")\n",
        "print(f\"Total expected value: ${strategy3_value:.2f}\")\n",
        "print(f\"Average value per call: ${strategy3_value/med_high_pred_count:.2f}\" if med_high_pred_count > 0 else \"No calls made\")\n",
        "print(f\"Value vs baseline: ${strategy3_value - baseline_value:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROI analysis and sensitivity testing\n",
        "print(\"\\n=== ROI ANALYSIS ===\")\n",
        "\n",
        "strategies = {\n",
        "    'Baseline (All)': {\n",
        "        'calls': total_calls_baseline,\n",
        "        'value': baseline_value,\n",
        "        'cost': total_calls_baseline * business_values['cost_per_call']\n",
        "    },\n",
        "    'High Only': {\n",
        "        'calls': high_pred_count,\n",
        "        'value': strategy2_value,\n",
        "        'cost': high_pred_count * business_values['cost_per_call']\n",
        "    },\n",
        "    'Medium + High': {\n",
        "        'calls': med_high_pred_count,\n",
        "        'value': strategy3_value,\n",
        "        'cost': med_high_pred_count * business_values['cost_per_call']\n",
        "    }\n",
        "}\n",
        "\n",
        "roi_data = []\n",
        "for strategy_name, data in strategies.items():\n",
        "    revenue = data['value'] + data['cost']  # Add back the cost to get gross revenue\n",
        "    profit = data['value']\n",
        "    roi = (profit / data['cost']) * 100 if data['cost'] > 0 else 0\n",
        "    \n",
        "    roi_data.append({\n",
        "        'Strategy': strategy_name,\n",
        "        'Calls': data['calls'],\n",
        "        'Cost': data['cost'],\n",
        "        'Revenue': revenue,\n",
        "        'Profit': profit,\n",
        "        'ROI (%)': roi,\n",
        "        'Profit per Call': profit / data['calls'] if data['calls'] > 0 else 0\n",
        "    })\n",
        "\n",
        "roi_df = pd.DataFrame(roi_data)\n",
        "print(roi_df.to_string(index=False, float_format='%.2f'))\n",
        "\n",
        "# Visualize ROI comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Profit comparison\n",
        "axes[0,0].bar(roi_df['Strategy'], roi_df['Profit'], alpha=0.8, color='green')\n",
        "axes[0,0].set_ylabel('Total Profit ($)')\n",
        "axes[0,0].set_title('Total Profit by Strategy')\n",
        "axes[0,0].tick_params(axis='x', rotation=45)\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# ROI comparison\n",
        "axes[0,1].bar(roi_df['Strategy'], roi_df['ROI (%)'], alpha=0.8, color='blue')\n",
        "axes[0,1].set_ylabel('ROI (%)')\n",
        "axes[0,1].set_title('Return on Investment by Strategy')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Calls vs Profit\n",
        "axes[1,0].scatter(roi_df['Calls'], roi_df['Profit'], s=100, alpha=0.8)\n",
        "for i, strategy in enumerate(roi_df['Strategy']):\n",
        "    axes[1,0].annotate(strategy, (roi_df['Calls'].iloc[i], roi_df['Profit'].iloc[i]),\n",
        "                      xytext=(5, 5), textcoords='offset points')\n",
        "axes[1,0].set_xlabel('Number of Calls')\n",
        "axes[1,0].set_ylabel('Total Profit ($)')\n",
        "axes[1,0].set_title('Calls vs Profit')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Profit per call\n",
        "axes[1,1].bar(roi_df['Strategy'], roi_df['Profit per Call'], alpha=0.8, color='orange')\n",
        "axes[1,1].set_ylabel('Profit per Call ($)')\n",
        "axes[1,1].set_title('Profit per Call by Strategy')\n",
        "axes[1,1].tick_params(axis='x', rotation=45)\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sensitivity analysis\n",
        "print(\"\\n=== SENSITIVITY ANALYSIS ===\")\n",
        "\n",
        "# Test different cost scenarios\n",
        "cost_scenarios = [2.0, 5.0, 10.0, 15.0]  # Different cost per call\n",
        "sensitivity_results = []\n",
        "\n",
        "for cost in cost_scenarios:\n",
        "    # Recalculate expected values\n",
        "    exp_val_low = business_values['revenue_low'] * business_values['conversion_rate_low'] - cost\n",
        "    exp_val_med = business_values['revenue_medium'] * business_values['conversion_rate_medium'] - cost\n",
        "    exp_val_high = business_values['revenue_high'] * business_values['conversion_rate_high'] - cost\n",
        "    \n",
        "    # Baseline strategy value\n",
        "    baseline_val = (exp_val_low * np.sum(y_test_categories == 'Low') +\n",
        "                   exp_val_med * np.sum(y_test_categories == 'Medium') +\n",
        "                   exp_val_high * np.sum(y_test_categories == 'High'))\n",
        "    \n",
        "    # High-only strategy value\n",
        "    high_only_val = (exp_val_low * np.sum(high_pred_actual == 'Low') +\n",
        "                    exp_val_med * np.sum(high_pred_actual == 'Medium') +\n",
        "                    exp_val_high * np.sum(high_pred_actual == 'High'))\n",
        "    \n",
        "    sensitivity_results.append({\n",
        "        'Cost per Call': cost,\n",
        "        'Baseline Profit': baseline_val,\n",
        "        'High-Only Profit': high_only_val,\n",
        "        'Improvement': high_only_val - baseline_val,\n",
        "        'Improvement %': ((high_only_val - baseline_val) / abs(baseline_val)) * 100 if baseline_val != 0 else 0\n",
        "    })\n",
        "\n",
        "sensitivity_df = pd.DataFrame(sensitivity_results)\n",
        "print(\"Sensitivity to Cost per Call:\")\n",
        "print(sensitivity_df.to_string(index=False, float_format='%.2f'))\n",
        "\n",
        "# Plot sensitivity\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(sensitivity_df['Cost per Call'], sensitivity_df['Baseline Profit'], \n",
        "         'b-o', label='Baseline (All Customers)', linewidth=2)\n",
        "plt.plot(sensitivity_df['Cost per Call'], sensitivity_df['High-Only Profit'], \n",
        "         'r-o', label='High Engagement Only', linewidth=2)\n",
        "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "plt.xlabel('Cost per Call ($)')\n",
        "plt.ylabel('Total Profit ($)')\n",
        "plt.title('Profit Sensitivity to Cost per Call')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Find break-even point\n",
        "break_even_cost = business_values['revenue_high'] * business_values['conversion_rate_high']\n",
        "print(f\"\\nBreak-even cost per call for high engagement: ${break_even_cost:.2f}\")\n",
        "print(f\"Current cost assumption: ${business_values['cost_per_call']:.2f}\")\n",
        "print(f\"Safety margin: {((break_even_cost - business_values['cost_per_call']) / business_values['cost_per_call']) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Business Impact Analysis Insights:\n",
        "\n",
        "**Key Business Findings:**\n",
        "\n",
        "1. **Model Performance by Engagement Level:**\n",
        "   - High engagement customers are most valuable but hardest to predict\n",
        "   - Medium engagement provides good balance of value and predictability\n",
        "   - Low engagement customers have negative expected value\n",
        "\n",
        "2. **Strategic Recommendations:**\n",
        "   - **Selective Follow-up:** Only contact predicted medium/high engagement customers\n",
        "   - **Resource Optimization:** Focus limited resources on highest-value prospects\n",
        "   - **Cost Management:** Monitor cost per call to maintain profitability\n",
        "\n",
        "3. **Financial Impact:**\n",
        "   - Model-driven strategies can significantly improve ROI\n",
        "   - Profit per call increases when focusing on high-value segments\n",
        "   - Break-even analysis helps set cost thresholds\n",
        "\n",
        "**Implementation Considerations:**\n",
        "\n",
        "1. **Model Accuracy Trade-offs:**\n",
        "   - False positives: Waste resources on low-value customers\n",
        "   - False negatives: Miss high-value opportunities\n",
        "   - Optimize threshold based on business costs\n",
        "\n",
        "2. **Operational Changes:**\n",
        "   - Train call center staff on engagement indicators\n",
        "   - Implement real-time scoring system\n",
        "   - Monitor and update model performance regularly\n",
        "\n",
        "3. **Risk Management:**\n",
        "   - Test strategies on small samples first\n",
        "   - Monitor customer satisfaction impacts\n",
        "   - Have fallback procedures for model failures\n",
        "\n",
        "**Success Metrics:**\n",
        "- Increase in profit per call\n",
        "- Improvement in conversion rates\n",
        "- Reduction in wasted follow-up calls\n",
        "- Overall ROI improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 9: Advanced Diagnostics\n",
        "\n",
        "**Task:** Perform advanced model diagnostics to identify potential issues.\n",
        "\n",
        "### Solution Approach:\n",
        "1. Calculate Cook's distance for influential observations\n",
        "2. Check multicollinearity using Variance Inflation Factor (VIF)\n",
        "3. Test for autocorrelation in residuals\n",
        "4. Provide improvement recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 9 Solution\n",
        "\n",
        "print(\"=== ADVANCED MODEL DIAGNOSTICS ===\")\n",
        "\n",
        "# First, let's use statsmodels for more detailed diagnostics\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import OLSInfluence\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "\n",
        "# Prepare data for statsmodels (add constant for intercept)\n",
        "X_train_sm = sm.add_constant(X_train_banking)\n",
        "X_test_sm = sm.add_constant(X_test_banking)\n",
        "\n",
        "# Fit OLS model using statsmodels\n",
        "model_sm = sm.OLS(y_train_banking.ravel(), X_train_sm).fit()\n",
        "\n",
        "print(\"Model Summary:\")\n",
        "print(model_sm.summary())\n",
        "\n",
        "# Get predictions and residuals\n",
        "y_train_pred_sm = model_sm.predict(X_train_sm)\n",
        "residuals_sm = model_sm.resid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Cook's Distance Analysis\n",
        "print(\"\\n=== COOK'S DISTANCE ANALYSIS ===\")\n",
        "\n",
        "# Calculate influence measures\n",
        "influence = OLSInfluence(model_sm)\n",
        "cooks_d = influence.cooks_distance[0]\n",
        "leverage = influence.hat_matrix_diag\n",
        "studentized_residuals = influence.resid_studentized_external\n",
        "\n",
        "# Identify influential observations\n",
        "n = len(X_train_sm)\n",
        "p = X_train_sm.shape[1]\n",
        "cooks_threshold = 4 / n  # Common threshold\n",
        "leverage_threshold = 2 * p / n  # Common threshold\n",
        "\n",
        "influential_cooks = np.where(cooks_d > cooks_threshold)[0]\n",
        "high_leverage = np.where(leverage > leverage_threshold)[0]\n",
        "outliers = np.where(np.abs(studentized_residuals) > 3)[0]  # |t| > 3\n",
        "\n",
        "print(f\"Cook's distance threshold: {cooks_threshold:.6f}\")\n",
        "print(f\"Leverage threshold: {leverage_threshold:.6f}\")\n",
        "print(f\"Influential observations (Cook's D): {len(influential_cooks)} ({len(influential_cooks)/n*100:.2f}%)\")\n",
        "print(f\"High leverage observations: {len(high_leverage)} ({len(high_leverage)/n*100:.2f}%)\")\n",
        "print(f\"Outliers (|studentized residual| > 3): {len(outliers)} ({len(outliers)/n*100:.2f}%)\")\n",
        "\n",
        "# Plot diagnostic plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Cook's distance plot\n",
        "axes[0,0].stem(range(len(cooks_d)), cooks_d, basefmt=\" \")\n",
        "axes[0,0].axhline(y=cooks_threshold, color='red', linestyle='--', label=f'Threshold ({cooks_threshold:.4f})')\n",
        "axes[0,0].set_xlabel('Observation Index')\n",
        "axes[0,0].set_ylabel(\"Cook's Distance\")\n",
        "axes[0,0].set_title(\"Cook's Distance\")\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Leverage plot\n",
        "axes[0,1].scatter(range(len(leverage)), leverage, alpha=0.6)\n",
        "axes[0,1].axhline(y=leverage_threshold, color='red', linestyle='--', label=f'Threshold ({leverage_threshold:.4f})')\n",
        "axes[0,1].set_xlabel('Observation Index')\n",
        "axes[0,1].set_ylabel('Leverage')\n",
        "axes[0,1].set_title('Leverage Values')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals vs Leverage\n",
        "axes[1,0].scatter(leverage, studentized_residuals, alpha=0.6)\n",
        "axes[1,0].axhline(y=3, color='red', linestyle='--', alpha=0.7)\n",
        "axes[1,0].axhline(y=-3, color='red', linestyle='--', alpha=0.7)\n",
        "axes[1,0].axvline(x=leverage_threshold, color='red', linestyle='--', alpha=0.7)\n",
        "axes[1,0].set_xlabel('Leverage')\n",
        "axes[1,0].set_ylabel('Studentized Residuals')\n",
        "axes[1,0].set_title('Residuals vs Leverage')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Cook's distance vs Leverage\n",
        "axes[1,1].scatter(leverage, cooks_d, alpha=0.6)\n",
        "axes[1,1].axhline(y=cooks_threshold, color='red', linestyle='--', alpha=0.7)\n",
        "axes[1,1].axvline(x=leverage_threshold, color='red', linestyle='--', alpha=0.7)\n",
        "axes[1,1].set_xlabel('Leverage')\n",
        "axes[1,1].set_ylabel(\"Cook's Distance\")\n",
        "axes[1,1].set_title(\"Cook's Distance vs Leverage\")\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show most influential observations\n",
        "if len(influential_cooks) > 0:\n",
        "    print(f\"\\nTop 10 most influential observations (Cook's Distance):\")\n",
        "    top_influential = np.argsort(cooks_d)[-10:][::-1]\n",
        "    for i, idx in enumerate(top_influential):\n",
        "        print(f\"{i+1}. Index {idx}: Cook's D = {cooks_d[idx]:.6f}, Leverage = {leverage[idx]:.6f}, Studentized Residual = {studentized_residuals[idx]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Multicollinearity Analysis (VIF)\n",
        "print(\"\\n=== MULTICOLLINEARITY ANALYSIS (VIF) ===\")\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Calculate VIF for each feature (excluding constant)\n",
        "X_for_vif = X_train_sm.iloc[:, 1:]  # Exclude constant column\n",
        "vif_data = []\n",
        "\n",
        "for i in range(X_for_vif.shape[1]):\n",
        "    try:\n",
        "        vif = variance_inflation_factor(X_for_vif.values, i)\n",
        "        vif_data.append({\n",
        "            'Feature': X_df_banking.columns[i],\n",
        "            'VIF': vif\n",
        "        })\n",
        "    except:\n",
        "        vif_data.append({\n",
        "            'Feature': X_df_banking.columns[i],\n",
        "            'VIF': np.inf  # Perfect multicollinearity\n",
        "        })\n",
        "\n",
        "vif_df = pd.DataFrame(vif_data)\n",
        "vif_df = vif_df.sort_values('VIF', ascending=False)\n",
        "\n",
        "print(\"Variance Inflation Factors:\")\n",
        "print(\"VIF > 10: High multicollinearity\")\n",
        "print(\"VIF > 5: Moderate multicollinearity\")\n",
        "print(\"VIF < 5: Low multicollinearity\")\n",
        "print()\n",
        "print(vif_df.to_string(index=False, float_format='%.3f'))\n",
        "\n",
        "# Identify problematic features\n",
        "high_vif = vif_df[vif_df['VIF'] > 10]\n",
        "moderate_vif = vif_df[(vif_df['VIF'] > 5) & (vif_df['VIF'] <= 10)]\n",
        "\n",
        "print(f\"\\nFeatures with high multicollinearity (VIF > 10): {len(high_vif)}\")\n",
        "if len(high_vif) > 0:\n",
        "    print(high_vif[['Feature', 'VIF']].to_string(index=False))\n",
        "\n",
        "print(f\"\\nFeatures with moderate multicollinearity (5 < VIF <= 10): {len(moderate_vif)}\")\n",
        "if len(moderate_vif) > 0:\n",
        "    print(moderate_vif[['Feature', 'VIF']].to_string(index=False))\n",
        "\n",
        "# Visualize VIF\n",
        "plt.figure(figsize=(12, 8))\n",
        "colors = ['red' if vif > 10 else 'orange' if vif > 5 else 'green' for vif in vif_df['VIF']]\n",
        "plt.barh(range(len(vif_df)), vif_df['VIF'], color=colors, alpha=0.7)\n",
        "plt.yticks(range(len(vif_df)), vif_df['Feature'])\n",
        "plt.xlabel('Variance Inflation Factor (VIF)')\n",
        "plt.title('Multicollinearity Analysis: VIF by Feature')\n",
        "plt.axvline(x=5, color='orange', linestyle='--', alpha=0.7, label='Moderate threshold (5)')\n",
        "plt.axvline(x=10, color='red', linestyle='--', alpha=0.7, label='High threshold (10)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Heteroscedasticity Tests\n",
        "print(\"\\n=== HETEROSCEDASTICITY TESTS ===\")\n",
        "\n",
        "# Breusch-Pagan test\n",
        "bp_stat, bp_pvalue, bp_fstat, bp_fpvalue = het_breuschpagan(residuals_sm, X_train_sm)\n",
        "print(f\"Breusch-Pagan Test:\")\n",
        "print(f\"  LM statistic: {bp_stat:.6f}\")\n",
        "print(f\"  p-value: {bp_pvalue:.6f}\")\n",
        "print(f\"  Interpretation: {'Heteroscedasticity detected' if bp_pvalue < 0.05 else 'Homoscedasticity (constant variance)'}\")\n",
        "\n",
        "# White test\n",
        "white_stat, white_pvalue, white_fstat, white_fpvalue = het_white(residuals_sm, X_train_sm)\n",
        "print(f\"\\nWhite Test:\")\n",
        "print(f\"  LM statistic: {white_stat:.6f}\")\n",
        "print(f\"  p-value: {white_pvalue:.6f}\")\n",
        "print(f\"  Interpretation: {'Heteroscedasticity detected' if white_pvalue < 0.05 else 'Homoscedasticity (constant variance)'}\")\n",
        "\n",
        "# 4. Autocorrelation Test (Durbin-Watson)\n",
        "print(\"\\n=== AUTOCORRELATION TEST ===\")\n",
        "\n",
        "dw_stat = durbin_watson(residuals_sm)\n",
        "print(f\"Durbin-Watson statistic: {dw_stat:.6f}\")\n",
        "print(f\"Interpretation:\")\n",
        "print(f\"  DW ≈ 2: No autocorrelation\")\n",
        "print(f\"  DW < 2: Positive autocorrelation\")\n",
        "print(f\"  DW > 2: Negative autocorrelation\")\n",
        "if dw_stat < 1.5:\n",
        "    print(f\"  Result: Strong positive autocorrelation detected\")\n",
        "elif dw_stat < 1.8:\n",
        "    print(f\"  Result: Moderate positive autocorrelation\")\n",
        "elif dw_stat > 2.5:\n",
        "    print(f\"  Result: Strong negative autocorrelation detected\")\n",
        "elif dw_stat > 2.2:\n",
        "    print(f\"  Result: Moderate negative autocorrelation\")\n",
        "else:\n",
        "    print(f\"  Result: No significant autocorrelation\")\n",
        "\n",
        "# 5. Normality Tests (additional)\n",
        "print(\"\\n=== ADDITIONAL NORMALITY TESTS ===\")\n",
        "\n",
        "from scipy.stats import jarque_bera, shapiro, normaltest\n",
        "\n",
        "# Jarque-Bera test\n",
        "jb_stat, jb_pvalue = jarque_bera(residuals_sm)\n",
        "print(f\"Jarque-Bera Test:\")\n",
        "print(f\"  Statistic: {jb_stat:.6f}\")\n",
        "print(f\"  p-value: {jb_pvalue:.6f}\")\n",
        "print(f\"  Interpretation: {'Residuals deviate from normality' if jb_pvalue < 0.05 else 'Residuals appear normal'}\")\n",
        "\n",
        "# D'Agostino's normality test\n",
        "dag_stat, dag_pvalue = normaltest(residuals_sm)\n",
        "print(f\"\\nD'Agostino's Normality Test:\")\n",
        "print(f\"  Statistic: {dag_stat:.6f}\")\n",
        "print(f\"  p-value: {dag_pvalue:.6f}\")\n",
        "print(f\"  Interpretation: {'Residuals deviate from normality' if dag_pvalue < 0.05 else 'Residuals appear normal'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Model Improvement Recommendations\n",
        "print(\"\\n=== MODEL IMPROVEMENT RECOMMENDATIONS ===\")\n",
        "\n",
        "recommendations = []\n",
        "\n",
        "# Based on influential observations\n",
        "if len(influential_cooks) > len(X_train_sm) * 0.05:  # More than 5% influential\n",
        "    recommendations.append(\n",
        "        f\"🔍 INFLUENTIAL OBSERVATIONS: {len(influential_cooks)} observations have high Cook's distance. \"\n",
        "        \"Consider investigating these data points for errors or outliers.\"\n",
        "    )\n",
        "\n",
        "# Based on multicollinearity\n",
        "if len(high_vif) > 0:\n",
        "    recommendations.append(\n",
        "        f\"⚠️ MULTICOLLINEARITY: {len(high_vif)} features have VIF > 10. \"\n",
        "        \"Consider removing highly correlated features or using regularization (Ridge/Lasso).\"\n",
        "    )\n",
        "\n",
        "# Based on heteroscedasticity\n",
        "if bp_pvalue < 0.05 or white_pvalue < 0.05:\n",
        "    recommendations.append(\n",
        "        \"📊 HETEROSCEDASTICITY: Non-constant variance detected. \"\n",
        "        \"Consider using robust standard errors, weighted least squares, or transforming the target variable.\"\n",
        "    )\n",
        "\n",
        "# Based on autocorrelation\n",
        "if abs(dw_stat - 2) > 0.5:\n",
        "    recommendations.append(\n",
        "        \"🔄 AUTOCORRELATION: Residuals show autocorrelation. \"\n",
        "        \"Consider adding lagged variables or using time series models if data has temporal structure.\"\n",
        "    )\n",
        "\n",
        "# Based on normality\n",
        "if jb_pvalue < 0.05:\n",
        "    recommendations.append(\n",
        "        \"📈 NON-NORMALITY: Residuals deviate from normality. \"\n",
        "        \"Consider transforming the target variable (log, Box-Cox) or using robust regression methods.\"\n",
        "    )\n",
        "\n",
        "# Model complexity\n",
        "if len(X_df_banking.columns) > len(X_train_sm) / 10:  # More features than 10% of observations\n",
        "    recommendations.append(\n",
        "        \"🎯 MODEL COMPLEXITY: High feature-to-observation ratio. \"\n",
        "        \"Consider feature selection, dimensionality reduction (PCA), or regularization.\"\n",
        "    )\n",
        "\n",
        "if len(recommendations) == 0:\n",
        "    print(\"✅ MODEL DIAGNOSTICS PASSED: No major issues detected!\")\n",
        "    print(\"The model appears to satisfy linear regression assumptions reasonably well.\")\n",
        "else:\n",
        "    print(\"Issues detected and recommendations:\")\n",
        "    for i, rec in enumerate(recommendations, 1):\n",
        "        print(f\"\\n{i}. {rec}\")\n",
        "\n",
        "# Specific actionable steps\n",
        "print(\"\\n=== SPECIFIC ACTIONABLE STEPS ===\")\n",
        "\n",
        "action_steps = [\n",
        "    \"1. DATA QUALITY:\",\n",
        "    \"   - Investigate influential observations for data entry errors\",\n",
        "    \"   - Consider robust regression methods for outlier handling\",\n",
        "    \"   - Validate extreme values with domain experts\",\n",
        "    \"\",\n",
        "    \"2. FEATURE ENGINEERING:\",\n",
        "    \"   - Remove or combine highly correlated features (VIF > 10)\",\n",
        "    \"   - Consider polynomial or interaction terms for non-linearity\",\n",
        "    \"   - Apply feature scaling/normalization\",\n",
        "    \"\",\n",
        "    \"3. MODEL ALTERNATIVES:\",\n",
        "    \"   - Try regularized regression (Ridge/Lasso) for multicollinearity\",\n",
        "    \"   - Consider robust regression for outliers\",\n",
        "    \"   - Explore non-linear models if assumptions are severely violated\",\n",
        "    \"\",\n",
        "    \"4. VALIDATION:\",\n",
        "    \"   - Use cross-validation for robust performance estimates\",\n",
        "    \"   - Monitor model performance on new data\",\n",
        "    \"   - Implement model retraining procedures\"\n",
        "]\n",
        "\n",
        "for step in action_steps:\n",
        "    print(step)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n=== DIAGNOSTIC SUMMARY ===\")\n",
        "summary_stats = {\n",
        "    'Total Observations': len(X_train_sm),\n",
        "    'Features': X_train_sm.shape[1] - 1,  # Exclude constant\n",
        "    'Influential Obs (%)': len(influential_cooks) / len(X_train_sm) * 100,\n",
        "    'High VIF Features': len(high_vif),\n",
        "    'Heteroscedasticity p-value': min(bp_pvalue, white_pvalue),\n",
        "    'Durbin-Watson Stat': dw_stat,\n",
        "    'Normality p-value': jb_pvalue,\n",
        "    'Model R²': model_sm.rsquared,\n",
        "    'Adjusted R²': model_sm.rsquared_adj\n",
        "}\n",
        "\n",
        "for key, value in summary_stats.items():\n",
        "    if 'p-value' in key:\n",
        "        print(f\"{key}: {value:.6f}\")\n",
        "    elif '%' in key or 'R²' in key or 'Stat' in key:\n",
        "        print(f\"{key}: {value:.3f}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Diagnostics Insights:\n",
        "\n",
        "**Key Diagnostic Tools:**\n",
        "\n",
        "1. **Cook's Distance:**\n",
        "   - Identifies observations that heavily influence model coefficients\n",
        "   - High values indicate potential outliers or data errors\n",
        "   - Threshold: 4/n (where n = sample size)\n",
        "\n",
        "2. **Variance Inflation Factor (VIF):**\n",
        "   - Measures multicollinearity between features\n",
        "   - VIF > 10: High multicollinearity (problematic)\n",
        "   - VIF > 5: Moderate multicollinearity (concerning)\n",
        "\n",
        "3. **Heteroscedasticity Tests:**\n",
        "   - Breusch-Pagan: Tests for linear heteroscedasticity\n",
        "   - White Test: Tests for general heteroscedasticity\n",
        "   - Violation affects standard errors and confidence intervals\n",
        "\n",
        "4. **Durbin-Watson Test:**\n",
        "   - Tests for autocorrelation in residuals\n",
        "   - Important for time series or ordered data\n",
        "   - Values near 2 indicate no autocorrelation\n",
        "\n",
        "**Common Issues and Solutions:**\n",
        "\n",
        "1. **Influential Observations:**\n",
        "   - Investigate for data errors\n",
        "   - Consider robust regression methods\n",
        "   - Use outlier detection algorithms\n",
        "\n",
        "2. **Multicollinearity:**\n",
        "   - Remove highly correlated features\n",
        "   - Use regularization (Ridge/Lasso)\n",
        "   - Apply principal component analysis (PCA)\n",
        "\n",
        "3. **Heteroscedasticity:**\n",
        "   - Transform target variable (log, square root)\n",
        "   - Use weighted least squares\n",
        "   - Apply robust standard errors\n",
        "\n",
        "4. **Non-normality:**\n",
        "   - Transform variables (Box-Cox, log)\n",
        "   - Use robust regression methods\n",
        "   - Consider non-parametric alternatives\n",
        "\n",
        "**Best Practices:**\n",
        "- Always check assumptions before interpreting results\n",
        "- Use multiple diagnostic tests for robustness\n",
        "- Document any assumption violations and remedial actions\n",
        "- Consider the business impact of model limitations\n",
        "- Regularly re-evaluate model diagnostics with new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection Questions - Detailed Answers\n",
        "\n",
        "### 1. When might linear regression not be appropriate?\n",
        "\n",
        "**Non-linear relationships:**\n",
        "- When the relationship between features and target is curved, exponential, or has other non-linear patterns\n",
        "- Example: Population growth, compound interest, or diminishing returns\n",
        "\n",
        "**Categorical outcomes:**\n",
        "- Linear regression predicts continuous values, not categories\n",
        "- Use logistic regression for binary outcomes, multinomial regression for multiple categories\n",
        "\n",
        "**Assumption violations:**\n",
        "- Severe heteroscedasticity (non-constant variance)\n",
        "- Strong autocorrelation in residuals\n",
        "- Non-normal residuals with small sample sizes\n",
        "- Extreme multicollinearity\n",
        "\n",
        "**Alternative approaches:**\n",
        "- Polynomial regression for curved relationships\n",
        "- Tree-based models for complex interactions\n",
        "- Neural networks for highly non-linear patterns\n",
        "\n",
        "### 2. How do you balance model complexity with interpretability?\n",
        "\n",
        "**Start simple:**\n",
        "- Begin with basic linear model\n",
        "- Add complexity only when justified by performance gains\n",
        "- Use statistical tests to validate additional features\n",
        "\n",
        "**Use regularization:**\n",
        "- Ridge regression maintains all features but shrinks coefficients\n",
        "- Lasso automatically selects important features\n",
        "- Elastic Net combines both approaches\n",
        "\n",
        "**Business context matters:**\n",
        "- Regulatory environments may require interpretable models\n",
        "- High-stakes decisions need explainable predictions\n",
        "- Operational teams need to understand model logic\n",
        "\n",
        "**Practical strategies:**\n",
        "- Create separate models for different purposes (simple for explanation, complex for prediction)\n",
        "- Use feature importance rankings to focus on key drivers\n",
        "- Provide model summaries at different technical levels\n",
        "\n",
        "### 3. What are the key assumptions of linear regression and why do they matter?\n",
        "\n",
        "**Linearity:**\n",
        "- Relationship between features and target is linear\n",
        "- Violation: Biased predictions, poor fit\n",
        "- Business impact: Wrong understanding of factor relationships\n",
        "\n",
        "**Independence:**\n",
        "- Observations are independent of each other\n",
        "- Violation: Underestimated standard errors, overconfident predictions\n",
        "- Business impact: False confidence in model reliability\n",
        "\n",
        "**Homoscedasticity:**\n",
        "- Constant variance of residuals\n",
        "- Violation: Unreliable confidence intervals\n",
        "- Business impact: Incorrect uncertainty estimates for decisions\n",
        "\n",
        "**Normality:**\n",
        "- Residuals follow normal distribution\n",
        "- Violation: Invalid hypothesis tests, poor confidence intervals\n",
        "- Business impact: Unreliable statistical inference\n",
        "\n",
        "**No multicollinearity:**\n",
        "- Features are not highly correlated\n",
        "- Violation: Unstable coefficients, difficult interpretation\n",
        "- Business impact: Wrong conclusions about factor importance\n",
        "\n",
        "### 4. How would you explain R² to a non-technical business stakeholder?\n",
        "\n",
        "**Simple explanation:**\n",
        "\"R² tells us what percentage of the variation in our target variable is explained by our model. It's like asking: 'How much of the ups and downs in our data can we predict using our features?'\"\n",
        "\n",
        "**Practical interpretation:**\n",
        "- R² = 0.80 means \"Our model explains 80% of why values vary\"\n",
        "- R² = 0.30 means \"Our model captures 30% of the pattern, 70% is due to other factors\"\n",
        "\n",
        "**Business context:**\n",
        "- Higher R² = More predictable outcomes\n",
        "- Lower R² = More uncertainty, need additional factors\n",
        "- Perfect R² (1.0) is rare in real business data\n",
        "\n",
        "**Avoid common misconceptions:**\n",
        "- R² doesn't prove causation\n",
        "- Higher R² doesn't always mean better business decisions\n",
        "- R² can be misleading with small samples or many features\n",
        "\n",
        "### 5. In what business scenarios would you prefer RMSE over R² as an evaluation metric?\n",
        "\n",
        "**When absolute errors matter:**\n",
        "- Financial forecasting: $1000 error has real cost regardless of scale\n",
        "- Inventory management: Overstocking/understocking has direct costs\n",
        "- Resource planning: Wrong headcount predictions affect operations\n",
        "\n",
        "**When comparing models with different scales:**\n",
        "- RMSE has same units as target variable\n",
        "- Easier to interpret business impact\n",
        "- Can set acceptable error thresholds\n",
        "\n",
        "**When stakeholders need concrete numbers:**\n",
        "- \"Average error is $500\" vs \"Model explains 85% of variance\"\n",
        "- RMSE directly relates to business costs\n",
        "- Easier to set performance targets\n",
        "\n",
        "**Examples:**\n",
        "- Sales forecasting: RMSE shows average dollar error\n",
        "- Demand planning: RMSE indicates typical unit shortage/surplus\n",
        "- Budget planning: RMSE reveals expected deviation from targets\n",
        "\n",
        "### 6. How might you improve the Bitcoin forecasting model?\n",
        "\n",
        "**Additional features:**\n",
        "- Market sentiment indicators (fear/greed index)\n",
        "- Trading volume and volatility measures\n",
        "- Macroeconomic indicators (interest rates, inflation)\n",
        "- Social media sentiment and news analysis\n",
        "\n",
        "**Advanced techniques:**\n",
        "- GARCH models for volatility clustering\n",
        "- ARIMA models for time series patterns\n",
        "- Ensemble methods combining multiple models\n",
        "- Deep learning for complex pattern recognition\n",
        "\n",
        "**Risk management:**\n",
        "- Implement position sizing based on prediction confidence\n",
        "- Add stop-loss and take-profit mechanisms\n",
        "- Consider transaction costs and market impact\n",
        "- Regular model retraining and performance monitoring\n",
        "\n",
        "**Practical considerations:**\n",
        "- Real-time data feeds for timely predictions\n",
        "- Backtesting with realistic trading constraints\n",
        "- Stress testing under different market conditions\n",
        "- Integration with existing trading infrastructure\n",
        "\n",
        "## Final Takeaways\n",
        "\n",
        "**Linear regression strengths:**\n",
        "- Simple, interpretable, and fast\n",
        "- Good baseline for more complex models\n",
        "- Well-understood statistical properties\n",
        "- Effective when assumptions are met\n",
        "\n",
        "**Key success factors:**\n",
        "- Always check and validate assumptions\n",
        "- Focus on business value, not just statistical metrics\n",
        "- Use appropriate evaluation methods (cross-validation)\n",
        "- Consider model limitations in decision-making\n",
        "\n",
        "**Best practices:**\n",
        "- Start simple, add complexity gradually\n",
        "- Document assumptions and limitations\n",
        "- Regular model monitoring and updates\n",
        "- Clear communication with stakeholders\n",
        "\n",
        "**Remember:** The best model is not always the most complex one, but the one that provides reliable, actionable insights for business decisions while being appropriately validated and understood by its users."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
