{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prEL8RD9uMQt"
      },
      "source": [
        "# EDFB - Digital Finance & Banking - Linear Probability Model and Logistic Regression\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The following script provides examples on how to model binary outcomes using both linear probability models and logistic regression in Python. We'll start with the linear probability model to understand its limitations, then move to logistic regression as a more appropriate approach for binary dependent variables. In order to run the script, you need to download the dataset \"banking.csv\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT1tRxsjuDXq"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import io\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSS2APamuO32"
      },
      "outputs": [],
      "source": [
        "# To make this notebook's output stable across runs (we make the output reproducable)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "collapsed": true,
        "id": "bPsaJ-cluP2X",
        "outputId": "51c80726-98f2-4bd8-8bdf-9eb466011bb7"
      },
      "outputs": [],
      "source": [
        "# Import real data from GitHub\n",
        "banking_url = \"https://raw.githubusercontent.com/umatter/EDFB/main/data/banking.csv\"\n",
        "print(\"Fetching banking.csv from GitHub...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf_QsBcBuRCR"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(banking_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N6GHSsdJuTlI",
        "outputId": "d07ee88c-2c8d-49b7-93e5-19b5b6a6d016"
      },
      "outputs": [],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "sJuw_a-ouV5a",
        "outputId": "cb4ef07e-d25a-46d9-80ff-9f098c23cdc2"
      },
      "outputs": [],
      "source": [
        "dataset.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz52USwauXU5"
      },
      "outputs": [],
      "source": [
        "# Define set of numerical and categorical variables\n",
        "num_var = dataset.drop(columns=['y']).select_dtypes([np.number]).columns\n",
        "cat_var = dataset.drop(columns=['y']).select_dtypes(include=object).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okZOkGVYuXwT",
        "outputId": "83ead278-54b1-4cdf-f532-b85acbcbb3cf"
      },
      "outputs": [],
      "source": [
        "num_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcFf9r-muZ_j",
        "outputId": "0bdebc4a-bf5d-4588-db5f-1de31366ddcd"
      },
      "outputs": [],
      "source": [
        "cat_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "yKcy8Bkoua86",
        "outputId": "d1757815-939b-4741-872c-84373d5d4f85"
      },
      "outputs": [],
      "source": [
        "# Check NAs\n",
        "dataset.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "fq-JZbVnucLU",
        "outputId": "c6d26a1b-f0f7-44ee-e632-4adadf2343a4"
      },
      "outputs": [],
      "source": [
        "# Get basic statistics for numerical variables\n",
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "OZ7BbXJLuesK",
        "outputId": "09f2dd49-6e9a-4951-fbc8-78b31b9a1f21"
      },
      "outputs": [],
      "source": [
        "# Check dispersion with box plot\n",
        "from sklearn import preprocessing\n",
        "def box_plot(df, standardize=True):\n",
        "\n",
        "    fig=plt.figure(figsize=(20,10))\n",
        "\n",
        "    if standardize==True:\n",
        "        # standardize columns for better visualization\n",
        "        df=pd.DataFrame(preprocessing.StandardScaler().fit_transform(df.values), columns = df.columns)\n",
        "    fig=sns.boxplot(x='value', y='variable', data=pd.melt(df.reset_index(), id_vars='index', value_vars=list(df.columns)),\n",
        "               orient='h')\n",
        "    fig.tick_params(labelsize=10)\n",
        "    fig.set_xlabel('')\n",
        "    fig.set_ylabel('')\n",
        "    fig.set_title('Note that variables are standardized\\nfor better visualization', fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "box_plot(dataset[num_var], standardize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81acPfUVugJC"
      },
      "outputs": [],
      "source": [
        "# We remove duration, pdays and previous\n",
        "dataset=dataset.drop(columns=['duration', 'pdays', 'age', 'campaign', 'previous'])\n",
        "num_var= dataset.drop(columns=['y']).select_dtypes([np.number]).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "0clA5hdPuiPi",
        "outputId": "a4c4a4d2-b1b5-477e-83d8-6da109f87153"
      },
      "outputs": [],
      "source": [
        "# Check distribution for target variable\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.catplot(x='y', kind=\"count\", data=dataset) # categorical plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvR5c-y1ujyK",
        "outputId": "f33d4a25-ede4-47ff-8a53-e3cc6f2828a1"
      },
      "outputs": [],
      "source": [
        "# Dataset is very unbalanced so we remove some observation for y=0 to be equal to 2*size of y=1.\n",
        "# This is called \"undersampling\"\n",
        "\n",
        "# We keep all y=1\n",
        "from sklearn.model_selection import train_test_split\n",
        "data_1 = dataset[dataset['y'] == 1]\n",
        "print(data_1.shape)\n",
        "\n",
        "# We take y=0 as double the size of data_1\n",
        "# Moreover we \"stratify\" the sampling in order to take the same distribution for each variable\n",
        "# We use the train_test_split function and we keep the test only\n",
        "all_data_0 = dataset[dataset['y'] == 0]\n",
        "percentage_corresponding_to_double_size = 2*data_1.shape[0] / all_data_0.shape[0] # 2*size_1 compared to size_0\n",
        "\n",
        "X = all_data_0.drop(columns=['y'])\n",
        "y = all_data_0['y'].to_frame()\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)\n",
        "data_0_big, data_0_small = train_test_split(all_data_0, test_size=percentage_corresponding_to_double_size,\n",
        "                                                    random_state=0, shuffle=True)\n",
        "print(data_0_big.shape) # remaining from the dataset\n",
        "print(data_0_small.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEqxjEViulhA",
        "outputId": "00fd5182-593f-498d-b4bb-1c87007e7243"
      },
      "outputs": [],
      "source": [
        "# Merge two dataset\n",
        "\n",
        "dataset=pd.concat([data_1, data_0_small], axis= 0).reset_index(drop=True)  # axis = 1 by column and = 0 by row\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "rHVZJ2X1umgk",
        "outputId": "70333985-8c6d-424c-c245-389c8ce5b552"
      },
      "outputs": [],
      "source": [
        "# Check distribution for target variable after downsampling\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.catplot(x='y', kind=\"count\", data=dataset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G52xWXIq9eEB",
        "outputId": "5ef90309-03fc-4d9d-c0f1-90414a8d1e49"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution and the boxplot of the numerical variables included in the dataset compared to the target (it's only 0 or 1)\n",
        "\n",
        "fig = plt.figure(figsize=(15,30))\n",
        "plot_count=1\n",
        "\n",
        "# scale variable for better visualizing boxplot\n",
        "dataset_scaled=pd.DataFrame(preprocessing.StandardScaler().fit_transform(dataset[num_var].values),columns = num_var)\n",
        "dataset_scaled['y']=dataset['y'].astype(str)\n",
        "y_1 = dataset.loc[dataset['y'] == 1] #.loc - access group of values using labels.\n",
        "y_0 = dataset.loc[dataset['y'] == 0]\n",
        "\n",
        "for var in num_var:\n",
        "    # plot variable distribution\n",
        "    ax = fig.add_subplot(math.ceil(len(num_var) / 2), 2, plot_count)\n",
        "    sns.histplot(y_1[var], label='1', ax=ax, alpha=0.7, kde=True)\n",
        "    sns.histplot(y_0[var], label='0', ax=ax, alpha=0.7, kde=True)\n",
        "    ax.set_title('Distribution of ' + var, fontsize=20)\n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xlabel('')\n",
        "    ax.legend(fontsize=16)\n",
        "    plot_count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UxWxnfJVuqXY",
        "outputId": "50b73438-18c3-44ed-bc72-cb12776931ce"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of the levels of the categorical variables compared with the target\n",
        "\n",
        "fig = plt.figure(figsize=(15,30))\n",
        "plot_count=1\n",
        "\n",
        "for var in cat_var:\n",
        "    # plot variable distribution\n",
        "    ax = fig.add_subplot(math.ceil(len(cat_var) / 2), 2, plot_count)\n",
        "    plot_set = dataset.groupby([var, 'y']).size().reset_index().pivot(columns='y', index=var, values=0)\n",
        "    plot_set=plot_set.div(plot_set.sum(axis=1), axis=0).plot(kind='barh', stacked=True, ax=ax)\n",
        "    ax.set_title('Target variable distribution for each\\nlevel (' + str(len(dataset[var].unique())) +\n",
        "                 ') of ' + var, fontsize=20)\n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_ylabel('')\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), fontsize=16)\n",
        "    plot_count += 1\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SLly_nOSusI4",
        "outputId": "3a301e01-4f1c-4208-e974-d06808c57872"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPFz8OxduupH"
      },
      "outputs": [],
      "source": [
        "# Create dummy variables & standardize the dataset\n",
        "dataset_dummy=pd.get_dummies(dataset.copy(), dummy_na=False, drop_first=True) # Whether to get k-1 dummies out of k categorical levels by removing the first level.\n",
        "dataset_dummy[num_var]=pd.DataFrame(preprocessing.StandardScaler().fit_transform(dataset[num_var].values),columns = num_var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "dHA8jonwuvoP",
        "outputId": "dca486e4-fdad-425e-b945-56b857357487"
      },
      "outputs": [],
      "source": [
        "dataset_dummy.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLB5Qn78uwsf"
      },
      "outputs": [],
      "source": [
        "# Check the correlations between variables\n",
        "corrmat = dataset_dummy.corr()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "w8VyCv_b_YWN",
        "outputId": "8ba66e3e-2765-46a2-800f-6d281c3a6769"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix in key-values pairs\n",
        "corrmat *= np.where(np.tri(*corrmat.shape, k=-1)==0, np.nan, 1)  # puts NaN on upper triangular matrix, including diagonal (k=-1)\n",
        "corrmat_list=corrmat.unstack().to_frame()\n",
        "\n",
        "# Check highest correlations\n",
        "corrmat_list.columns=['correlation']\n",
        "corrmat_list['abs_corr']=corrmat_list.correlation.abs()\n",
        "corrmat_list.sort_values(by=['abs_corr'], ascending=False, na_position='last', inplace=True)\n",
        "corrmat_list.drop(columns=['abs_corr']).head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-X_0GJ9Nu6t5",
        "outputId": "f0903ad9-5799-4647-ee64-9bd209981889"
      },
      "outputs": [],
      "source": [
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(corrmat, cmap =\"YlGnBu\", linewidths = 0.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyWGR2m3u9LB"
      },
      "outputs": [],
      "source": [
        "# Drop highly correlated columns\n",
        "dataset_original=dataset.copy() # save original dataset\n",
        "\n",
        "# Rename dataset_dummy and drop columns\n",
        "col_to_drop=['emp_var_rate', 'cons_price_idx', 'euribor3m', 'nr_employed', 'loan_unknown', 'housing_unknown']\n",
        "dataset=dataset_dummy.drop(columns=col_to_drop)\n",
        "num_var=dataset.columns.intersection(num_var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUhL0Zxc_52q",
        "outputId": "f387cb7f-409c-45c5-ad8e-d0010a903d0b"
      },
      "outputs": [],
      "source": [
        "# Ready to train and test our models!\n",
        "X = dataset.drop(columns=['y'])\n",
        "y = dataset['y'].values\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pomfgiFvAEN",
        "outputId": "1da13ff2-1dd7-425e-96fe-793f0011db5b"
      },
      "outputs": [],
      "source": [
        "# Split train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=dataset['y'])\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Linear Probability Model (LPM)\n",
        "\n",
        "Before diving into logistic regression, let's start with the simpler Linear Probability Model. The LPM treats the binary dependent variable as if it were continuous and applies ordinary least squares (OLS) regression.\n",
        "\n",
        "**Model specification:** P(y=1|X) = β₀ + β₁X₁ + β₂X₂ + ... + βₖXₖ + ε\n",
        "\n",
        "While conceptually simple, the LPM has several important limitations that we'll explore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit Linear Probability Model using OLS\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train Linear Probability Model\n",
        "lpm_model = LinearRegression()\n",
        "lpm_model.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions\n",
        "y_train_pred_lpm = lpm_model.predict(X_train)\n",
        "y_test_pred_lpm = lpm_model.predict(X_test)\n",
        "\n",
        "print(f\"LPM Training R²: {lpm_model.score(X_train, y_train):.4f}\")\n",
        "print(f\"LPM Test R²: {lpm_model.score(X_test, y_test):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine LPM predictions and identify problems\n",
        "print(\"Linear Probability Model - Prediction Statistics:\")\n",
        "print(f\"Training set predictions - Min: {y_train_pred_lpm.min():.4f}, Max: {y_train_pred_lpm.max():.4f}\")\n",
        "print(f\"Test set predictions - Min: {y_test_pred_lpm.min():.4f}, Max: {y_test_pred_lpm.max():.4f}\")\n",
        "print(f\"\\nPredictions outside [0,1] range:\")\n",
        "print(f\"Training: {np.sum((y_train_pred_lpm < 0) | (y_train_pred_lpm > 1))} out of {len(y_train_pred_lpm)} ({100*np.sum((y_train_pred_lpm < 0) | (y_train_pred_lpm > 1))/len(y_train_pred_lpm):.1f}%)\")\n",
        "print(f\"Test: {np.sum((y_test_pred_lpm < 0) | (y_test_pred_lpm > 1))} out of {len(y_test_pred_lpm)} ({100*np.sum((y_test_pred_lpm < 0) | (y_test_pred_lpm > 1))/len(y_test_pred_lpm):.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize LPM predictions vs actual values\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot 1: Histogram of predicted probabilities\n",
        "ax1.hist(y_test_pred_lpm, bins=30, alpha=0.7, edgecolor='black')\n",
        "ax1.axvline(x=0, color='red', linestyle='--', label='Probability = 0')\n",
        "ax1.axvline(x=1, color='red', linestyle='--', label='Probability = 1')\n",
        "ax1.set_xlabel('Predicted Probability')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Distribution of LPM Predicted Probabilities')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Scatter plot of predictions vs actual\n",
        "ax2.scatter(y_test_pred_lpm, y_test, alpha=0.6)\n",
        "ax2.plot([0, 1], [0, 1], 'r--', label='Perfect prediction')\n",
        "ax2.set_xlabel('Predicted Probability (LPM)')\n",
        "ax2.set_ylabel('Actual Value')\n",
        "ax2.set_title('LPM: Predicted vs Actual Values')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert LPM predictions to binary classifications (using 0.5 threshold)\n",
        "y_test_pred_lpm_binary = (y_test_pred_lpm >= 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "lpm_accuracy = accuracy_score(y_test, y_test_pred_lpm_binary)\n",
        "print(f\"LPM Classification Accuracy: {lpm_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problems with the Linear Probability Model\n",
        "\n",
        "The Linear Probability Model has several fundamental issues when dealing with binary dependent variables:\n",
        "\n",
        "### 1. **Predicted probabilities outside [0,1] range**\n",
        "- As we saw above, LPM can predict negative probabilities or probabilities greater than 1\n",
        "- This violates the basic definition of probability\n",
        "\n",
        "### 2. **Heteroskedasticity**\n",
        "- The error variance is not constant: Var(ε|X) = P(X)[1-P(X)]\n",
        "- This violates the OLS assumption of homoskedasticity\n",
        "- Standard errors are biased, affecting hypothesis testing\n",
        "\n",
        "### 3. **Linear relationship assumption**\n",
        "- LPM assumes a linear relationship between X and P(y=1|X)\n",
        "- In reality, the effect of explanatory variables on probability is often non-linear\n",
        "- Marginal effects are constant across all values of X (unrealistic)\n",
        "\n",
        "### 4. **Distributional assumptions**\n",
        "- OLS assumes normally distributed errors\n",
        "- With binary outcomes, errors follow a Bernoulli distribution\n",
        "\n",
        "### 5. **Efficiency concerns**\n",
        "- Due to heteroskedasticity, OLS estimators are not efficient\n",
        "- Maximum likelihood estimation (as in logistic regression) is more efficient\n",
        "\n",
        "**Solution:** Use logistic regression, which addresses these issues by:\n",
        "- Ensuring predicted probabilities stay within [0,1]\n",
        "- Using the logistic function to model non-linear relationships\n",
        "- Employing maximum likelihood estimation\n",
        "- Properly handling the binary nature of the dependent variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Logistic Regression\n",
        "\n",
        "Now let's implement logistic regression, which addresses the limitations of the Linear Probability Model.\n",
        "\n",
        "**Model specification:** \n",
        "- P(y=1|X) = 1 / (1 + e^(-(β₀ + β₁X₁ + β₂X₂ + ... + βₖXₖ)))\n",
        "- This ensures probabilities remain between 0 and 1\n",
        "- The relationship between X and P(y=1|X) is non-linear and S-shaped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "2G3NZ2MwA8bp",
        "outputId": "72b4bd6b-6828-422b-fe95-2fd52b4ba7a2"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logit_model = LogisticRegression(solver='lbfgs', random_state=0) # solver (https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451)\n",
        "logit_model.fit(X_train, y_train) # training the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "1KUIl-tCwAA4",
        "outputId": "f2174fbe-f0a2-4e57-c257-678002d25152"
      },
      "outputs": [],
      "source": [
        "# Get fitted values on test set for logistic regression\n",
        "y_test_predicted_logit = logit_model.predict(X_test)\n",
        "y_test_predicted_prob_logit = logit_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "# Compare LPM vs Logistic Regression predictions\n",
        "comparison_df = pd.DataFrame({\n",
        "    'True': y_test.flatten(), \n",
        "    'LPM_prob': y_test_pred_lpm.flatten(),\n",
        "    'LPM_pred': y_test_pred_lpm_binary.flatten(),\n",
        "    'Logit_prob': y_test_predicted_prob_logit.flatten(), \n",
        "    'Logit_pred': y_test_predicted_logit.flatten()\n",
        "})\n",
        "display(comparison_df.head(20))\n",
        "\n",
        "print(f\"\\nModel Comparison:\")\n",
        "print(f\"LPM Accuracy: {accuracy_score(y_test, y_test_pred_lpm_binary):.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_test_predicted_logit):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekdOKief53Zp",
        "outputId": "226da415-725d-4152-f422-c68639834bd5"
      },
      "outputs": [],
      "source": [
        "# Evaluate confusion matrix for Logistic Regression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_test_predicted_logit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "4eWqDpCfxGXl",
        "outputId": "30563949-f9e9-4677-b802-9355d32e18d8"
      },
      "outputs": [],
      "source": [
        "# Evaluate confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = ['0', '1']\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix for Logistic Regression\n",
        "plot_confusion_matrix(y_test, y_test_predicted_logit, title='Logistic Regression Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUm4fOFtxPxB",
        "outputId": "46b8400d-73d9-467e-9f1e-a777b66d6e16"
      },
      "outputs": [],
      "source": [
        "# Evaluate precision, recall, F1-score on train set\n",
        "# A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally),\n",
        "# whereas a micro-average will aggregate the contributions of all classes to compute the average metric.\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_test_predicted_logit))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "0VacNokWxQU1",
        "outputId": "4e320dc4-300c-471a-9a73-bff023f9720a"
      },
      "outputs": [],
      "source": [
        "# Evaluate ROC curve\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "logit_roc_auc = roc_auc_score(y_test, y_test_predicted_logit)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_predicted_prob_logit)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
